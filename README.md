# 100 D√≠as de IA

| Libros y Recursos | Estado de Finalizaci√≥n |
| ----- | -----|
| 1. [**Machine Learning Specialization**](https://www.coursera.org/specializations/machine-learning-introduction?page=1) | ‚úÖ |
| 2. [**Deep Learning Specialization**](https://www.coursera.org/specializations/deep-learning?)| ‚úÖ  |
| 3. [**IA generativa con grandes modelos ling√º√≠sticos**](https://www.coursera.org/learn/generative-ai-with-llms/) | ‚úÖ |
| 4. [**Curso de Deep Learning**](https://youtube.com/playlist?list=PLcfxtMhW8iFNMTFKrYMYYzVTNzu-xG-Ys&si=lqAlbDIhtOJ5zMP8) | ‚úÖ |
| 5. [**Computer Vision**](https://youtube.com/playlist?list=PLISuMnTdVU-yvm6X7SwKtUosfr4ZarStU&si=FOMUjJ5SvotgMhHW) | ‚úÖ |

| Proyectos Completados |
| ----------------- |
| 1.  |
| 2.  |
| 3. ... |

# Temas Aprendidos en Cada D√≠a
| **D√≠as** | **Temas Cubiertos** | 
|--------- | ------------------ |
| [D√≠a1](#D√≠a1) | Introducci√≥n a Deep Learning | 
| [D√≠a2](#D√≠a2) | Historia y Evoluci√≥n de Deep Learning | 
| [D√≠a3](#D√≠a3) | Breve Descripci√≥n de las Diferentes T√©cnicas en Deep Learning | 
| [D√≠a4](#D√≠a4) | Comparaci√≥n y Aplicaciones de T√©cnicas de Deep Learning en el Mundo Real | 
| [D√≠a5](#D√≠a5) | Redes Neuronales Artificiales (ANNs) | 
| [D√≠a6](#D√≠a6) | Forward y Backward Propagation | 
| [D√≠a7](#D√≠a7) | Coste y Funciones de P√©rdida | 
| [D√≠a8](#D√≠a8) | Algoritmos de Optimizaci√≥n | 
| [D√≠a9](#D√≠a9) | Overfitting y T√©cnicas de Regularizaci√≥n | 
| [D√≠a10](#D√≠a10) | Construyendo una Red Neuronal desde Cero: Clasificaci√≥n de Flores Iris | 
| [D√≠a11](#D√≠a11) | Construyendo una Red Neuronal con Tensorflow: Clasificaci√≥n de Digitos Escritos a Mano | 
| [D√≠a12](#D√≠a12) | Redes Neuronales Profundas | 
| [D√≠a13](#D√≠a13) | Conceptos b√°sicos y arquitectura general de las CNNs | 
| [D√≠a14](#D√≠a14) | ¬øC√≥mo funcionan las CNNs en comparaci√≥n con las ANNs? | 
| [D√≠a15](#D√≠a15) | Ejemplos Pr√°cticos de Aplicaci√≥n en la Industria | 
| [D√≠a16](#D√≠a16) | Comprendiendo la Convoluci√≥n en Im√°genes | 
| [D√≠a17](#D√≠a17) | Entendiendo los Filtros y su Papel en la Extracci√≥n de Caracter√≠sticas | 
| [D√≠a18](#D√≠a18) | Stride y Padding en CNNs | 
| [D√≠a19](#D√≠a19) | Pooling en CNNs | 
| [D√≠a20](#D√≠a20) |  | 
| [D√≠a21](#D√≠a21) |  | 
| [D√≠a22](#D√≠a22) |  | 
| [D√≠a23](#D√≠a23) |  | 
| [D√≠a24](#D√≠a24) |  | 
| [D√≠a25](#D√≠a25) |  | 
| [D√≠a26](#D√≠a26) |  | 
| [D√≠a27](#D√≠a27) |  | 
| [D√≠a28](#D√≠a28) |  | 
| [D√≠a29](#D√≠a29) |  | 
| [D√≠a30](#D√≠a30) |  | 
| [D√≠a31](#D√≠a31) |  | 
| [D√≠a32](#D√≠a32) |  | 
| [D√≠a33](#D√≠a33) |  | 
| [D√≠a34](#D√≠a34) |  | 
| [D√≠a35](#D√≠a35) |  | 
| [D√≠a36](#D√≠a36) |  | 
| [D√≠a37](#D√≠a37) |  | 
| [D√≠a38](#D√≠a38) |  | 
| [D√≠a39](#D√≠a39) |  | 
| [D√≠a40](#D√≠a40) |  | 
| [D√≠a41](#D√≠a41) |  | 
| [D√≠a42](#D√≠a42) |  | 
| [D√≠a43](#D√≠a43) |  | 
| [D√≠a44](#D√≠a44) |  | 
| [D√≠a45](#D√≠a45) |  | 
| [D√≠a46](#D√≠a46) |  | 
| [D√≠a47](#D√≠a47) |  | 
| [D√≠a48](#D√≠a48) |  | 
| [D√≠a49](#D√≠a49) |  | 
| [D√≠a50](#D√≠a50) |  | 
| [D√≠a51](#D√≠a51) |  | 
| [D√≠a52](#D√≠a52) |  | 
| [D√≠a53](#D√≠a53) |  | 
| [D√≠a54](#D√≠a54) |  | 
| [D√≠a55](#D√≠a55) |  | 
| [D√≠a56](#D√≠a56) |  | 
| [D√≠a57](#D√≠a57) |  | 
| [D√≠a58](#D√≠a58) |  | 
| [D√≠a59](#D√≠a59) |  | 
| [D√≠a60](#D√≠a60) |  | 
| [D√≠a61](#D√≠a61) |  | 
| [D√≠a62](#D√≠a62) |  | 
| [D√≠a63](#D√≠a63) |  | 
| [D√≠a64](#D√≠a64) |  | 
| [D√≠a65](#D√≠a65) |  | 
| [D√≠a66](#D√≠a66) |  | 
| [D√≠a67](#D√≠a67) |  | 
| [D√≠a68](#D√≠a68) |  | 
| [D√≠a69](#D√≠a69) |  | 
| [D√≠a70](#D√≠a70) |  | 
| [D√≠a71](#D√≠a71) |  | 
| [D√≠a72](#D√≠a72) |  | 
| [D√≠a73](#D√≠a73) |  | 
| [D√≠a74](#D√≠a74) |  | 
| [D√≠a75](#D√≠a75) |  | 
| [D√≠a76](#D√≠a76) |  | 
| [D√≠a77](#D√≠a77) |  | 
| [D√≠a78](#D√≠a78) |  | 
| [D√≠a79](#D√≠a79) |  | 
| [D√≠a80](#D√≠a80) |  | 
| [D√≠a81](#D√≠a81) |  | 
| [D√≠a82](#D√≠a82) |  | 
| [D√≠a83](#D√≠a83) |  | 
| [D√≠a84](#D√≠a84) |  | 
| [D√≠a85](#D√≠a85) |  | 
| [D√≠a86](#D√≠a86) |  | 
| [D√≠a87](#D√≠a87) |  | 
| [D√≠a88](#D√≠a88) |  | 
| [D√≠a89](#D√≠a89) |  | 
| [D√≠a90](#D√≠a90) |  | 
| [D√≠a91](#D√≠a91) |  | 
| [D√≠a92](#D√≠a92) |  | 
| [D√≠a93](#D√≠a93) |  | 
| [D√≠a94](#D√≠a94) |  | 
| [D√≠a95](#D√≠a95) |  | 
| [D√≠a96](#D√≠a96) |  | 
| [D√≠a97](#D√≠a97) |  | 
| [D√≠a98](#D√≠a98) |  | 
| [D√≠a99](#D√≠a99) |  | 
| [D√≠a100](#D√≠a100) |  | 

# D√≠a1
---
## Introducci√≥n a Deep Learning üåü

¬°Bienvenidos al primer d√≠a de mi viaje de 100 d√≠as explorando la Inteligencia Artificial! üöÄ Hoy comenzamos con **Deep Learning**.

### ¬øQu√© es Deep Learning?

Deep Learning, o Aprendizaje Profundo, es una rama avanzada del **Machine Learning** que se inspira en la estructura y funci√≥n del cerebro humano. Utiliza **redes neuronales artificiales** para aprender de grandes vol√∫menes de datos y tomar decisiones o hacer predicciones precisas.

### ¬øPor qu√© es importante?

En los √∫ltimos a√±os, el Deep Learning ha revolucionado muchas industrias. Desde la **visi√≥n por computadora** que permite a los veh√≠culos aut√≥nomos ver el mundo, hasta el **procesamiento de lenguaje natural** que ayuda a las m√°quinas a entender y responder en lenguaje humano. Deep Learning es la tecnolog√≠a detr√°s de innovaciones impresionantes que est√°n cambiando la forma en que interactuamos con el mundo digital.

### ¬øC√≥mo funciona?

Las redes neuronales profundas est√°n compuestas por capas de neuronas artificiales. Cada capa transforma la entrada de datos en algo m√°s √∫til para la siguiente capa. A trav√©s de un proceso de entrenamiento, estas redes aprenden a extraer caracter√≠sticas complejas y patrones directamente de los datos.

### Ejemplos de Aplicaciones de Deep Learning:

- **Reconocimiento de Im√°genes**: Identificar objetos y personas en fotos y videos.
- **Traducci√≥n Autom√°tica**: Convertir texto de un idioma a otro con gran precisi√≥n.
- **Diagn√≥stico M√©dico**: Analizar im√°genes m√©dicas para detectar enfermedades.



 **Recursos para comenzar**üß†:
- **[APRENDIZAJE PROFUNDO EN INTELIGENCIA ARTIFICIAL](https://youtu.be/Zcb8R2TF3bI?si=f1NIEJgXh7cWdadV)** - Una breve esplicacion dew que es deep learning.
- **[¬øQUE ES EL DEEP LEARNING? - EXPLICADO MUY FACIL](https://youtu.be/s0SbvGiG28w?si=Rr51xld8H8ilsrz9)** - Video de Dalto explicando que es deep learning.
- **[¬øQu√© son el MACHINE LEARNING y el DEEP LEARNING?](https://youtu.be/HMEjoBnCc9c?si=U5MXn98cY7Yovy8w)** - Diferencias entre el Machine Learning y el Deep Learning.
- **[¬øDe qu√© es capaz la inteligencia artificial? ](https://youtu.be/34Kz-PP_X7c?si=sbV0ENQYtvT2JKiI)** - Documental de DW.

¬°√önete a m√≠ en este emocionante viaje y no dudes en compartir tus pensamientos y preguntas! üöÄ

---
# D√≠a2
---
## Historia y Evoluci√≥n de Deep Learning üìú

¬°Bienvenidos al segundo d√≠a de nuestra traves√≠a de 100 d√≠as en el mundo de la Inteligencia Artificial! Hoy, exploramos la fascinante **historia y evoluci√≥n de Deep Learning**. üåü

### Or√≠genes y Primeros Pasos

#### 1943: La Idea de una Neurona Artificial üí°
El viaje de Deep Learning comenz√≥ con Warren McCulloch y Walter Pitts, quienes propusieron el primer modelo matem√°tico de una **neurona artificial**. Su trabajo sent√≥ las bases para las redes neuronales, sugiriendo que las neuronas podr√≠an ser el equivalente funcional de un interruptor binario.

#### 1958: El Perceptr√≥n ü§ñ
Frank Rosenblatt desarroll√≥ el **Perceptr√≥n**, el primer modelo de red neuronal capaz de aprender. El perceptr√≥n es un tipo simple de red que puede clasificar datos en dos categor√≠as. Aunque su capacidad era limitada, fue un hito importante que inspir√≥ investigaciones futuras.

### El Invierno de la IA ‚ùÑÔ∏è

#### A√±os 70-80: Desaf√≠os y Dudas
Durante los a√±os 70 y 80, las expectativas sobre las redes neuronales no se cumplieron, y la falta de poder computacional y datos llev√≥ a lo que se conoce como el **"invierno de la IA"**. Durante este per√≠odo, la investigaci√≥n en redes neuronales se desaceler√≥ debido al escepticismo y la falta de avances significativos.

### Renacimiento y Avances üöÄ

#### 1986: El Redescubrimiento de la Propagaci√≥n hacia Atr√°s
En 1986, David Rumelhart, Geoffrey Hinton y Ronald Williams revitalizaron el inter√©s en las redes neuronales con su trabajo sobre la **retropropagaci√≥n**. Este algoritmo permiti√≥ el entrenamiento eficaz de redes neuronales multicapa, allanando el camino para el desarrollo de modelos m√°s complejos.

#### A√±os 90: Aplicaciones Pr√°cticas üåê
A medida que aumentaba el poder computacional y se dispon√≠a de m√°s datos, las redes neuronales comenzaron a mostrar su potencial en √°reas como el reconocimiento de patrones y la predicci√≥n financiera. Sin embargo, a√∫n quedaban desaf√≠os significativos por superar.

### La Era de Deep Learning üí•

#### 2006: El Avance de las Redes Profundas
Geoffrey Hinton y su equipo introdujeron el concepto de **preentrenamiento de capas** en redes profundas, lo que permiti√≥ entrenar eficientemente modelos con muchas capas. Este avance marc√≥ el comienzo de la **era de Deep Learning**, demostrando que las redes neuronales profundas pod√≠an superar a los m√©todos tradicionales en tareas complejas.

#### 2012: El Triunfo en ImageNet üèÜ
El hito crucial lleg√≥ en 2012 cuando una red profunda conocida como **AlexNet**, desarrollada por Alex Krizhevsky, Ilya Sutskever y Geoffrey Hinton, gan√≥ el desaf√≠o de reconocimiento de im√°genes de **ImageNet** con un margen significativo. Esto consolid√≥ a Deep Learning como la tecnolog√≠a l√≠der en visi√≥n por computadora.

### Transformadores y Nuevas Fronteras üöÄ

#### 2017: El Surgimiento de los Transformadores
En 2017, el art√≠culo "Attention is All You Need" de Google introdujo el **modelo Transformer**, revolucionando el procesamiento del lenguaje natural (NLP). Los Transformers, como **BERT** y **GPT**, demostraron capacidades impresionantes en tareas de lenguaje, superando a los modelos anteriores.

#### 2018: GPT y el Avance de los Modelos de Lenguaje
OpenAI lanz√≥ **GPT (Generative Pre-trained Transformer)**, seguido por GPT-2 y el famoso **GPT-3** en 2020. Estos modelos mostraron habilidades sin precedentes en generaci√≥n de texto, comprensi√≥n y traducci√≥n, marcando un hito en el desarrollo de la IA.

### Innovaciones Recientes üîÑ

#### 2021: DALL-E y la Creatividad Artificial
OpenAI present√≥ **DALL-E**, un modelo capaz de generar im√°genes a partir de descripciones textuales. Esta innovaci√≥n destac√≥ la capacidad de la IA para combinar lenguaje y visi√≥n, abriendo nuevas posibilidades en arte y dise√±o.

#### 2021: AlphaFold y la Revoluci√≥n en la Biolog√≠a
DeepMind's **AlphaFold** resolvi√≥ uno de los mayores desaf√≠os en biolog√≠a: la predicci√≥n de estructuras proteicas. Este avance promete acelerar el descubrimiento de medicamentos y mejorar nuestra comprensi√≥n de la biolog√≠a molecular.

#### 2022: ChatGPT y la Conversaci√≥n Natural
OpenAI lanz√≥ **ChatGPT**, una versi√≥n mejorada de GPT-3 optimizada para conversaciones interactivas. Este modelo demostr√≥ habilidades avanzadas en el di√°logo, respondiendo preguntas y asistiendo en diversas tareas de manera coherente y precisa.


### Recursos para Explorar M√°s:

- **[Breve Historia de las Redes Neuronales Artificiales](https://www.aprendemachinelearning.com/breve-historia-de-las-redes-neuronales-artificiales/)** - Un art√≠culo detallado sobre la evoluci√≥n de las redes neuronales.
- **[The brief history of artificial intelligence](https://ourworldindata.org/brief-history-of-ai)** - Un art√≠culo detallado sobre la evoluci√≥n de la IA.

### Evoluci√≥n de los modelos de IA con respecto a la computaci√≥n utilizada en su entrenamiento

<<<<<<< HEAD
=======
https://github.com/Oliver369X/100DaysOfAI/assets/110129950/64c6b46d-4c12-4e7a-8511-b35a2ad5be8e

>>>>>>> 52f223851c1f64cb143e7b84519e39d23a2985f8
---
# D√≠a3
---
## Breve Descripci√≥n de las Diferentes T√©cnicas en Deep Learning üß†


### 1. Redes Neuronales Convolucionales (CNN) üñºÔ∏è

#### Descripci√≥n
Las **Redes Neuronales Convolucionales (CNN)** est√°n dise√±adas para procesar datos con una estructura de grilla, como las im√°genes. Utilizan capas convolucionales que aplican filtros para detectar caracter√≠sticas como bordes, texturas y patrones en las im√°genes.

#### Componentes Clave
- **Capas Convolucionales**: Aplican filtros para extraer caracter√≠sticas locales.
- **Capas de Pooling**: Reducen la dimensionalidad y ayudan a generalizar.
- **Capas Completamente Conectadas**: Usadas para clasificar y tomar decisiones basadas en las caracter√≠sticas extra√≠das.

### 2. Redes Neuronales Recurrentes (RNN) üîÅ

#### Descripci√≥n
Las **Redes Neuronales Recurrentes (RNN)** est√°n dise√±adas para procesar secuencias de datos, como texto o series temporales. Tienen conexiones recurrentes que permiten que la informaci√≥n persista, lo que es √∫til para modelar dependencias temporales.

#### Componentes Clave
- **Celdas Recurrentes**: Mantienen un estado oculto que captura informaci√≥n de pasos anteriores.
- **LSTM y GRU**: Variantes avanzadas de RNN que abordan problemas de memoria a largo plazo.

### 3. Redes Generativas Adversariales (GAN) üé®

#### Descripci√≥n
Las **Redes Generativas Adversariales (GAN)** constan de dos redes: una generadora y una discriminadora. La generadora crea datos falsos, mientras que la discriminadora intenta distinguir entre datos reales y falsos. Este proceso competitivo mejora la capacidad de la generadora para producir datos realistas.

#### Componentes Clave
- **Generador**: Crea datos sint√©ticos.
- **Discriminador**: Distingue entre datos reales y generados.
- **Juego Adversarial**: La competencia entre las dos redes mejora el rendimiento del sistema.

### 4. Transformadores üîÑ

#### Descripci√≥n
Los **Transformadores** han revolucionado el procesamiento del lenguaje natural (NLP) con su mecanismo de atenci√≥n que permite procesar todas las palabras de una oraci√≥n en paralelo. Esto los hace altamente eficientes y precisos en tareas de lenguaje.

#### Componentes Clave
- **Mecanismo de Atenci√≥n**: Pondera la importancia de diferentes palabras en una oraci√≥n.
- **Codificadores y Decodificadores**: Procesan las secuencias de entrada y generan secuencias de salida.

### 5. Modelos de Difusi√≥n üå´Ô∏è

#### Descripci√≥n
Los **Modelos de Difusi√≥n** son una t√©cnica emergente en generaci√≥n de datos. Funcionan modelando la distribuci√≥n de los datos y luego generando nuevos ejemplos a partir de esta distribuci√≥n, similar a los procesos f√≠sicos de difusi√≥n.

#### Componentes Clave
- **Proceso de Difusi√≥n**: Modela c√≥mo los datos cambian con el tiempo.
- **Reconstrucci√≥n Inversa**: Genera nuevos datos a partir del proceso de difusi√≥n.

### 6. Modelos Multimodales üé•üéµüìù

#### Descripci√≥n
Los **Modelos Multimodales** integran y procesan m√∫ltiples tipos de datos, como texto, im√°genes y audio, para realizar tareas complejas que requieren comprensi√≥n de informaci√≥n diversa.

#### Componentes Clave
- **Fusi√≥n de Modalidades**: Combina diferentes tipos de datos en una representaci√≥n unificada.
- **Atenci√≥n Cruzada**: Captura interacciones entre diferentes modalidades.


### Recursos para Explorar M√°s:

- **[¬°Redes Neuronales CONVOLUCIONALES! ](https://youtu.be/V8j1oENVz00?si=RY91rvLjMXPbjRbF)** - Video detallado sobre CNN.
- **[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)** - Una explicaci√≥n profunda sobre las RNN y LSTM.
- **[GANs in Action](https://www.youtube.com/watch?v=8L11aMN5KY8)** - Un video tutorial sobre GANs.
- **[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)** - Una gu√≠a visual sobre transformadores.
- **[C√≥mo funciona la generaci√≥n de im√°genes con IA (modelos de difusi√≥n)](https://youtu.be/mNxzQvdVSQI?si=_Lno74MYiqcbidei)** - Introducci√≥n a los modelos de difusi√≥n.
- **[Multimodal learning](https://en.wikipedia.org/wiki/Multimodal_learning)** - Definicion de Wikipedia.

---

# D√≠a4
---
## Comparaci√≥n y Aplicaciones de T√©cnicas de Deep Learning en el Mundo Real üåç

¬°Hola a todos! compararemos las diferentes t√©cnicas de Deep Learning que discutimos ayer y exploraremos sus aplicaciones en el mundo real. Vamos a sumergirnos en c√≥mo se utilizan las **CNN, RNN, GAN, Transformadores, Modelos de Difusi√≥n y Modelos Multimodales** en diversos campos. üåê

### Comparaci√≥n de T√©cnicas de Deep Learning

| T√©cnica         | Descripci√≥n                                                   | Fortalezas                                                     | Limitaciones                                                       |
|-----------------|---------------------------------------------------------------|----------------------------------------------------------------|--------------------------------------------------------------------|
| **CNN**         | Procesan datos con estructura de grilla (como im√°genes).       | Excelente para tareas de visi√≥n por computadora.                | No maneja bien datos secuenciales o dependencias temporales.       |
| **RNN**         | Procesan secuencias de datos (como texto o series temporales). | Capturan dependencias temporales y contextuales.                | Pueden sufrir de problemas de gradiente desaparecido/explosivo.    |
| **GAN**         | Generan datos sint√©ticos mediante una competencia entre dos redes. | Producen datos realistas en imagen, video y audio.              | Dificultad en entrenamiento y estabilidad.                         |
| **Transformadores** | Procesan secuencias de datos en paralelo utilizando atenci√≥n. | Eficientes y precisos en procesamiento de lenguaje natural.     | Requieren grandes cantidades de datos y recursos computacionales.  |
| **Modelos de Difusi√≥n** | Modelan la distribuci√≥n de datos para generaci√≥n.        | Alta calidad en generaci√≥n de im√°genes y datos.                 | T√©cnicamente complejos y requieren mucho tiempo de entrenamiento.  |
| **Modelos Multimodales** | Integran m√∫ltiples tipos de datos (texto, imagen, audio). | Capturan interacciones complejas entre diferentes tipos de datos. | Complejidad en la fusi√≥n de datos y gesti√≥n de m√∫ltiples modalidades. |

### Aplicaciones en el Mundo Real

#### 1. Redes Neuronales Convolucionales (CNN) üñºÔ∏è

**Aplicaciones:**
- **Reconocimiento de Im√°genes**: Identificaci√≥n de objetos, personas y escenas en im√°genes.
- **Diagn√≥stico M√©dico**: An√°lisis de im√°genes m√©dicas, como radiograf√≠as y resonancias magn√©ticas.
- **Seguridad y Vigilancia**: Detecci√≥n de anomal√≠as y reconocimiento facial.

#### 2. Redes Neuronales Recurrentes (RNN) üîÅ

**Aplicaciones:**
- **Procesamiento del Lenguaje Natural (NLP)**: Traducci√≥n autom√°tica, generaci√≥n de texto, chatbots.
- **An√°lisis de Series Temporales**: Predicci√≥n de mercados financieros, demanda energ√©tica, clima.
- **Reconocimiento de Voz**: Transcripci√≥n y comandos de voz en asistentes virtuales.

#### 3. Redes Generativas Adversariales (GAN) üé®

**Aplicaciones:**
- **Generaci√≥n de Im√°genes y Videos**: Creaci√≥n de arte digital, efectos visuales en pel√≠culas.
- **Aumento de Datos**: Generaci√≥n de datos sint√©ticos para mejorar el entrenamiento de modelos.
- **Restauraci√≥n de Im√°genes**: Mejora de resoluci√≥n, eliminaci√≥n de ruido, restauraci√≥n de im√°genes antiguas.

#### 4. Transformadores üîÑ

**Aplicaciones:**
- **Procesamiento del Lenguaje Natural (NLP)**: Modelos de lenguaje avanzados como GPT, BERT, traducci√≥n autom√°tica.
- **Generaci√≥n de Texto**: Resumen autom√°tico, generaci√≥n de contenido, respuestas autom√°ticas en chats.
- **An√°lisis de Datos**: Clasificaci√≥n de documentos, detecci√≥n de entidades nombradas, an√°lisis de sentimientos.

#### 5. Modelos de Difusi√≥n üå´Ô∏è

**Aplicaciones:**
- **Generaci√≥n de Im√°genes**: Creaci√≥n de im√°genes de alta calidad a partir de descripciones textuales.
- **Simulaci√≥n de Procesos F√≠sicos**: Modelado de fen√≥menos naturales como la difusi√≥n de gases.
- **Dise√±o Gr√°fico**: Creaci√≥n de patrones y texturas para dise√±o digital.

#### 6. Modelos Multimodales üé•üéµüìù

**Aplicaciones:**
- **Sistemas de Recomendaci√≥n**: Recomendaciones personalizadas basadas en m√∫ltiples tipos de datos (texto, im√°genes, audio).
- **An√°lisis de Redes Sociales**: Comprensi√≥n de publicaciones multimedia, an√°lisis de sentimientos.
- **Asistentes Virtuales**: Integraci√≥n de voz, texto e im√°genes para interacci√≥n m√°s natural y completa.


### Recursos para Explorar M√°s:


- **[The GAN Zoo](https://github.com/hindupuravinash/the-gan-zoo)** - Una colecci√≥n de diferentes tipos de GANs.
- **[Attention is All You Need](https://arxiv.org/abs/1706.03762)** - El art√≠culo seminal sobre transformadores.
- **[Explicaci√≥n Completa: Attention is All You Need](https://youtu.be/as2FFM3c6mI?si=_pNuRFCEHHYsizro)** - Un video detallado explicando los transformadores.

---

# D√≠a5
---
## Redes Neuronales Artificiales (ANNs)  üß†

¬°Hola a todos! En el quinto d√≠a de nuestra traves√≠a de 100 d√≠as en el mundo de la Inteligencia Artificial, exploraremos la estructura b√°sica de las Redes Neuronales Artificiales (ANNs) y entenderemos c√≥mo funcionan sus capas neuronales. üåü

### ¬øQu√© son las Redes Neuronales Artificiales (ANNs)?

Las Redes Neuronales Artificiales (ANNs) son modelos computacionales inspirados en el funcionamiento del cerebro humano. Est√°n dise√±adas para reconocer patrones y resolver problemas complejos mediante el aprendizaje a partir de datos. üåê

### Estructura B√°sica de una Red Neuronal

Una red neuronal t√≠pica consta de tres tipos de capas:

1. **Capa de Entrada (Input Layer)**: Recibe los datos iniciales.
2. **Capas Ocultas (Hidden Layers)**: Procesan la informaci√≥n recibida de la capa de entrada.
3. **Capa de Salida (Output Layer)**: Genera el resultado final.


#### 1. **Capa de Entrada (Input Layer)**
La capa de entrada es la primera capa de la red neuronal. Cada nodo en esta capa representa una caracter√≠stica del conjunto de datos de entrada. Por ejemplo, en una red que procesa im√°genes, cada nodo podr√≠a representar el valor de un p√≠xel de la imagen.

#### 2. **Capas Ocultas (Hidden Layers)**
Las capas ocultas son las encargadas de realizar la mayor parte del procesamiento de la red. Pueden existir m√∫ltiples capas ocultas, cada una compuesta por m√∫ltiples nodos o "neuronas". Cada neurona en una capa est√° conectada a todas las neuronas de la capa anterior y de la capa siguiente.

##### Funcionamiento de las Capas Ocultas:
- **Pesos y Sesgos (Weights and Biases)**: Cada conexi√≥n entre neuronas tiene un peso asignado que indica la importancia de la entrada correspondiente. Adem√°s, cada neurona tiene un valor de sesgo que ajusta la salida del nodo.
- **Funciones de Activaci√≥n (Activation Functions)**: Despu√©s de que una neurona recibe la entrada ponderada, aplica una funci√≥n de activaci√≥n para introducir no linealidades en el modelo. Las funciones de activaci√≥n comunes incluyen ReLU (Rectified Linear Unit), Sigmoid y Tanh.



#### 3. **Capa de Salida (Output Layer)**
La capa de salida es la √∫ltima capa de la red neuronal y proporciona el resultado final. La estructura de esta capa depende del tipo de tarea que est√© realizando la red. Por ejemplo, en un problema de clasificaci√≥n binaria, la capa de salida podr√≠a tener una sola neurona con una funci√≥n de activaci√≥n Sigmoid.

### ¬øC√≥mo Aprenden las Redes Neuronales?

El aprendizaje en redes neuronales implica ajustar los pesos y los sesgos de la red para minimizar el error en las predicciones. Este proceso se realiza mediante un algoritmo de optimizaci√≥n llamado **Backpropagation** (retropropagaci√≥n), que utiliza el **Gradiente Descendente** para ajustar los pesos de manera iterativa.


### Recursos para Explorar M√°s:

- **[C√≥mo funcionan las redes neuronales](https://youtu.be/CU24iC3grq8?si=9UT2DpOAA1cQ1Ay0)** (Video).
- **[¬øQu√© es una Red Neuronal?](https://youtu.be/jKCQsndqEGQ?si=jNASfwuoQB9tXyle)** - (Video).
- **[Funciones de activaci√≥n a detalle](https://youtu.be/_0wdproot34?si=B27NeiOze7QGGi6K)** - (Video).
- **[Juegue con una red neuronal ](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=1&seed=0.87931&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)** - Juegue con una red neuronal aqu√≠ mismo en su navegador.
No te preocupes, no puedes romperlo.
![ANNs](https://github.com/Oliver369X/100DaysOfAI/assets/110129950/6de8c3e3-ea5a-46e0-8fd0-600e794b422d)

![back2](https://github.com/Oliver369X/100DaysOfAI/assets/110129950/2ebfb67d-7af9-49bd-a509-b1d6babf0148)

---

# D√≠a6
---
## Conceptos de Forward y Backward Propagation üß†üîÑ

¬°Hola a todos! Hoy, en el sexto d√≠a de nuestro viaje de 100 d√≠as en el mundo de la Inteligencia Artificial, exploraremos dos conceptos fundamentales para el entrenamiento de redes neuronales: **Forward Propagation** y **Backward Propagation**. Estos procesos son esenciales para que las redes neuronales aprendan de los datos y mejoren su rendimiento. üöÄ

### ¬øQu√© es Forward Propagation?

**Forward Propagation** es el proceso mediante el cual los datos de entrada se transmiten a trav√©s de la red neuronal para generar una salida. Este flujo de informaci√≥n comienza en la capa de entrada, pasa por las capas ocultas y finalmente llega a la capa de salida.

#### Pasos de Forward Propagation:

1. **Entrada**: Los datos de entrada se presentan a la red neuronal.
2. **Ponderaci√≥n**: Cada neurona en la capa de entrada env√≠a sus datos ponderados a cada neurona de la primera capa oculta.
3. **Activaci√≥n**: Las neuronas de la capa oculta calculan una suma ponderada de sus entradas, aplican una funci√≥n de activaci√≥n y transmiten el resultado a la siguiente capa.
4. **Salida**: Este proceso se repite capa por capa hasta que los datos alcanzan la capa de salida, donde se generan las predicciones finales.

### ¬øQu√© es Backward Propagation?

**Backward Propagation** (o retropropagaci√≥n) es el proceso mediante el cual la red neuronal ajusta sus pesos y sesgos para minimizar el error en sus predicciones. Este ajuste se realiza mediante la propagaci√≥n del error desde la capa de salida hacia atr√°s a trav√©s de las capas ocultas, hasta llegar a la capa de entrada.

#### Pasos de Backward Propagation:

1. **C√°lculo del Error**: Se calcula la diferencia entre la salida real de la red y la salida esperada (etiquetas verdaderas).
2. **Propagaci√≥n del Error**: El error se propaga hacia atr√°s a trav√©s de la red. En cada neurona, se calcula el gradiente del error con respecto a sus pesos y sesgos.
3. **Ajuste de Pesos y Sesgos**: Los pesos y sesgos se actualizan utilizando el gradiente calculado y una tasa de aprendizaje, reduciendo as√≠ el error de la red.

### C√≥mo Funcionan Juntos Forward y Backward Propagation

1. **Forward Propagation**: Los datos de entrada se procesan a trav√©s de la red para generar una predicci√≥n.
2. **C√°lculo del Error**: Se compara la predicci√≥n con la etiqueta verdadera para calcular el error.
3. **Backward Propagation**: El error se propaga hacia atr√°s a trav√©s de la red, y los pesos y sesgos se ajustan en consecuencia.
4. **Actualizaci√≥n de Par√°metros**: Los par√°metros de la red se actualizan para reducir el error en futuras predicciones.

### Ejemplo Simplificado

Imaginemos que estamos entrenando una red neuronal para predecir el precio de una casa basado en su tama√±o.

1. **Forward Propagation**:
   - Entrada: Tama√±o de la casa.
   - C√°lculo: La red multiplica el tama√±o por un peso, a√±ade un sesgo y aplica una funci√≥n de activaci√≥n.
   - Salida: Predicci√≥n del precio de la casa.

2. **C√°lculo del Error**:
   - Comparamos la predicci√≥n con el precio real y calculamos el error.

3. **Backward Propagation**:
   - Propagamos el error hacia atr√°s a trav√©s de la red, calculando el gradiente del error con respecto a cada peso y sesgo.
   - Ajustamos los pesos y sesgos para minimizar el error en futuras predicciones.


### Recursos para Explorar M√°s:

- **[Redes Neuronales (forward propagation y backpropagation)](https://youtu.be/A9jZflhT2R0?si=uQj8Xw1xa2_O1kDO)** -Explicacion matematica(Video).
- **[Las Matem√°ticas de Backpropagation | DotCSV](https://youtu.be/M5QHwkkHgAA?si=ZiX3Gp9I25liaNFq)** - Explicacion matematica(Video).

---

# D√≠a7

---
## Conceptos de Coste y Funciones de P√©rdida üí°üìâ

¬°Hola a todos! Hoy, en el s√©ptimo d√≠a de nuestro reto #100DaysOfAI, exploraremos dos conceptos fundamentales para el entrenamiento de redes neuronales: **coste** y **funciones de p√©rdida**. Estos conceptos son esenciales para evaluar el rendimiento de nuestros modelos y guiar el proceso de aprendizaje. üöÄ

### ¬øQu√© es el Coste?

El **coste** se refiere a la medida de lo mal que un modelo de red neuronal est√° realizando sus predicciones en comparaci√≥n con los valores reales. En otras palabras, es una representaci√≥n cuantitativa del error del modelo. Cuanto menor sea el coste, mejor ser√° el rendimiento del modelo.

### ¬øQu√© es una Funci√≥n de P√©rdida?

Una **funci√≥n de p√©rdida** es una funci√≥n matem√°tica que mide la discrepancia entre las predicciones del modelo y los valores reales. Durante el entrenamiento, el objetivo es minimizar esta funci√≥n de p√©rdida para mejorar la precisi√≥n del modelo. 

### Tipos Comunes de Funciones de P√©rdida:

1. **Error Cuadr√°tico Medio (Mean Squared Error, MSE)**:

2. **Error Absoluto Medio (Mean Absolute Error, MAE)**:


3. **Entrop√≠a Cruzada (Cross-Entropy)**:


### Relaci√≥n entre Coste y Funci√≥n de P√©rdida:

- **Coste Total**: La funci√≥n de p√©rdida calcula el error para una sola instancia de datos, mientras que el coste total (tambi√©n conocido como funci√≥n de coste o funci√≥n de error) es la media de las p√©rdidas para todo el conjunto de entrenamiento.
- **Optimizaci√≥n**: Durante el entrenamiento, el algoritmo de optimizaci√≥n ajusta los pesos de la red neuronal para minimizar el coste total. Esto se realiza t√≠picamente mediante un algoritmo de optimizaci√≥n como el gradiente descendente.

### Importancia en el Entrenamiento

1. **Evaluaci√≥n del Modelo**: Las funciones de p√©rdida nos permiten evaluar cu√°n bien o mal est√° desempe√±√°ndose el modelo.
2. **Gu√≠a para la Optimizaci√≥n**: Proveen la se√±al que gu√≠a el proceso de optimizaci√≥n durante el entrenamiento. Sin una funci√≥n de p√©rdida, no podr√≠amos ajustar los pesos de manera efectiva.
3. **Selecci√≥n de Modelos**: Diferentes problemas pueden requerir diferentes funciones de p√©rdida. Elegir la funci√≥n correcta es crucial para el √©xito del modelo.


### Recursos para Explorar M√°s:

- **[3Blue1Brown's YouTube Series on Neural Networks](https://youtu.be/mwHiaTrQOiI?si=j_a-9WxP_1um9YVc)** - Una serie de videos educativos que visualizan estos procesos de manera intuitiva.

---

# D√≠a8

---
## Algoritmos de Optimizaci√≥n  üöÄüìà

¬°Hola a todos! En el d√≠a 8 de nuestro reto #100DaysOfAI, vamos a profundizar en los **algoritmos de optimizaci√≥n avanzados**. Estos algoritmos son esenciales para mejorar el rendimiento y la eficiencia de los modelos de aprendizaje profundo. ¬°Vamos a explorarlos juntos! üåü

### ¬øQu√© es la Optimizaci√≥n?

La **optimizaci√≥n** en el contexto del aprendizaje profundo se refiere al proceso de ajustar los par√°metros del modelo (como los pesos de las redes neuronales) para minimizar la funci√≥n de p√©rdida. Este proceso es crucial para que el modelo pueda aprender de los datos y hacer predicciones precisas.

### Algoritmos de Optimizaci√≥n Comunes

1. **Gradiente Descendente Estoc√°stico (SGD)**:
   - **Descripci√≥n**: En lugar de utilizar todo el conjunto de datos para calcular los gradientes, el SGD actualiza los par√°metros del modelo usando un solo ejemplo de entrenamiento a la vez.
   - **Ventaja**: Es m√°s r√°pido y puede manejar grandes conjuntos de datos.

2. **Gradiente Descendente por Minilotes (Mini-batch Gradient Descent)**:
   - **Descripci√≥n**: Combina los enfoques de SGD y del gradiente descendente de lote completo, actualizando los par√°metros utilizando un peque√±o subconjunto (mini-lote) de los datos de entrenamiento.
   - **Ventaja**: Equilibra la estabilidad del gradiente descendente de lote completo y la rapidez del SGD.

### Algoritmos de Optimizaci√≥n Avanzados

1. **Momentum**:
   - **Descripci√≥n**: Agrega una fracci√≥n del gradiente anterior al gradiente actual para acelerar la convergencia y evitar quedarse atrapado en m√≠nimos locales.
   - **Ventaja**: Mejora la velocidad y estabilidad del SGD.
  

2. **RMSprop**:
   - **Descripci√≥n**: Divide la tasa de aprendizaje por una media m√≥vil de la magnitud de los gradientes recientes. Esto ayuda a mantener una tasa de aprendizaje adecuada y evita oscilaciones.
   - **Ventaja**: Mantiene una tasa de aprendizaje adaptativa.
  

3. **Adam (Adaptive Moment Estimation)**:
   - **Descripci√≥n**: Combina las ideas de Momentum y RMSprop. Utiliza medias m√≥viles de los gradientes y sus cuadrados, adaptando as√≠ la tasa de aprendizaje para cada par√°metro.
   - **Ventaja**: Convergencia r√°pida y robusta.
  

4. **AdaGrad**:
   - **Descripci√≥n**: Ajusta la tasa de aprendizaje para cada par√°metro en funci√≥n de los gradientes acumulados pasados. 
   - **Ventaja**: Beneficioso para caracter√≠sticas raras y evita el ajuste excesivo en caracter√≠sticas comunes.
   
### Comparaci√≥n de Algoritmos

- **SGD**: Simple y eficiente para grandes conjuntos de datos, pero puede ser ruidoso.
- **Momentum**: Acelera el SGD y suaviza la convergencia.
- **RMSprop**: Adapta la tasa de aprendizaje, √∫til para problemas con tasas de aprendizaje inestables.
- **Adam**: Combina las ventajas de Momentum y RMSprop, ampliamente utilizado.
- **AdaGrad**: Ajusta la tasa de aprendizaje para cada par√°metro, √∫til para datos dispersos.


### Recursos para Explorar M√°s:

- **[Adam - A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)** - El art√≠culo original que introduce Adam.
- **[Algoritmos de Optimizaci√≥n ](https://youtu.be/1GFu3nOya4c?si=v3jnhocKnb_R0Xw_)** - Explicacion completa (Video).


---

# D√≠a9
---
## Overfitting y T√©cnicas de Regularizaci√≥n üß†üîç

¬°Hola a todos! En el d√≠a 9 de nuestro desaf√≠o #100DaysOfAI, vamos a sumergirnos en el concepto de **overfitting** y las t√©cnicas de **regularizaci√≥n**. Estas son herramientas fundamentales para mejorar la capacidad predictiva y la generalizaci√≥n de nuestros modelos de aprendizaje profundo. ¬°Vamos a explorarlas juntos! üìâüìö

### ¬øQu√© es el Overfitting?

El **overfitting** ocurre cuando nuestro modelo se ajusta demasiado bien a los datos de entrenamiento, capturando no solo la se√±al real sino tambi√©n el ruido. Como resultado, el modelo puede tener un rendimiento deficiente en datos nuevos y no vistos, lo que lleva a una baja capacidad de generalizaci√≥n.

### T√©cnicas de Regularizaci√≥n

1. **Regularizaci√≥n L1 y L2**:
   - **Descripci√≥n**: Agrega un t√©rmino de penalizaci√≥n a la funci√≥n de p√©rdida que es proporcional a la norma L1 o L2 de los pesos del modelo.
   - **Ventaja**: Ayuda a prevenir el overfitting al penalizar los pesos grandes.

2. **Dropout**:
   - **Descripci√≥n**: Aleatoriamente "apaga" una fracci√≥n de las neuronas durante el entrenamiento, lo que obliga al modelo a aprender caracter√≠sticas m√°s robustas y reduce la dependencia entre las neuronas.
   - **Ventaja**: Act√∫a como una forma de regularizaci√≥n al evitar la coadaptaci√≥n de las neuronas.

3. **Data Augmentation**:
   - **Descripci√≥n**: Aumenta el tama√±o del conjunto de datos de entrenamiento aplicando transformaciones como rotaciones, traslaciones y zoom a las im√°genes originales.
   - **Ventaja**: Ayuda a diversificar el conjunto de datos de entrenamiento y a mejorar la generalizaci√≥n del modelo.

4. **Early Stopping**:
   - **Descripci√≥n**: Detiene el entrenamiento del modelo cuando el rendimiento en un conjunto de datos de validaci√≥n deja de mejorar.
   - **Ventaja**: Evita el sobreajuste al detener el entrenamiento antes de que el modelo comience a sobreajustarse a los datos de entrenamiento.

### Aplicaci√≥n en la Pr√°ctica

Para aplicar estas t√©cnicas de regularizaci√≥n en nuestros modelos, debemos ajustar los hiperpar√°metros adecuados y experimentar con diferentes configuraciones para encontrar el equilibrio √≥ptimo entre la capacidad de ajuste y la generalizaci√≥n.

### Recursos para Explorar M√°s:

- **[Overfitting ](https://youtube.com/playlist?list=PLWP2CHQigyUSw1TJkOdAxzBC0BtKrYAnz&si=InFqmXxk1iRgX611)** - Playlists de underfitting y overfitting.
- **[Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)** - El art√≠culo seminal que introduce la t√©cnica de dropout.
- **[T√©cnicas de Regularizaci√≥n](https://youtu.be/qa9M4NBV9Lk?si=G09xw9uQaTsmwmY4)** - Explicaion practica.


---

# D√≠a10
---
## Construyendo una Red Neuronal desde Cero: Clasificaci√≥n de Flores Iris
### Introducci√≥n al Problema y Objetivos

En esta pr√°ctica, vamos a implementar una red neuronal simple desde cero para resolver el problema de clasificaci√≥n de flores Iris. Este es un problema cl√°sico en el aprendizaje autom√°tico y es perfecto para entender los fundamentos de las redes neuronales.

**Objetivo:** Crear una red neuronal que pueda clasificar correctamente las flores Iris en sus tres especies (setosa, versicolor, virginica) bas√°ndose en cuatro caracter√≠sticas: longitud del s√©palo, ancho del s√©palo, longitud del p√©talo y ancho del p√©talo.

**¬øPor qu√© usar redes neuronales?** Las redes neuronales son excelentes para encontrar patrones complejos en los datos. En este caso, pueden aprender las relaciones no lineales entre las caracter√≠sticas de las flores y sus especies, permitiendo una clasificaci√≥n precisa.
![iris_flowers](https://github.com/Oliver369X/100DaysOfAI/assets/110129950/c98d7fec-a4ad-478e-ba49-bdc24b63e98e)

---

## Importaci√≥n de Librer√≠as

Primero, importaremos las librer√≠as necesarias para nuestro proyecto.

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
import matplotlib.pyplot as plt
```

Explicaci√≥n:
- `numpy`: Para operaciones num√©ricas eficientes.
- `sklearn.datasets`: Para cargar el conjunto de datos Iris.
- `sklearn.model_selection`: Para dividir nuestros datos en conjuntos de entrenamiento y prueba.
- `sklearn.preprocessing`: Para codificar nuestras etiquetas.
- `matplotlib.pyplot`: Para visualizar nuestros resultados.

---

## Carga y Preparaci√≥n del Conjunto de Datos

El conjunto de datos Iris es un conjunto cl√°sico en aprendizaje autom√°tico. Contiene 150 muestras de flores Iris, con 50 muestras de cada una de las tres especies.

```python
# Cargar el conjunto de datos Iris
iris = load_iris()
X = iris.data
y = iris.target.reshape(-1, 1)

# One-hot encoding para las etiquetas
encoder = OneHotEncoder(sparse=False)
y = encoder.fit_transform(y)

# Dividir datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Forma de X_train:", X_train.shape)
print("Forma de y_train:", y_train.shape)
print("Forma de X_test:", X_test.shape)
print("Forma de y_test:", y_test.shape)
```

Explicaci√≥n:
- Cargamos el conjunto de datos Iris.
- Aplicamos one-hot encoding a las etiquetas para convertirlas en un formato adecuado para la red neuronal.
- Dividimos los datos en conjuntos de entrenamiento (80%) y prueba (20%).
- Imprimimos las formas de nuestros conjuntos de datos para verificar.

---

## Implementaci√≥n de la Red Neuronal

Ahora, implementaremos nuestra clase de red neuronal simple.

```python
class SimpleNeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        self.W1 = np.random.randn(input_size, hidden_size) * 0.01
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, output_size) * 0.01
        self.b2 = np.zeros((1, output_size))

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def softmax(self, x):
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)
```

Explicaci√≥n:
- Inicializamos los pesos (`W1`, `W2`) y sesgos (`b1`, `b2`) de nuestra red.
- Implementamos la funci√≥n de activaci√≥n sigmoid para la capa oculta.
- Implementamos la funci√≥n softmax para la capa de salida, que nos dar√° probabilidades para cada clase.

---

## Forward Propagation

Implementamos el paso hacia adelante (forward propagation) de nuestra red.

```python
def forward(self, X):
    self.z1 = np.dot(X, self.W1) + self.b1
    self.a1 = self.sigmoid(self.z1)
    self.z2 = np.dot(self.a1, self.W2) + self.b2
    self.a2 = self.softmax(self.z2)
    return self.a2
```

Explicaci√≥n:
- Calculamos la salida de la capa oculta (`z1`) y aplicamos la funci√≥n sigmoid (`a1`).
- Calculamos la salida de la capa final (`z2`) y aplicamos softmax (`a2`).
- Retornamos la salida final, que son las probabilidades para cada clase.

---

## Funci√≥n de P√©rdida

Implementamos la funci√≥n de p√©rdida de entrop√≠a cruzada.

```python
def cross_entropy_loss(self, y_true, y_pred):
    m = y_true.shape[0]
    log_likelihood = -np.log(y_pred[range(m), y_true.argmax(axis=1)])
    loss = np.sum(log_likelihood) / m
    return loss
```

Explicaci√≥n:
- Calculamos la p√©rdida de entrop√≠a cruzada entre las etiquetas verdaderas y las predicciones.
- Esta funci√≥n mide qu√© tan bien nuestras predicciones se ajustan a las etiquetas reales.

---

## Backward Propagation

Implementamos la retropropagaci√≥n (backward propagation) para actualizar los pesos.

```python
def backward(self, X, y, learning_rate):
    m = X.shape[0]
    
    dZ2 = self.a2 - y
    dW2 = np.dot(self.a1.T, dZ2) / m
    db2 = np.sum(dZ2, axis=0, keepdims=True) / m
    
    dZ1 = np.dot(dZ2, self.W2.T) * (self.a1 * (1 - self.a1))
    dW1 = np.dot(X.T, dZ1) / m
    db1 = np.sum(dZ1, axis=0, keepdims=True) / m
    
    self.W2 -= learning_rate * dW2
    self.b2 -= learning_rate * db2
    self.W1 -= learning_rate * dW1
    self.b1 -= learning_rate * db1
```

Explicaci√≥n:
- Calculamos los gradientes para cada capa.
- Actualizamos los pesos y sesgos usando estos gradientes y la tasa de aprendizaje.

---

## Entrenamiento

Implementamos el bucle de entrenamiento.

```python
def train(self, X, y, epochs, learning_rate, batch_size):
    losses = []
    for epoch in range(epochs):
        for i in range(0, X.shape[0], batch_size):
            X_batch = X[i:i+batch_size]
            y_batch = y[i:i+batch_size]
            
            y_pred = self.forward(X_batch)
            loss = self.cross_entropy_loss(y_batch, y_pred)
            self.backward(X_batch, y_batch, learning_rate)
            
        if epoch % 100 == 0:
            losses.append(loss)
            print(f"Epoch {epoch}, Loss: {loss}")
    return losses
```

Explicaci√≥n:
- Entrenamos la red durante un n√∫mero especificado de √©pocas.
- Usamos mini-batch gradient descent para actualizar los pesos.
- Registramos la p√©rdida cada 100 √©pocas para monitorear el progreso.

---

## Evaluaci√≥n

Implementamos funciones para hacer predicciones y calcular la precisi√≥n.

```python
def predict(self, X):
    return np.argmax(self.forward(X), axis=1)

def accuracy(self, X, y):
    predictions = self.predict(X)
    return np.mean(predictions == np.argmax(y, axis=1))
```

Explicaci√≥n:
- `predict`: Hace predicciones para nuevos datos.
- `accuracy`: Calcula la precisi√≥n de nuestras predicciones.

---

## Entrenamiento y Evaluaci√≥n del Modelo

Ahora, entrenamos nuestro modelo y evaluamos su rendimiento.

```python
# Crear y entrenar el modelo
model = SimpleNeuralNetwork(input_size=4, hidden_size=10, output_size=3)
losses = model.train(X_train, y_train, epochs=1000, learning_rate=0.1, batch_size=32)

# Evaluar el modelo
train_accuracy = model.accuracy(X_train, y_train)
test_accuracy = model.accuracy(X_test, y_test)

print(f"Precisi√≥n en entrenamiento: {train_accuracy:.4f}")
print(f"Precisi√≥n en prueba: {test_accuracy:.4f}")
```

Explicaci√≥n:
- Creamos una instancia de nuestra red neuronal.
- Entrenamos el modelo durante 1000 √©pocas.
- Evaluamos la precisi√≥n en los conjuntos de entrenamiento y prueba.

---

## Visualizaci√≥n de Resultados

Finalmente, visualizamos c√≥mo la p√©rdida cambia durante el entrenamiento.

```python
plt.plot(range(0, 1000, 100), losses)
plt.xlabel('√âpocas')
plt.ylabel('P√©rdida')
plt.title('P√©rdida durante el entrenamiento')
plt.show()
```

Explicaci√≥n:
- Graficamos la p√©rdida a lo largo de las √©pocas de entrenamiento.
- Esto nos ayuda a visualizar c√≥mo el modelo aprende con el tiempo.


### Recursos para Explorar M√°s:

- **[An√°lisis exploratorio de datos del conjunto de datos Iris](https://youtu.be/yu4SYEYkZ6U?si=oOb1DEuG5GcS-f4e)** MasterClass (video)
- **[Analisis Exploratorio de Datos dataset Iris](https://www.kaggle.com/code/joeportilla/analisis-exploratorio-de-datos-dataset-iris)** - Notebook Kaggle.

## Colab Notebooks

- [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Qv7LRrhvzGuJPkelYWz9zYJz-oPYIqDJ?usp=sharing) [D√≠a 10: Clasificaci√≥n de Flores Iris](https://colab.research.google.com/drive/1Qv7LRrhvzGuJPkelYWz9zYJz-oPYIqDJ?usp=sharing) 


---

# D√≠a11
---

## Redes Neuronales Artificiales (ANNs) con MNIST

### Introducci√≥n

En este proyecto, exploraremos la estructura b√°sica de las Redes Neuronales Artificiales (ANNs) y su funcionamiento implementando un modelo para clasificar d√≠gitos escritos a mano utilizando el dataset MNIST.

Las ANNs son modelos computacionales inspirados en el cerebro humano. Est√°n dise√±adas para reconocer patrones y resolver problemas complejos a partir de datos. Una ANN t√≠pica consta de tres tipos de capas:
- **Capa de Entrada**: Recibe los datos iniciales.
- **Capas Ocultas**: Procesan la informaci√≥n.
- **Capa de Salida**: Genera el resultado final.

Nuestro objetivo es construir, entrenar y evaluar una ANN usando el dataset MNIST para clasificar im√°genes de d√≠gitos escritos a mano.
![Clasificaci√≥n de Numeros Escritos a Mano](https://github.com/Oliver369X/100DaysOfAI/assets/110129950/4ca7e2c8-2f28-423c-b13b-2fcca5647892)

### Importaci√≥n de Bibliotecas y Dataset

En este punto, importaremos las bibliotecas necesarias y cargaremos el dataset MNIST. Tambi√©n explicaremos el dataset y proporcionaremos el enlace original.

#### Explicaci√≥n del C√≥digo y Dataset

```python
# Importaci√≥n de Bibliotecas
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt
import numpy as np

# Cargando y Preprocesando el Dataset MNIST
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalizaci√≥n de las Im√°genes
x_train = x_train.reshape(-1, 28*28).astype('float32') / 255
x_test = x_test.reshape(-1, 28*28).astype('float32') / 255

# Conversi√≥n de Etiquetas a Categ√≥ricas
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Visualizaci√≥n de ejemplos de im√°genes y sus etiquetas
plt.figure(figsize=(10, 5))
for i in range(10):
    plt.subplot(2, 5, i+1)
    plt.imshow(x_train[i].reshape(28, 28), cmap='gray')
    plt.title(f"Etiqueta: {np.argmax(y_train[i])}")
    plt.axis('off')
plt.show()
```

### Explicaci√≥n del Dataset MNIST

El dataset MNIST (Modified National Institute of Standards and Technology) es una colecci√≥n de im√°genes de d√≠gitos escritos a mano, ampliamente utilizado para entrenar y probar modelos de reconocimiento de im√°genes. El dataset contiene:
- **60,000 im√°genes de entrenamiento**: utilizadas para entrenar el modelo.
- **10,000 im√°genes de prueba**: utilizadas para evaluar el rendimiento del modelo.

Cada imagen tiene un tama√±o de 28x28 p√≠xeles y est√° en escala de grises. Las etiquetas corresponden a d√≠gitos del 0 al 9.

Enlace original al dataset MNIST: [MNIST Database](http://yann.lecun.com/exdb/mnist/)

### Definiendo la Estructura de la Red Neuronal

Ahora definiremos la estructura b√°sica de nuestra red neuronal usando Keras, una biblioteca de alto nivel para redes neuronales.

```python
# Definici√≥n de la Estructura de la Red Neuronal
model = Sequential([
    Dense(512, input_shape=(784,), activation='relu'), # Primera capa oculta con 512 neuronas y ReLU
    Dropout(0.2), # Dropout con el 20% de las neuronas apagadas durante el entrenamiento
    Dense(512, activation='relu'), # Segunda capa oculta con 512 neuronas y ReLU
    Dropout(0.2), # Dropout con el 20% de las neuronas apagadas durante el entrenamiento
    Dense(10, activation='softmax') # Capa de salida con 10 neuronas (una por clase) y Softmax
])

# Compilando el Modelo
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Resumen de la Red Neuronal
model.summary()
```

### Explicaci√≥n de la Estructura de la Red Neuronal

- **Primera Capa Oculta**: Tiene 512 neuronas y utiliza la funci√≥n de activaci√≥n ReLU (Rectified Linear Unit) para introducir no linealidades en el modelo.
- **Dropout**: Aplica Dropout con una tasa del 20% para evitar el sobreajuste.
- **Segunda Capa Oculta**: Similar a la primera, con 512 neuronas y ReLU.
- **Dropout**: Otro Dropout con una tasa del 20%.
- **Capa de Salida**: Tiene 10 neuronas, una para cada clase en el dataset MNIST, y utiliza la funci√≥n de activaci√≥n Softmax para producir probabilidades de clasificaci√≥n.

La red se compila utilizando la p√©rdida de entrop√≠a cruzada categ√≥rica y el optimizador Adam, y se eval√∫a la precisi√≥n durante el entrenamiento.

### Entrenamiento del Modelo

#### Propagaci√≥n Hacia Adelante

La Propagaci√≥n Hacia Adelante es el proceso mediante el cual los datos de entrada se transmiten a trav√©s de la red neuronal para generar una salida. Este flujo de informaci√≥n comienza en la capa de entrada, pasa por las capas ocultas y finalmente llega a la capa de salida.

### Explicaci√≥n del Proceso

1. **Entrada**: Los datos de entrada se presentan a la red neuronal.
2. **Ponderaci√≥n**: Cada neurona en la capa de entrada env√≠a sus datos ponderados a cada neurona en la primera capa oculta.
3. **Activaci√≥n**: Las neuronas en la capa oculta calculan una suma ponderada de sus entradas, aplican una funci√≥n de activaci√≥n y transmiten el resultado a la siguiente capa.
4. **Salida**: Este proceso se repite capa por capa hasta que los datos llegan a la capa de salida, donde se generan las predicciones finales.

```python
# Propagaci√≥n Hacia Adelante usando Keras
# Ya hemos definido y compilado el modelo en el paso anterior
# Entrenamiento del Modelo
history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2, verbose=1)
```

### Visualizaci√≥n del Proceso

Podemos visualizar c√≥mo se transmiten los datos a trav√©s de la red utilizando gr√°ficos de entrenamiento.

```python
# Graficando precisi√≥n y p√©rdida durante el entrenamiento
plt.figure(figsize=(12, 4))
# Precisi√≥n
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Precisi√≥n de Entrenamiento')
plt.plot(history.history['val_accuracy'], label='Precisi√≥n de Validaci√≥n')
plt.title('Precisi√≥n durante el Entrenamiento')
plt.xlabel('√âpoca')
plt.ylabel('Precisi√≥n')
plt.legend()
# P√©rdida
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='P√©rdida de Entrenamiento')
plt.plot(history.history['val_loss'], label='P√©rdida de Validaci√≥n')
plt.title('P√©rdida durante el Entrenamiento')
plt.xlabel('√âpoca')
plt.ylabel('P√©rdida')
plt.legend()
plt.show()
```

### Evaluaci√≥n del Modelo

Evaluaremos el rendimiento del modelo en el conjunto de datos de prueba y mostraremos ejemplos de predicciones.

```python
# La retropropagaci√≥n y el ajuste de pesos se realizan autom√°ticamente durante el entrenamiento
# utilizando el m√©todo fit como se mostr√≥ anteriormente
# Aqu√≠ mostramos la evaluaci√≥n del modelo
loss, accuracy = model.evaluate(x_test, y_test, verbose=0)
print(f'P√©rdida en el conjunto de prueba: {loss:.4f}')
print(f'Precisi√≥n en el conjunto de prueba: {accuracy:.4f}')

import numpy as np
# Haciendo predicciones
predictions = model.predict(x_test)

# Mostrando ejemplos de predicciones
num_rows, num_cols = 2, 5
num_images = num_rows * num_cols
plt.figure(figsize=(10, 5))
for i in range(num_images):
    plt.subplot(num_rows, num_cols, i+1)
    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')
    plt.title(f"Pred: {np.argmax(predictions[i])}")
    plt.axis('off')
plt.show()
```

### T√©cnicas de Regularizaci√≥n

Implementaremos y explicaremos t√©cnicas como Dropout y regularizaci√≥n L2, y mostraremos c√≥mo estas t√©cnicas afectan el rendimiento del modelo.

```python
from keras.layers import Dropout
# Redefiniendo el modelo con Dropout y Regularizaci√≥n L2
from keras.regularizers import l2

model = Sequential([
    Dense(512, activation='relu', input_shape=(784,), kernel_regularizer=l2(0.001)),
    Dropout(0.5),
    Dense(512, activation='relu', kernel_regularizer=l2(0.001)),
    Dropout(0.5),
    Dense(10, activation='softmax')
])
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Entrenando el modelo con regularizaci√≥n
history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2, verbose=1)

# Graficando precisi√≥n y p√©rdida durante el entrenamiento con regularizaci√≥n
plt.figure(figsize=(12, 4))
#

 Precisi√≥n
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Precisi√≥n de Entrenamiento')
plt.plot(history.history['val_accuracy'], label='Precisi√≥n de Validaci√≥n')
plt.title('Precisi√≥n durante el Entrenamiento con Regularizaci√≥n')
plt.xlabel('√âpoca')
plt.ylabel('Precisi√≥n')
plt.legend()
# P√©rdida
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='P√©rdida de Entrenamiento')
plt.plot(history.history['val_loss'], label='P√©rdida de Validaci√≥n')
plt.title('P√©rdida durante el Entrenamiento con Regularizaci√≥n')
plt.xlabel('√âpoca')
plt.ylabel('P√©rdida')
plt.legend()
plt.show()
```

### Recursos para Explorar M√°s:

- **[Hola Mundo del Deep Learning](https://youtube.com/playlist?list=PLWP2CHQigyURotrsA7m39odxXuYAOMvEc&si=nJKJn4Xm1szrwGEZ)** PlayList de 0 a 100 para poder hacer y enteder el hola mundo del Deep Learning
- **[Taller - Fundamentos de Deep Learning con Python y PyTorch](https://youtu.be/XtLpw3SFrz4?si=YeQQu8yB_zmoxcf4)** 

## Colab Notebooks


- [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1jokMDImAKHwhucMxo6ZW1Cs2OXlHUpyR?usp=sharing) [D√≠a 11: Hola Mundo (Deep Learning)](https://colab.research.google.com/drive/1jokMDImAKHwhucMxo6ZW1Cs2OXlHUpyR?usp=sharing)

---
# D√≠a12
---
## ¬øQu√© son las Redes Profundas? üåêüß†


Las **redes profundas**, tambi√©n conocidas como **redes neuronales profundas**, son un tipo de arquitectura de aprendizaje profundo que consta de m√∫ltiples capas de neuronas artificiales. A diferencia de las redes neuronales poco profundas, que tienen solo una o dos capas ocultas, las redes profundas pueden tener muchas capas ocultas, lo que les permite aprender representaciones cada vez m√°s abstractas y complejas de los datos de entrada.


https://github.com/Oliver369X/100DaysOfAI/assets/110129950/10319956-2748-4d00-885a-f9f590ead99f


### Caracter√≠sticas Principales:

1. **Capas Ocultas M√∫ltiples**: Las redes profundas consisten en una serie de capas ocultas entre la capa de entrada y la capa de salida. Cada capa oculta realiza transformaciones no lineales en los datos de entrada, permitiendo que el modelo aprenda caracter√≠sticas jer√°rquicas.

2. **Aprendizaje Jer√°rquico de Caracter√≠sticas**: A medida que los datos fluyen a trav√©s de las capas de la red, se extraen y aprenden caracter√≠sticas cada vez m√°s abstractas y significativas. Esto permite a las redes profundas capturar y modelar relaciones complejas en los datos.

3. **Representaciones de Datos Abstracciones**: Las capas intermedias de una red profunda act√∫an como extractores de caracter√≠sticas, aprendiendo representaciones de datos cada vez m√°s abstractas y de alto nivel. Estas representaciones abstra√≠das son esenciales para la capacidad del modelo de comprender y generalizar a partir de datos no vistos.

### Aplicaciones:

- **Visi√≥n por Computadora**: Las redes profundas han demostrado un rendimiento sobresaliente en tareas como clasificaci√≥n de im√°genes, detecci√≥n de objetos, segmentaci√≥n sem√°ntica y generaci√≥n de im√°genes.

- **Procesamiento del Lenguaje Natural**: En el campo del procesamiento del lenguaje natural (NLP), las redes profundas se utilizan para tareas como clasificaci√≥n de texto, traducci√≥n autom√°tica, generaci√≥n de texto y an√°lisis de sentimientos.

- **Reconocimiento de Voz**: Las redes profundas son fundamentales en sistemas de reconocimiento de voz, donde se utilizan para traducir se√±ales de audio en texto y viceversa.



## Ventajas y Desaf√≠os de Redes M√°s Profundas üåüüß†

Vamos a explorar las ventajas y desaf√≠os asociados con el uso de **redes m√°s profundas** en el aprendizaje profundo. Estas redes neuronales, con m√∫ltiples capas ocultas, han demostrado ser poderosas en la extracci√≥n de caracter√≠sticas complejas de los datos, pero tambi√©n presentan ciertos desaf√≠os que debemos tener en cuenta. ¬°Vamos a sumergirnos en este tema! üöÄüìä

### Ventajas de las Redes M√°s Profundas:

1. **Extracci√≥n Jer√°rquica de Caracter√≠sticas**: Las redes profundas pueden aprender representaciones de datos jer√°rquicas y complejas, lo que les permite capturar caracter√≠sticas abstractas y significativas de los datos de entrada.

2. **Mayor Capacidad de Aprendizaje**: Con m√°s capas ocultas, las redes profundas tienen una mayor capacidad para aprender y modelar relaciones complejas en los datos, lo que puede llevar a un rendimiento mejorado en tareas de aprendizaje autom√°tico.

3. **Generalizaci√≥n Mejorada**: Al aprender representaciones de datos m√°s abstractas y de alto nivel, las redes profundas tienden a generalizar mejor a datos no vistos, lo que les permite realizar predicciones precisas en nuevas instancias.

4. **Rendimiento Superior en Tareas Complejas**: Las redes m√°s profundas han demostrado un rendimiento sobresaliente en una variedad de tareas complejas, como la visi√≥n por computadora, el procesamiento del lenguaje natural y el reconocimiento de voz.

### Desaf√≠os de las Redes M√°s Profundas:

1. **Dificultad de Entrenamiento**: Entrenar redes profundas puede ser computacionalmente costoso y requiere grandes conjuntos de datos etiquetados, as√≠ como una capacidad de c√≥mputo significativa, lo que puede ser un desaf√≠o en entornos con recursos limitados.

2. **Sobreajuste (Overfitting)**: Las redes profundas pueden ser propensas al sobreajuste, especialmente en conjuntos de datos peque√±os o ruidosos, lo que puede resultar en un rendimiento deficiente en datos no vistos.

3. **Gradiente que Desaparece/Explode**: En redes muy profundas, el gradiente puede desvanecerse (cuando se vuelve muy peque√±o) o explotar (cuando se vuelve muy grande) durante el entrenamiento, lo que puede dificultar la convergencia del modelo.

4. **Interpretabilidad Limitada**: A medida que aumenta la complejidad de la red, la interpretaci√≥n de sus decisiones puede volverse m√°s dif√≠cil, lo que puede ser problem√°tico en aplicaciones donde la transparencia y la explicabilidad son importantes.

### Recursos para Explorar M√°s:

- **[¬øCu√°les son los desaf√≠os y limitaciones actuales de las redes neuronales y el aprendizaje profundo?](https://www.linkedin.com/advice/3/what-current-challenges-limitations-neural?lang=es&originalSubdomain=es)**.





---

# D√≠a13
---
## Conceptos b√°sicos y arquitectura general de las CNNs üß†üñºÔ∏è


https://github.com/Oliver369X/100DaysOfAI/assets/110129950/f76483f7-fe9f-4c48-8e66-bc8ab8d8d360


1Ô∏è‚É£ Definici√≥n de CNN ü§ñ
Las Redes Neuronales Convolucionales son un tipo especializado de red neuronal dise√±ada principalmente para procesar datos con estructura de cuadr√≠cula, como im√°genes. Se inspiran en el procesamiento visual del cerebro humano y son muy eficaces en tareas de visi√≥n por computador. üëÅÔ∏è‚Äçüó®Ô∏è

2Ô∏è‚É£ Componentes principales de una CNN üß±
a) Capa de entrada: Recibe la imagen como tensor 3D
b) Capas convolucionales: Aplican filtros para detectar caracter√≠sticas
c) Funciones de activaci√≥n: Introducen no-linealidad (t√≠picamente ReLU)
d) Capas de pooling: Reducen la dimensionalidad espacial
e) Capa de aplanamiento: Convierte datos en vector unidimensional
f) Capas completamente conectadas: Realizan la clasificaci√≥n final
g) Capa de salida: Produce la predicci√≥n final

3Ô∏è‚É£ Proceso de convoluci√≥n üîÑ
- Operaci√≥n fundamental en CNNs
- Un filtro se desliza sobre la imagen de entrada
- Multiplicaci√≥n elemento por elemento y suma del resultado
- Crea un mapa de caracter√≠sticas que resalta patrones espec√≠ficos

4Ô∏è‚É£ Caracter√≠sticas clave de las CNNs üîë
a) Conectividad local: Cada neurona se conecta solo a una regi√≥n local
b) Compartici√≥n de par√°metros: Mismos pesos en m√∫ltiples ubicaciones
c) Invariancia a la traslaci√≥n: Detectan caracter√≠sticas independientemente de su posici√≥n

### Recursos para Explorar M√°s:

- **[funcionamiento de las redes neuronales convolucionales](https://youtu.be/4sWhhQwHqug?si=qvxBksruxjAbWVkC)** 
- **[¬°Redes Neuronales CONVOLUCIONALES! ¬øC√≥mo funcionan?](https://youtu.be/V8j1oENVz00?si=1PNlj6GPLEqP66sZ)**

---

# D√≠a14
----
## ¬øC√≥mo funcionan las CNNs en comparaci√≥n con las ANNs? ü§îüîç
Vamos a explorar c√≥mo funcionan las Redes Neuronales Convolucionales (CNNs) en comparaci√≥n con las Redes Neuronales Artificiales (ANNs). Ambas son arquitecturas importantes en el campo del aprendizaje profundo, pero tienen diferencias clave en su estructura y funcionamiento. ¬°Vamos a analizarlas! üß†üìä

### Redes Neuronales Artificiales (ANNs):

Las Redes Neuronales Artificiales (ANNs), tambi√©n conocidas como perceptrones multicapa, son una arquitectura cl√°sica de redes neuronales que consiste en m√∫ltiples capas de neuronas artificiales interconectadas. Cada neurona en una capa est√° conectada a todas las neuronas de la capa siguiente, lo que permite una representaci√≥n compleja de funciones no lineales.

**Funcionamiento:**
1. **Propagaci√≥n hacia Adelante (Forward Propagation):** Durante la propagaci√≥n hacia adelante, los datos de entrada se alimentan a trav√©s de la red neuronal, capa por capa, y se calculan las activaciones de cada neurona utilizando una combinaci√≥n lineal de las entradas y pesos, seguida de una funci√≥n de activaci√≥n no lineal.

2. **C√°lculo del Error:** Despu√©s de la propagaci√≥n hacia adelante, se compara la salida predicha de la red con la salida deseada utilizando una funci√≥n de p√©rdida, y se calcula el error de predicci√≥n.

3. **Propagaci√≥n hacia Atr√°s (Backward Propagation):** Durante la propagaci√≥n hacia atr√°s, el error calculado se propaga hacia atr√°s a trav√©s de la red para ajustar los pesos de cada neurona, utilizando algoritmos de optimizaci√≥n como el descenso de gradiente estoc√°stico (SGD).

### Redes Neuronales Convolucionales (CNNs):

Las Redes Neuronales Convolucionales (CNNs) son una variante especializada de las ANNs dise√±adas espec√≠ficamente para el procesamiento de im√°genes. Integran capas convolucionales que aplican filtros a las im√°genes de entrada para extraer caracter√≠sticas relevantes de manera eficiente.

**Principales Diferencias:**
1. **Estructura:** Mientras que las ANNs est√°n completamente conectadas, las CNNs utilizan capas convolucionales y de pooling para operar directamente sobre las caracter√≠sticas de la imagen, lo que reduce dr√°sticamente el n√∫mero de par√°metros y la complejidad computacional.

2. **Convoluci√≥n:** Las CNNs utilizan operaciones de convoluci√≥n para detectar caracter√≠sticas locales en las im√°genes, lo que les permite capturar patrones espaciales y de proximidad que son fundamentales en tareas de visi√≥n por computadora.

3. **Par√°metros Compartidos:** En las CNNs, los mismos pesos de filtro se comparten en diferentes regiones de la imagen, lo que les permite generalizar y aprender patrones independientemente de su ubicaci√≥n en la imagen.

### Aplicaciones:
- Las ANNs son m√°s adecuadas para tareas de aprendizaje supervisado en datos tabulares o secuenciales.
- Las CNNs son ideales para tareas de visi√≥n por computadora, como reconocimiento de objetos, detecci√≥n de objetos, segmentaci√≥n sem√°ntica y m√°s.

### Recursos para Explorar M√°s:
- **[CNN vs RNN vs ANN: Explicando las redes neuronales](https://www.linkedin.com/advice/0/how-do-you-explain-concepts-intuitions-behind?lang=es&originalSubdomain=es)**.


---
# D√≠a15
---
## Ejemplos Pr√°cticos de Aplicaci√≥n en la Industria üè≠ü§ñ

Vamos a explorar algunos ejemplos pr√°cticos de c√≥mo se aplican las redes neuronales convolucionales (CNNs) en la industria. Las CNNs son una poderosa herramienta en el campo del aprendizaje profundo, especialmente en aplicaciones de visi√≥n por computadora. Veamos algunos ejemplos interesantes:


https://github.com/Oliver369X/100DaysOfAI/assets/110129950/996d7620-d10f-40dd-b6ae-329945c07cd0


#### 1. Diagn√≥stico M√©dico:
- **Detecci√≥n de C√°ncer de Mama:** Las CNNs pueden analizar im√°genes de mamograf√≠as para detectar signos tempranos de c√°ncer de mama, ayudando a los m√©dicos en el diagn√≥stico precoz y la planificaci√≥n del tratamiento.

#### 2. Automatizaci√≥n Industrial:
- **Inspecci√≥n de Calidad en Manufactura:** Las CNNs pueden inspeccionar visualmente productos manufacturados en busca de defectos o imperfecciones, garantizando la calidad del producto final y reduciendo el desperdicio.

#### 3. Automotriz y Conducci√≥n Aut√≥noma:
- **Detecci√≥n de Peatones y Objetos:** Las CNNs integradas en sistemas de conducci√≥n aut√≥noma pueden identificar peatones, veh√≠culos y otros objetos en tiempo real, permitiendo que los veh√≠culos tomen decisiones de conducci√≥n seguras.

#### 4. Agricultura de Precisi√≥n:
- **Monitoreo de Cultivos:** Las CNNs pueden analizar im√°genes satelitales para monitorear el crecimiento de los cultivos, identificar √°reas de estr√©s vegetal y optimizar el uso de recursos agr√≠colas como el agua y los fertilizantes.

#### 5. Seguridad y Vigilancia:
- **Reconocimiento Facial y de Objeto:** Las CNNs pueden analizar im√°genes de c√°maras de seguridad para identificar caras de inter√©s, detectar intrusiones no autorizadas y alertar sobre actividades sospechosas.

#### 6. Retail y Experiencia del Cliente:
- **Personalizaci√≥n de Recomendaciones:** Las CNNs pueden analizar el historial de compras y las preferencias del cliente para ofrecer recomendaciones de productos altamente personalizadas, mejorando la experiencia de compra en l√≠nea.

## Recursos sobre Aplicaciones Pr√°cticas de Redes Neuronales Convolucionales (CNNs):

### **1. Diagn√≥stico M√©dico:**

* **Detecci√≥n de C√°ncer de Mama:**
    * **Art√≠culo:** "Aplicaci√≥n de redes neuronales convolucionales para la detecci√≥n de c√°ncer de mama en im√°genes de mamograf√≠a" ([https://revistascientificas.cuc.edu.co/CESTA/article/view/3922/3919](https://revistascientificas.cuc.edu.co/CESTA/article/view/3922/3919))
    * **Video:** "Redes Neuronales Convolucionales para Detecci√≥n de C√°ncer de Mama" ([https://www.youtube.com/watch?v=06TugnwqZCQ](https://www.youtube.com/watch?v=06TugnwqZCQ))

### **2. Automatizaci√≥n Industrial:**

* **Inspecci√≥n de Calidad en Manufactura:**
    * **Art√≠culo:** "Inspecci√≥n de defectos en productos manufacturados utilizando redes neuronales convolucionales" ([http://www.scielo.org.co/scielo.php?script=sci_arttext&pid=S0121-750X2023000100204](http://www.scielo.org.co/scielo.php?script=sci_arttext&pid=S0121-750X2023000100204))
    * **Video:** "Automatizaci√≥n de la Inspecci√≥n Visual en la Industria Manufacturera con Redes Neuronales Convolucionales" ([https://www.youtube.com/watch?v=FkvWe00Pjgs](https://www.youtube.com/watch?v=FkvWe00Pjgs))

### **3. Automotriz y Conducci√≥n Aut√≥noma:**

* **Detecci√≥n de Peatones y Objetos:**

    * **Video:** "Visi√≥n Artificial para Veh√≠culos Aut√≥nomos: Detecci√≥n de Peatones y Objetos con Redes Neuronales Convolucionales" ([https://www.youtube.com/watch?v=WC8dm4dxqPw](https://www.youtube.com/watch?v=WC8dm4dxqPw))

---

# D√≠a16
---
## Comprendiendo la Convoluci√≥n en Im√°genes üì∏üîç

#### ¬øQu√© es la Convoluci√≥n?
La convoluci√≥n es una operaci√≥n matem√°tica fundamental en el procesamiento de se√±ales y el aprendizaje profundo. En el contexto de las im√°genes, la convoluci√≥n implica deslizar una peque√±a ventana (llamada kernel o filtro) sobre la imagen de entrada y realizar operaciones matem√°ticas en cada regi√≥n de la imagen.

#### Aplicaci√≥n en Im√°genes:
- **Extracci√≥n de Caracter√≠sticas:** La convoluci√≥n se utiliza para extraer caracter√≠sticas importantes de una imagen, como bordes, texturas y patrones, mediante la detecci√≥n de caracter√≠sticas locales en diferentes partes de la imagen.
- **Reducci√≥n de Dimensionalidad:** Al aplicar convoluciones sucesivas con diferentes filtros, se obtienen mapas de caracter√≠sticas que resumen la informaci√≥n clave de la imagen, lo que permite una representaci√≥n m√°s compacta y manejable para la red neuronal.
- **Detecci√≥n de Objetos:** En el contexto del aprendizaje profundo, las convoluciones son fundamentales en las arquitecturas de redes neuronales convolucionales (CNNs) para la detecci√≥n y clasificaci√≥n de objetos en im√°genes.

#### Proceso de Convoluci√≥n:
1. **Deslizamiento del Kernel:** El kernel se desliza sobre la imagen de entrada, multiplicando sus valores por los p√≠xeles correspondientes en cada regi√≥n.
2. **Operaci√≥n de Producto Punto:** Se calcula el producto punto entre los valores del kernel y los p√≠xeles de la regi√≥n de la imagen.
3. **Suma y Bias:** Se suman los resultados de la operaci√≥n de producto punto y se agrega un t√©rmino de sesgo (bias).
4. **Aplicaci√≥n de Funci√≥n de Activaci√≥n:** Opcionalmente, se aplica una funci√≥n de activaci√≥n no lineal, como ReLU, para introducir no linealidades en la red.

### Recursos para Explorar M√°s:
- **[La CONVOLUCI√ìN en las REDES CONVOLUCIONALES](https://youtu.be/ySbmdeqR0-4?si=_lp6W3jjBWVu0E5e)**.
- **[Convoluciones y filtros](https://youtu.be/AwTH_0yW9_I?si=2EuPLMROMmReZR1T)**.

---

# D√≠a17
---
## Entendiendo los Filtros y su Papel en la Extracci√≥n de Caracter√≠sticas üåüüîç

Hoy vamos a explorar m√°s a fondo los filtros en el contexto de las redes neuronales convolucionales (CNNs) y c√≥mo desempe√±an un papel crucial en la extracci√≥n de caracter√≠sticas de las im√°genes.

#### ¬øQu√© son los Filtros en CNNs?
Los filtros, tambi√©n conocidos como kernels, son matrices peque√±as de pesos que se utilizan en las capas convolucionales de las CNNs. Cada filtro se desliza sobre la imagen de entrada y realiza operaciones de convoluci√≥n para extraer caracter√≠sticas espec√≠ficas.

#### Funci√≥n de los Filtros:
- **Detecci√≥n de Caracter√≠sticas:** Cada filtro est√° dise√±ado para detectar una caracter√≠stica espec√≠fica en la imagen, como bordes, texturas, formas o patrones.
- **Aprendizaje de Caracter√≠sticas:** Durante el entrenamiento de la red neuronal, los valores de los filtros se ajustan autom√°ticamente para aprender las caracter√≠sticas m√°s relevantes para la tarea espec√≠fica.

#### Proceso de Extracci√≥n de Caracter√≠sticas:
1. **Convoluci√≥n:** El filtro se aplica a la imagen de entrada mediante la operaci√≥n de convoluci√≥n, multiplicando sus valores por los p√≠xeles correspondientes y sumando los resultados.
2. **Mapa de Activaci√≥n:** La salida de la convoluci√≥n se conoce como mapa de activaci√≥n, que resalta la presencia de la caracter√≠stica detectada en diferentes regiones de la imagen.
3. **Pooling:** Opcionalmente, se puede aplicar una capa de pooling despu√©s de la convoluci√≥n para reducir la dimensionalidad y mejorar la eficiencia computacional.



#### Importancia en el Aprendizaje Profundo:
- Los filtros son esenciales para el aprendizaje profundo, ya que permiten que la red neuronal aprenda representaciones jer√°rquicas de las caracter√≠sticas de las im√°genes.
- Al apilar capas convolucionales con diferentes filtros, la red puede aprender caracter√≠sticas cada vez m√°s abstractas y complejas, lo que mejora su capacidad para realizar tareas de visi√≥n por computadora.


### Recursos para Explorar M√°s:
- **[Filtros espaciales aplicados a im√°genes](https://youtu.be/K9Tx4NOWUSg?si=4UdJDFUQuzCJRTJJ)**.

---

# D√≠a18
---
## Stride y Padding en CNNs üö∂üèª‚Äç‚ôÇÔ∏èüõå

Hoy vamos a explorar dos conceptos importantes en las redes neuronales convolucionales (CNNs): Stride y Padding. Estos conceptos son fundamentales para el dise√±o y la configuraci√≥n de las capas convolucionales.

#### Stride:
- **Definici√≥n:** El stride (paso) es la cantidad de p√≠xeles que el filtro se desplaza en cada paso mientras se aplica a la imagen de entrada.
- **Efecto:** Un stride mayor reduce la dimensi√≥n espacial de la salida (mapa de activaci√≥n), ya que el filtro se mueve m√°s r√°pido a lo largo de la imagen.
- **Control de Dimensionalidad:** El stride se utiliza para controlar la reducci√≥n de dimensionalidad en las capas convolucionales, lo que puede ser √∫til para reducir el costo computacional y el overfitting.

#### Padding:
- **Definici√≥n:** El padding (relleno) consiste en agregar p√≠xeles adicionales alrededor de la imagen de entrada antes de aplicar la convoluci√≥n.
- **Uso:** El padding se utiliza para mantener la dimensi√≥n espacial de la salida despu√©s de la convoluci√≥n, especialmente en los bordes de la imagen.
- **Beneficios:** Al agregar padding, se conserva m√°s informaci√≥n espacial de la imagen de entrada y se evita la p√©rdida de caracter√≠sticas en los bordes.
- **Tipos de Padding:** Se pueden utilizar diferentes tipos de padding, como "same" (mismo tama√±o de entrada y salida) o "valid" (sin relleno), seg√∫n los requisitos de la arquitectura de la red.




#### Importancia en las CNNs:
- El stride y el padding son par√°metros importantes que afectan la dimensi√≥n espacial de la salida y la cantidad de informaci√≥n preservada.
- Ajustar adecuadamente el stride y el padding puede mejorar el rendimiento y la eficiencia de la red neuronal convolucional en tareas de visi√≥n por computadora.

### Recursos para Explorar M√°s:
- **[Padding, strides, max pooling y stacking en las REDES CONVOLUCIONALES](https://youtu.be/QLy8v6LL_4A?si=6ElSwovGCi-Eljj3)**.

---

# D√≠a19
---
## Pooling en CNNs üèä‚Äç‚ôÇÔ∏èüîç

¬°Hola a todos! Hoy vamos a explorar una t√©cnica fundamental en las redes neuronales convolucionales (CNNs): el Pooling. El Pooling es una operaci√≥n importante para la reducci√≥n de dimensionalidad y la extracci√≥n de caracter√≠sticas en las CNNs.

![Pooling](https://github.com/Oliver369X/100DaysOfAI/assets/110129950/9e06b087-f42c-4ce3-af88-cbfc80ce9d82)
![pooling](https://github.com/Oliver369X/100DaysOfAI/assets/110129950/38bc0a1a-fc89-4c66-94d4-5c43a2443bb9)

#### Introducci√≥n al Pooling:
- **Definici√≥n:** El Pooling es una operaci√≥n que reduce la dimensionalidad de cada mapa de activaci√≥n, conservando solo la informaci√≥n m√°s importante.
- **Tipos de Pooling:** Los tipos comunes de Pooling son el Max Pooling y el Average Pooling.
- **Funcionamiento:** En el Max Pooling, se selecciona el valor m√°ximo de un √°rea definida en el mapa de activaci√≥n. En el Average Pooling, se calcula el promedio de los valores en el √°rea especificada.
- **Reducci√≥n de Dimensionalidad:** El Pooling reduce el tama√±o espacial de la entrada, lo que disminuye el n√∫mero de par√°metros y operaciones en la red neuronal.

#### Impacto en las CNNs:
- **Reducci√≥n de Overfitting:** Al reducir la dimensionalidad, el Pooling ayuda a prevenir el overfitting al eliminar informaci√≥n redundante y mejorar la generalizaci√≥n del modelo.
- **Invariancia a las Transformaciones:** El Pooling hace que la red sea m√°s invariante a peque√±as traslaciones y deformaciones en las caracter√≠sticas detectadas.
- **Extracci√≥n de Caracter√≠sticas:** Al conservar solo las caracter√≠sticas m√°s importantes, el Pooling facilita la identificaci√≥n de patrones relevantes en los mapas de activaci√≥n.



En la imagen de arriba, se muestra un ejemplo de Max Pooling aplicado a un mapa de activaci√≥n. La regi√≥n de 2x2 se desliza sobre el mapa, seleccionando el valor m√°ximo en cada regi√≥n para formar la salida.

### Recursos para Explorar M√°s:
- **[CNN vs RNN vs ANN: Explicando las redes neuronales](https://www.linkedin.com/advice/0/how-do-you-explain-concepts-intuitions-behind?lang=es&originalSubdomain=es)**.
- **[Capas de pooling en una red neuronal convolucional](https://keepcoding.io/blog/capas-pooling-red-neuronal-convolucional/)**.
- **[Pooling and their types in CNN
](https://medium.com/@abhishekjainindore24/pooling-and-their-types-in-cnn-4a4b8a7a4611)**.

---

# D√≠a20
# D√≠a21
# D√≠a22
# D√≠a23
# D√≠a24
# D√≠a25
# D√≠a26
# D√≠a27
# D√≠a28
# D√≠a29
# D√≠a30
# D√≠a31
# D√≠a32
# D√≠a33
# D√≠a34
# D√≠a35
# D√≠a36
# D√≠a37
# D√≠a38
# D√≠a39
# D√≠a40
# D√≠a41
# D√≠a42
# D√≠a43
# D√≠a44
# D√≠a45
# D√≠a46
# D√≠a47
# D√≠a48
# D√≠a49
# D√≠a50
# D√≠a51
# D√≠a52
# D√≠a53
# D√≠a54
# D√≠a55
# D√≠a56
# D√≠a57
# D√≠a58
# D√≠a59
# D√≠a60
# D√≠a61
# D√≠a62
# D√≠a63
# D√≠a64
# D√≠a65
# D√≠a66
# D√≠a67
# D√≠a68
# D√≠a69
# D√≠a70
# D√≠a71
# D√≠a72
# D√≠a73
# D√≠a74
# D√≠a75
# D√≠a76
# D√≠a77
# D√≠a78
# D√≠a79
# D√≠a80
# D√≠a81
# D√≠a82
# D√≠a83
# D√≠a84
# D√≠a85
# D√≠a86
# D√≠a87
# D√≠a88
# D√≠a89
# D√≠a90
# D√≠a91
# D√≠a92
# D√≠a93
# D√≠a94
# D√≠a95
# D√≠a96
# D√≠a97
# D√≠a98
# D√≠a99
# D√≠a100
