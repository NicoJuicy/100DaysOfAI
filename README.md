# 100 Días de IA

| Libros y Recursos | Estado de Finalización |
| ----- | -----|
| 1. [**Machine Learning Specialization**](https://www.coursera.org/specializations/machine-learning-introduction?page=1) | La "Especialización en Aprendizaje Automático" es un programa en línea de 3 cursos creado por DeepLearning.AI y Stanford Online, dirigido por Andrew Ng. Está diseñado para principiantes y ofrece una introducción completa al aprendizaje automático moderno. Los estudiantes aprenderán sobre aprendizaje supervisado, como la regresión y redes neuronales, y no supervisado, como agrupación y sistemas de recomendación. El curso también cubre las mejores prácticas en IA utilizadas en la industria. |
| 2. [**Deep Learning Specialization**](https://www.coursera.org/specializations/deep-learning?)| La "Especialización en Aprendizaje Profundo" es un programa de 5 cursos que te capacitará para comprender y aplicar redes neuronales avanzadas. Aprenderás a construir y entrenar arquitecturas como redes convolucionales, recurrentes, LSTMs y transformadores, utilizando Python y TensorFlow. Además, adquirirás habilidades para mejorar modelos con técnicas como Dropout y BatchNorm, y aplicar el aprendizaje profundo en áreas como reconocimiento de voz, procesamiento de lenguaje natural y síntesis musical. Este curso te preparará para enfrentar desafíos industriales y avanzar en tu carrera en el campo de la IA. |
| 3. [**IA generativa con grandes modelos lingüísticos**](https://www.coursera.org/learn/generative-ai-with-llms/) |El curso "Generative AI with Large Language Models (LLMs)" te enseña los fundamentos de la IA generativa y cómo aplicarla en situaciones reales. Aprenderás a comprender el ciclo de vida de un modelo basado en LLM, desde la recopilación de datos hasta su implementación. Además, explorarás la arquitectura de transformadores, el ajuste fino de modelos, y cómo optimizar su rendimiento utilizando leyes de escalado.  |
| 4. [**Curso de Deep Learning**](https://youtube.com/playlist?list=PLcfxtMhW8iFNMTFKrYMYYzVTNzu-xG-Ys&si=lqAlbDIhtOJ5zMP8) | Este curso de Deep Learning en español, disponible en YouTube, abarca desde conceptos básicos de Machine Learning hasta temas avanzados de Deep Learning, utilizando PyTorch como la librería principal. A lo largo de las clases, se exploran redes neuronales simples, regresión lineal, clasificación con Softmax, redes multicapa (MLP), retropropagación, y el uso de GPU con PyTorch. Además, se cubren técnicas de regularización, validación cruzada, y optimización. También se profundiza en redes neuronales recurrentes (RNN), embeddings de palabras, modelos de secuencia a secuencia (Seq2Seq), transformers, redes convolucionales (CNN), segmentación semántica y redes generativas adversarias (GANs), proporcionando una base sólida tanto teórica como práctica para el desarrollo de proyectos de Deep Learning. |
| 5. [**Computer Vision**](https://youtube.com/playlist?list=PLISuMnTdVU-yvm6X7SwKtUosfr4ZarStU&si=FOMUjJ5SvotgMhHW) | Esta serie de clases de Computer Vision en español, ofrecida por el Instituto Humai, cubre desde los fundamentos del procesamiento de imágenes con OpenCV hasta técnicas avanzadas de visión por computadora. A lo largo del curso, se exploran temas como convoluciones, arquitecturas clásicas de redes neuronales convolucionales (AlexNet, VGG, GoogLeNet, ResNet), visualización de características, transferencia de conocimiento, fine-tuning, y transferencia de estilos. También se abordan técnicas más avanzadas como detección de objetos, segmentación semántica, convoluciones transpuestas, redes totalmente convolucionales (FCN), y redes generativas adversarias (GANs) |


| Proyectos Completados |
| ----------------- |
| [1. Clasificación de Flores Iris](https://colab.research.google.com/drive/1Qv7LRrhvzGuJPkelYWz9zYJz-oPYIqDJ?usp=sharing)  |
| [2. Hola Mundo (Deep Learning)](https://colab.research.google.com/drive/1jokMDImAKHwhucMxo6ZW1Cs2OXlHUpyR?usp=sharing) |
| [3. Clasificador de perros y gatos](https://colab.research.google.com/drive/1Efva3sau54WHusFRnfcFxL-9sLuO27L0) |
| 4.  |

# Temas Cubiertos en Cada Día
| **Días** | **Temas Cubiertos** | 
|--------- | ------------------ |
| [Día1](#Día1) | Introducción a Deep Learning | 
| [Día2](#Día2) | Historia y Evolución de Deep Learning | 
| [Día3](#Día3) | Breve Descripción de las Diferentes Técnicas en Deep Learning | 
| [Día4](#Día4) | Comparación y Aplicaciones de Técnicas de Deep Learning en el Mundo Real | 
| [Día5](#Día5) | Redes Neuronales Artificiales (ANNs) | 
| [Día6](#Día6) | Forward y Backward Propagation | 
| [Día7](#Día7) | Coste y Funciones de Pérdida | 
| [Día8](#Día8) | Algoritmos de Optimización | 
| [Día9](#Día9) | Overfitting y Técnicas de Regularización | 
| [Día10](#Día10) | Construyendo una Red Neuronal desde Cero: Clasificación de Flores Iris | 
| [Día11](#Día11) | Construyendo una Red Neuronal con Tensorflow: Clasificación de Digitos Escritos a Mano | 
| [Día12](#Día12) | Redes Neuronales Profundas | 
| [Día13](#Día13) | Conceptos básicos y arquitectura general de las CNNs | 
| [Día14](#Día14) | ¿Cómo funcionan las CNNs en comparación con las ANNs? | 
| [Día15](#Día15) | Ejemplos Prácticos de Aplicación en la Industria | 
| [Día16](#Día16) | Comprendiendo la Convolución en Imágenes | 
| [Día17](#Día17) | Entendiendo los Filtros y su Papel en la Extracción de Características | 
| [Día18](#Día18) | Stride y Padding en CNNs | 
| [Día19](#Día19) | Pooling en CNNs | 
| [Día20](#Día20) | Funciones de Activación | 
| [Día21](#Día21) | Construcción de Capas en CNNs | 
| [Día22](#Día22) | Capas Completamente Conectadas (Fully Connected Layers) | 
| [Día23](#Día23) | Regularización en CNNs | 
| [Día24](#Día24) | Backpropagation en CNNs | 
| [Día25](#Día25) | Actualización de Pesos y Ajuste de Filtros | 
| [Día26](#Día26) | Clasificador de perros y gatos | 
| [Día27](#Día27) | Explorando arquitecturas influyentes en el aprendizaje profundo | 
| [Día28](#Día28) | Arquitecturas Específicas en Visión por Computadora | 
| [Día29](#Día29) | Concepto de Transfer Learning | 
| [Día30](#Día30) | Técnicas de Transfer Learning | 
| [Día31](#Día31) | Detección de Objetos | 
| [Día32](#Día32) | Evolución de YOLO: Desde 2015 hasta 2024 | 
| [Día33](#Día33) | YOLOv8 y sus Variantes con Ultralytics | 
| [Día34](#Día34) | Aplicaciones Avanzadas de Detección de Objetos | 
| [Día35](#Día35) | Técnicas de Mejora de Precisión en Detección de Objetos | 
| [Día36](#Día36) | Segmentación de Imágenes | 
| [Día37](#Día37) | Implementación de Segmentación de Imágenes con YOLO | 
| [Día38](#Día38) | Introducción a los Modelos Preentrenados | 
| [Día39](#Día39) | Explorando los Avances en Detección de Objetos con YOLOv5, YOLOv8 y YOLOv10 | 
| [Día40](#Día40) | RT-DETR revoluciona la detección de objetos en tiempo real | 
| [Día41](#Día41) | Explorando U-Net: un hito en la segmentación de imágenes | 
| [Día42](#Día42) | Inferencia  con YOLOv8 sobre Santa Cruz de la Sierra | 
| [Día43](#Día43) | Mapas de Calor con Ultralytics YOLOv8 | 
| [Día44](#Día44) | Recuento de Objetos Mediante Ultralytics YOLOv8 | 
| [Día45](#Día45) | Sistema de Alarma de Seguridad con YOLOv8 | 
| [Día46](#Día46) | Gestión de Colas con YOLOv8 | 
| [Día47](#Día47) | Gestión de Aparcamientos Mediante Ultralytics YOLOv8 | 
| [Día48](#Día48) | Combatiendo Incendios Forestales con IA | 
| [Día49](#Día49) | Agricultura Inteligente con IA | 
| [Día50](#Día50) | Introducción a NLP: Definición, aplicaciones e historia | 
| [Día51](#Día51) | Tokenización, Lematización y Stemming | 
| [Día52](#Día52) | Preprocesamiento de texto y normalización. | 
| [Día53](#Día53) |  | 
| [Día54](#Día54) |  | 
| [Día55](#Día55) |  | 
| [Día56](#Día56) |  | 
| [Día57](#Día57) |  | 
| [Día58](#Día58) |  | 
| [Día59](#Día59) |  | 
| [Día60](#Día60) |  | 
| [Día61](#Día61) |  | 
| [Día62](#Día62) |  | 
| [Día63](#Día63) |  | 
| [Día64](#Día64) |  | 
| [Día65](#Día65) |  | 
| [Día66](#Día66) |  | 
| [Día67](#Día67) |  | 
| [Día68](#Día68) |  | 
| [Día69](#Día69) |  | 
| [Día70](#Día70) |  | 
| [Día71](#Día71) |  | 
| [Día72](#Día72) |  | 
| [Día73](#Día73) |  | 
| [Día74](#Día74) |  | 
| [Día75](#Día75) |  | 
| [Día76](#Día76) |  | 
| [Día77](#Día77) |  | 
| [Día78](#Día78) |  | 
| [Día79](#Día79) |  | 
| [Día80](#Día80) |  | 
| [Día81](#Día81) |  | 
| [Día82](#Día82) |  | 
| [Día83](#Día83) |  | 
| [Día84](#Día84) |  | 
| [Día85](#Día85) |  | 
| [Día86](#Día86) |  | 
| [Día87](#Día87) |  | 
| [Día88](#Día88) |  | 
| [Día89](#Día89) |  | 
| [Día90](#Día90) |  | 
| [Día91](#Día91) |  | 
| [Día92](#Día92) |  | 
| [Día93](#Día93) |  | 
| [Día94](#Día94) |  | 
| [Día95](#Día95) |  | 
| [Día96](#Día96) |  | 
| [Día97](#Día97) |  | 
| [Día98](#Día98) |  | 
| [Día99](#Día99) |  | 
| [Día100](#Día100) |  | 

# Día1
---
## Introducción a Deep Learning 🌟

¡Bienvenidos al primer día de mi viaje de 100 días explorando la Inteligencia Artificial! 🚀 Hoy comenzamos con **Deep Learning**.

### ¿Qué es Deep Learning?

Deep Learning, o Aprendizaje Profundo, es una rama avanzada del **Machine Learning** que se inspira en la estructura y función del cerebro humano. Utiliza **redes neuronales artificiales** para aprender de grandes volúmenes de datos y tomar decisiones o hacer predicciones precisas.

### ¿Por qué es importante?

En los últimos años, el Deep Learning ha revolucionado muchas industrias. Desde la **visión por computadora** que permite a los vehículos autónomos ver el mundo, hasta el **procesamiento de lenguaje natural** que ayuda a las máquinas a entender y responder en lenguaje humano. Deep Learning es la tecnología detrás de innovaciones impresionantes que están cambiando la forma en que interactuamos con el mundo digital.

### ¿Cómo funciona?

Las redes neuronales profundas están compuestas por capas de neuronas artificiales. Cada capa transforma la entrada de datos en algo más útil para la siguiente capa. A través de un proceso de entrenamiento, estas redes aprenden a extraer características complejas y patrones directamente de los datos.

### Ejemplos de Aplicaciones de Deep Learning:

- **Reconocimiento de Imágenes**: Identificar objetos y personas en fotos y videos.
- **Traducción Automática**: Convertir texto de un idioma a otro con gran precisión.
- **Diagnóstico Médico**: Analizar imágenes médicas para detectar enfermedades.



 **Recursos para comenzar**🧠:
- **[APRENDIZAJE PROFUNDO EN INTELIGENCIA ARTIFICIAL](https://youtu.be/Zcb8R2TF3bI?si=f1NIEJgXh7cWdadV)** - Una breve esplicacion dew que es deep learning.
- **[¿QUE ES EL DEEP LEARNING? - EXPLICADO MUY FACIL](https://youtu.be/s0SbvGiG28w?si=Rr51xld8H8ilsrz9)** - Video de Dalto explicando que es deep learning.
- **[¿Qué son el MACHINE LEARNING y el DEEP LEARNING?](https://youtu.be/HMEjoBnCc9c?si=U5MXn98cY7Yovy8w)** - Diferencias entre el Machine Learning y el Deep Learning.
- **[¿De qué es capaz la inteligencia artificial? ](https://youtu.be/34Kz-PP_X7c?si=sbV0ENQYtvT2JKiI)** - Documental de DW.

¡Únete a mí en este emocionante viaje y no dudes en compartir tus pensamientos y preguntas! 🚀

---
# Día2
---
## Historia y Evolución de Deep Learning 📜

¡Bienvenidos al segundo día de nuestra travesía de 100 días en el mundo de la Inteligencia Artificial! Hoy, exploramos la fascinante **historia y evolución de Deep Learning**. 🌟

### Orígenes y Primeros Pasos

#### 1943: La Idea de una Neurona Artificial 💡
El viaje de Deep Learning comenzó con Warren McCulloch y Walter Pitts, quienes propusieron el primer modelo matemático de una **neurona artificial**. Su trabajo sentó las bases para las redes neuronales, sugiriendo que las neuronas podrían ser el equivalente funcional de un interruptor binario.

#### 1958: El Perceptrón 🤖
Frank Rosenblatt desarrolló el **Perceptrón**, el primer modelo de red neuronal capaz de aprender. El perceptrón es un tipo simple de red que puede clasificar datos en dos categorías. Aunque su capacidad era limitada, fue un hito importante que inspiró investigaciones futuras.

### El Invierno de la IA ❄️

#### Años 70-80: Desafíos y Dudas
Durante los años 70 y 80, las expectativas sobre las redes neuronales no se cumplieron, y la falta de poder computacional y datos llevó a lo que se conoce como el **"invierno de la IA"**. Durante este período, la investigación en redes neuronales se desaceleró debido al escepticismo y la falta de avances significativos.

### Renacimiento y Avances 🚀

#### 1986: El Redescubrimiento de la Propagación hacia Atrás
En 1986, David Rumelhart, Geoffrey Hinton y Ronald Williams revitalizaron el interés en las redes neuronales con su trabajo sobre la **retropropagación**. Este algoritmo permitió el entrenamiento eficaz de redes neuronales multicapa, allanando el camino para el desarrollo de modelos más complejos.

#### Años 90: Aplicaciones Prácticas 🌐
A medida que aumentaba el poder computacional y se disponía de más datos, las redes neuronales comenzaron a mostrar su potencial en áreas como el reconocimiento de patrones y la predicción financiera. Sin embargo, aún quedaban desafíos significativos por superar.

### La Era de Deep Learning 💥

#### 2006: El Avance de las Redes Profundas
Geoffrey Hinton y su equipo introdujeron el concepto de **preentrenamiento de capas** en redes profundas, lo que permitió entrenar eficientemente modelos con muchas capas. Este avance marcó el comienzo de la **era de Deep Learning**, demostrando que las redes neuronales profundas podían superar a los métodos tradicionales en tareas complejas.

#### 2012: El Triunfo en ImageNet 🏆
El hito crucial llegó en 2012 cuando una red profunda conocida como **AlexNet**, desarrollada por Alex Krizhevsky, Ilya Sutskever y Geoffrey Hinton, ganó el desafío de reconocimiento de imágenes de **ImageNet** con un margen significativo. Esto consolidó a Deep Learning como la tecnología líder en visión por computadora.

### Transformadores y Nuevas Fronteras 🚀

#### 2017: El Surgimiento de los Transformadores
En 2017, el artículo "Attention is All You Need" de Google introdujo el **modelo Transformer**, revolucionando el procesamiento del lenguaje natural (NLP). Los Transformers, como **BERT** y **GPT**, demostraron capacidades impresionantes en tareas de lenguaje, superando a los modelos anteriores.

#### 2018: GPT y el Avance de los Modelos de Lenguaje
OpenAI lanzó **GPT (Generative Pre-trained Transformer)**, seguido por GPT-2 y el famoso **GPT-3** en 2020. Estos modelos mostraron habilidades sin precedentes en generación de texto, comprensión y traducción, marcando un hito en el desarrollo de la IA.

### Innovaciones Recientes 🔄

#### 2021: DALL-E y la Creatividad Artificial
OpenAI presentó **DALL-E**, un modelo capaz de generar imágenes a partir de descripciones textuales. Esta innovación destacó la capacidad de la IA para combinar lenguaje y visión, abriendo nuevas posibilidades en arte y diseño.

#### 2021: AlphaFold y la Revolución en la Biología
DeepMind's **AlphaFold** resolvió uno de los mayores desafíos en biología: la predicción de estructuras proteicas. Este avance promete acelerar el descubrimiento de medicamentos y mejorar nuestra comprensión de la biología molecular.

#### 2022: ChatGPT y la Conversación Natural
OpenAI lanzó **ChatGPT**, una versión mejorada de GPT-3 optimizada para conversaciones interactivas. Este modelo demostró habilidades avanzadas en el diálogo, respondiendo preguntas y asistiendo en diversas tareas de manera coherente y precisa.


### Recursos para Explorar Más:

- **[Breve Historia de las Redes Neuronales Artificiales](https://www.aprendemachinelearning.com/breve-historia-de-las-redes-neuronales-artificiales/)** - Un artículo detallado sobre la evolución de las redes neuronales.
- **[The brief history of artificial intelligence](https://ourworldindata.org/brief-history-of-ai)** - Un artículo detallado sobre la evolución de la IA.

### Evolución de los modelos de IA con respecto a la computación utilizada en su entrenamiento

<<<<<<< HEAD
=======
https://github.com/Oliver369X/100DaysOfAI/assets/110129950/64c6b46d-4c12-4e7a-8511-b35a2ad5be8e

>>>>>>> 52f223851c1f64cb143e7b84519e39d23a2985f8
---
# Día3
---
## Breve Descripción de las Diferentes Técnicas en Deep Learning 🧠


### 1. Redes Neuronales Convolucionales (CNN) 🖼️

#### Descripción
Las **Redes Neuronales Convolucionales (CNN)** están diseñadas para procesar datos con una estructura de grilla, como las imágenes. Utilizan capas convolucionales que aplican filtros para detectar características como bordes, texturas y patrones en las imágenes.

#### Componentes Clave
- **Capas Convolucionales**: Aplican filtros para extraer características locales.
- **Capas de Pooling**: Reducen la dimensionalidad y ayudan a generalizar.
- **Capas Completamente Conectadas**: Usadas para clasificar y tomar decisiones basadas en las características extraídas.

### 2. Redes Neuronales Recurrentes (RNN) 🔁

#### Descripción
Las **Redes Neuronales Recurrentes (RNN)** están diseñadas para procesar secuencias de datos, como texto o series temporales. Tienen conexiones recurrentes que permiten que la información persista, lo que es útil para modelar dependencias temporales.

#### Componentes Clave
- **Celdas Recurrentes**: Mantienen un estado oculto que captura información de pasos anteriores.
- **LSTM y GRU**: Variantes avanzadas de RNN que abordan problemas de memoria a largo plazo.

### 3. Redes Generativas Adversariales (GAN) 🎨

#### Descripción
Las **Redes Generativas Adversariales (GAN)** constan de dos redes: una generadora y una discriminadora. La generadora crea datos falsos, mientras que la discriminadora intenta distinguir entre datos reales y falsos. Este proceso competitivo mejora la capacidad de la generadora para producir datos realistas.

#### Componentes Clave
- **Generador**: Crea datos sintéticos.
- **Discriminador**: Distingue entre datos reales y generados.
- **Juego Adversarial**: La competencia entre las dos redes mejora el rendimiento del sistema.

### 4. Transformadores 🔄

#### Descripción
Los **Transformadores** han revolucionado el procesamiento del lenguaje natural (NLP) con su mecanismo de atención que permite procesar todas las palabras de una oración en paralelo. Esto los hace altamente eficientes y precisos en tareas de lenguaje.

#### Componentes Clave
- **Mecanismo de Atención**: Pondera la importancia de diferentes palabras en una oración.
- **Codificadores y Decodificadores**: Procesan las secuencias de entrada y generan secuencias de salida.

### 5. Modelos de Difusión 🌫️

#### Descripción
Los **Modelos de Difusión** son una técnica emergente en generación de datos. Funcionan modelando la distribución de los datos y luego generando nuevos ejemplos a partir de esta distribución, similar a los procesos físicos de difusión.

#### Componentes Clave
- **Proceso de Difusión**: Modela cómo los datos cambian con el tiempo.
- **Reconstrucción Inversa**: Genera nuevos datos a partir del proceso de difusión.

### 6. Modelos Multimodales 🎥🎵📝

#### Descripción
Los **Modelos Multimodales** integran y procesan múltiples tipos de datos, como texto, imágenes y audio, para realizar tareas complejas que requieren comprensión de información diversa.

#### Componentes Clave
- **Fusión de Modalidades**: Combina diferentes tipos de datos en una representación unificada.
- **Atención Cruzada**: Captura interacciones entre diferentes modalidades.


### Recursos para Explorar Más:

- **[¡Redes Neuronales CONVOLUCIONALES! ](https://youtu.be/V8j1oENVz00?si=RY91rvLjMXPbjRbF)** - Video detallado sobre CNN.
- **[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)** - Una explicación profunda sobre las RNN y LSTM.
- **[GANs in Action](https://www.youtube.com/watch?v=8L11aMN5KY8)** - Un video tutorial sobre GANs.
- **[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)** - Una guía visual sobre transformadores.
- **[Cómo funciona la generación de imágenes con IA (modelos de difusión)](https://youtu.be/mNxzQvdVSQI?si=_Lno74MYiqcbidei)** - Introducción a los modelos de difusión.
- **[Multimodal learning](https://en.wikipedia.org/wiki/Multimodal_learning)** - Definicion de Wikipedia.

---

# Día4
---
## Comparación y Aplicaciones de Técnicas de Deep Learning en el Mundo Real 🌍

¡Hola a todos! compararemos las diferentes técnicas de Deep Learning que discutimos ayer y exploraremos sus aplicaciones en el mundo real. Vamos a sumergirnos en cómo se utilizan las **CNN, RNN, GAN, Transformadores, Modelos de Difusión y Modelos Multimodales** en diversos campos. 🌐

### Comparación de Técnicas de Deep Learning

| Técnica         | Descripción                                                   | Fortalezas                                                     | Limitaciones                                                       |
|-----------------|---------------------------------------------------------------|----------------------------------------------------------------|--------------------------------------------------------------------|
| **CNN**         | Procesan datos con estructura de grilla (como imágenes).       | Excelente para tareas de visión por computadora.                | No maneja bien datos secuenciales o dependencias temporales.       |
| **RNN**         | Procesan secuencias de datos (como texto o series temporales). | Capturan dependencias temporales y contextuales.                | Pueden sufrir de problemas de gradiente desaparecido/explosivo.    |
| **GAN**         | Generan datos sintéticos mediante una competencia entre dos redes. | Producen datos realistas en imagen, video y audio.              | Dificultad en entrenamiento y estabilidad.                         |
| **Transformadores** | Procesan secuencias de datos en paralelo utilizando atención. | Eficientes y precisos en procesamiento de lenguaje natural.     | Requieren grandes cantidades de datos y recursos computacionales.  |
| **Modelos de Difusión** | Modelan la distribución de datos para generación.        | Alta calidad en generación de imágenes y datos.                 | Técnicamente complejos y requieren mucho tiempo de entrenamiento.  |
| **Modelos Multimodales** | Integran múltiples tipos de datos (texto, imagen, audio). | Capturan interacciones complejas entre diferentes tipos de datos. | Complejidad en la fusión de datos y gestión de múltiples modalidades. |

### Aplicaciones en el Mundo Real

#### 1. Redes Neuronales Convolucionales (CNN) 🖼️

**Aplicaciones:**
- **Reconocimiento de Imágenes**: Identificación de objetos, personas y escenas en imágenes.
- **Diagnóstico Médico**: Análisis de imágenes médicas, como radiografías y resonancias magnéticas.
- **Seguridad y Vigilancia**: Detección de anomalías y reconocimiento facial.

#### 2. Redes Neuronales Recurrentes (RNN) 🔁

**Aplicaciones:**
- **Procesamiento del Lenguaje Natural (NLP)**: Traducción automática, generación de texto, chatbots.
- **Análisis de Series Temporales**: Predicción de mercados financieros, demanda energética, clima.
- **Reconocimiento de Voz**: Transcripción y comandos de voz en asistentes virtuales.

#### 3. Redes Generativas Adversariales (GAN) 🎨

**Aplicaciones:**
- **Generación de Imágenes y Videos**: Creación de arte digital, efectos visuales en películas.
- **Aumento de Datos**: Generación de datos sintéticos para mejorar el entrenamiento de modelos.
- **Restauración de Imágenes**: Mejora de resolución, eliminación de ruido, restauración de imágenes antiguas.

#### 4. Transformadores 🔄

**Aplicaciones:**
- **Procesamiento del Lenguaje Natural (NLP)**: Modelos de lenguaje avanzados como GPT, BERT, traducción automática.
- **Generación de Texto**: Resumen automático, generación de contenido, respuestas automáticas en chats.
- **Análisis de Datos**: Clasificación de documentos, detección de entidades nombradas, análisis de sentimientos.

#### 5. Modelos de Difusión 🌫️

**Aplicaciones:**
- **Generación de Imágenes**: Creación de imágenes de alta calidad a partir de descripciones textuales.
- **Simulación de Procesos Físicos**: Modelado de fenómenos naturales como la difusión de gases.
- **Diseño Gráfico**: Creación de patrones y texturas para diseño digital.

#### 6. Modelos Multimodales 🎥🎵📝

**Aplicaciones:**
- **Sistemas de Recomendación**: Recomendaciones personalizadas basadas en múltiples tipos de datos (texto, imágenes, audio).
- **Análisis de Redes Sociales**: Comprensión de publicaciones multimedia, análisis de sentimientos.
- **Asistentes Virtuales**: Integración de voz, texto e imágenes para interacción más natural y completa.


### Recursos para Explorar Más:


- **[The GAN Zoo](https://github.com/hindupuravinash/the-gan-zoo)** - Una colección de diferentes tipos de GANs.
- **[Attention is All You Need](https://arxiv.org/abs/1706.03762)** - El artículo seminal sobre transformadores.
- **[Explicación Completa: Attention is All You Need](https://youtu.be/as2FFM3c6mI?si=_pNuRFCEHHYsizro)** - Un video detallado explicando los transformadores.

---

# Día5
---
## Redes Neuronales Artificiales (ANNs)  🧠

¡Hola a todos! En el quinto día de nuestra travesía de 100 días en el mundo de la Inteligencia Artificial, exploraremos la estructura básica de las Redes Neuronales Artificiales (ANNs) y entenderemos cómo funcionan sus capas neuronales. 🌟

### ¿Qué son las Redes Neuronales Artificiales (ANNs)?

Las Redes Neuronales Artificiales (ANNs) son modelos computacionales inspirados en el funcionamiento del cerebro humano. Están diseñadas para reconocer patrones y resolver problemas complejos mediante el aprendizaje a partir de datos. 🌐

### Estructura Básica de una Red Neuronal

Una red neuronal típica consta de tres tipos de capas:

1. **Capa de Entrada (Input Layer)**: Recibe los datos iniciales.
2. **Capas Ocultas (Hidden Layers)**: Procesan la información recibida de la capa de entrada.
3. **Capa de Salida (Output Layer)**: Genera el resultado final.


#### 1. **Capa de Entrada (Input Layer)**
La capa de entrada es la primera capa de la red neuronal. Cada nodo en esta capa representa una característica del conjunto de datos de entrada. Por ejemplo, en una red que procesa imágenes, cada nodo podría representar el valor de un píxel de la imagen.

#### 2. **Capas Ocultas (Hidden Layers)**
Las capas ocultas son las encargadas de realizar la mayor parte del procesamiento de la red. Pueden existir múltiples capas ocultas, cada una compuesta por múltiples nodos o "neuronas". Cada neurona en una capa está conectada a todas las neuronas de la capa anterior y de la capa siguiente.

##### Funcionamiento de las Capas Ocultas:
- **Pesos y Sesgos (Weights and Biases)**: Cada conexión entre neuronas tiene un peso asignado que indica la importancia de la entrada correspondiente. Además, cada neurona tiene un valor de sesgo que ajusta la salida del nodo.
- **Funciones de Activación (Activation Functions)**: Después de que una neurona recibe la entrada ponderada, aplica una función de activación para introducir no linealidades en el modelo. Las funciones de activación comunes incluyen ReLU (Rectified Linear Unit), Sigmoid y Tanh.



#### 3. **Capa de Salida (Output Layer)**
La capa de salida es la última capa de la red neuronal y proporciona el resultado final. La estructura de esta capa depende del tipo de tarea que esté realizando la red. Por ejemplo, en un problema de clasificación binaria, la capa de salida podría tener una sola neurona con una función de activación Sigmoid.

### ¿Cómo Aprenden las Redes Neuronales?

El aprendizaje en redes neuronales implica ajustar los pesos y los sesgos de la red para minimizar el error en las predicciones. Este proceso se realiza mediante un algoritmo de optimización llamado **Backpropagation** (retropropagación), que utiliza el **Gradiente Descendente** para ajustar los pesos de manera iterativa.


### Recursos para Explorar Más:

- **[Cómo funcionan las redes neuronales](https://youtu.be/CU24iC3grq8?si=9UT2DpOAA1cQ1Ay0)** (Video).
- **[¿Qué es una Red Neuronal?](https://youtu.be/jKCQsndqEGQ?si=jNASfwuoQB9tXyle)** - (Video).
- **[Funciones de activación a detalle](https://youtu.be/_0wdproot34?si=B27NeiOze7QGGi6K)** - (Video).
- **[Juegue con una red neuronal ](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=1&seed=0.87931&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)** - Juegue con una red neuronal aquí mismo en su navegador.
No te preocupes, no puedes romperlo.
![ANNs](https://github.com/Oliver369X/100DaysOfAI/assets/110129950/6de8c3e3-ea5a-46e0-8fd0-600e794b422d)

![back2](https://github.com/Oliver369X/100DaysOfAI/assets/110129950/2ebfb67d-7af9-49bd-a509-b1d6babf0148)

---

# Día6
---
## Conceptos de Forward y Backward Propagation 🧠🔄

¡Hola a todos! Hoy, en el sexto día de nuestro viaje de 100 días en el mundo de la Inteligencia Artificial, exploraremos dos conceptos fundamentales para el entrenamiento de redes neuronales: **Forward Propagation** y **Backward Propagation**. Estos procesos son esenciales para que las redes neuronales aprendan de los datos y mejoren su rendimiento. 🚀

### ¿Qué es Forward Propagation?

**Forward Propagation** es el proceso mediante el cual los datos de entrada se transmiten a través de la red neuronal para generar una salida. Este flujo de información comienza en la capa de entrada, pasa por las capas ocultas y finalmente llega a la capa de salida.

#### Pasos de Forward Propagation:

1. **Entrada**: Los datos de entrada se presentan a la red neuronal.
2. **Ponderación**: Cada neurona en la capa de entrada envía sus datos ponderados a cada neurona de la primera capa oculta.
3. **Activación**: Las neuronas de la capa oculta calculan una suma ponderada de sus entradas, aplican una función de activación y transmiten el resultado a la siguiente capa.
4. **Salida**: Este proceso se repite capa por capa hasta que los datos alcanzan la capa de salida, donde se generan las predicciones finales.

### ¿Qué es Backward Propagation?

**Backward Propagation** (o retropropagación) es el proceso mediante el cual la red neuronal ajusta sus pesos y sesgos para minimizar el error en sus predicciones. Este ajuste se realiza mediante la propagación del error desde la capa de salida hacia atrás a través de las capas ocultas, hasta llegar a la capa de entrada.

#### Pasos de Backward Propagation:

1. **Cálculo del Error**: Se calcula la diferencia entre la salida real de la red y la salida esperada (etiquetas verdaderas).
2. **Propagación del Error**: El error se propaga hacia atrás a través de la red. En cada neurona, se calcula el gradiente del error con respecto a sus pesos y sesgos.
3. **Ajuste de Pesos y Sesgos**: Los pesos y sesgos se actualizan utilizando el gradiente calculado y una tasa de aprendizaje, reduciendo así el error de la red.

### Cómo Funcionan Juntos Forward y Backward Propagation

1. **Forward Propagation**: Los datos de entrada se procesan a través de la red para generar una predicción.
2. **Cálculo del Error**: Se compara la predicción con la etiqueta verdadera para calcular el error.
3. **Backward Propagation**: El error se propaga hacia atrás a través de la red, y los pesos y sesgos se ajustan en consecuencia.
4. **Actualización de Parámetros**: Los parámetros de la red se actualizan para reducir el error en futuras predicciones.

### Ejemplo Simplificado

Imaginemos que estamos entrenando una red neuronal para predecir el precio de una casa basado en su tamaño.

1. **Forward Propagation**:
   - Entrada: Tamaño de la casa.
   - Cálculo: La red multiplica el tamaño por un peso, añade un sesgo y aplica una función de activación.
   - Salida: Predicción del precio de la casa.

2. **Cálculo del Error**:
   - Comparamos la predicción con el precio real y calculamos el error.

3. **Backward Propagation**:
   - Propagamos el error hacia atrás a través de la red, calculando el gradiente del error con respecto a cada peso y sesgo.
   - Ajustamos los pesos y sesgos para minimizar el error en futuras predicciones.


### Recursos para Explorar Más:

- **[Redes Neuronales (forward propagation y backpropagation)](https://youtu.be/A9jZflhT2R0?si=uQj8Xw1xa2_O1kDO)** -Explicacion matematica(Video).
- **[Las Matemáticas de Backpropagation | DotCSV](https://youtu.be/M5QHwkkHgAA?si=ZiX3Gp9I25liaNFq)** - Explicacion matematica(Video).

---

# Día7

---
## Conceptos de Coste y Funciones de Pérdida 💡📉

¡Hola a todos! Hoy, en el séptimo día de nuestro reto #100DaysOfAI, exploraremos dos conceptos fundamentales para el entrenamiento de redes neuronales: **coste** y **funciones de pérdida**. Estos conceptos son esenciales para evaluar el rendimiento de nuestros modelos y guiar el proceso de aprendizaje. 🚀

### ¿Qué es el Coste?

El **coste** se refiere a la medida de lo mal que un modelo de red neuronal está realizando sus predicciones en comparación con los valores reales. En otras palabras, es una representación cuantitativa del error del modelo. Cuanto menor sea el coste, mejor será el rendimiento del modelo.

### ¿Qué es una Función de Pérdida?

Una **función de pérdida** es una función matemática que mide la discrepancia entre las predicciones del modelo y los valores reales. Durante el entrenamiento, el objetivo es minimizar esta función de pérdida para mejorar la precisión del modelo. 

### Tipos Comunes de Funciones de Pérdida:

1. **Error Cuadrático Medio (Mean Squared Error, MSE)**:

2. **Error Absoluto Medio (Mean Absolute Error, MAE)**:


3. **Entropía Cruzada (Cross-Entropy)**:


### Relación entre Coste y Función de Pérdida:

- **Coste Total**: La función de pérdida calcula el error para una sola instancia de datos, mientras que el coste total (también conocido como función de coste o función de error) es la media de las pérdidas para todo el conjunto de entrenamiento.
- **Optimización**: Durante el entrenamiento, el algoritmo de optimización ajusta los pesos de la red neuronal para minimizar el coste total. Esto se realiza típicamente mediante un algoritmo de optimización como el gradiente descendente.

### Importancia en el Entrenamiento

1. **Evaluación del Modelo**: Las funciones de pérdida nos permiten evaluar cuán bien o mal está desempeñándose el modelo.
2. **Guía para la Optimización**: Proveen la señal que guía el proceso de optimización durante el entrenamiento. Sin una función de pérdida, no podríamos ajustar los pesos de manera efectiva.
3. **Selección de Modelos**: Diferentes problemas pueden requerir diferentes funciones de pérdida. Elegir la función correcta es crucial para el éxito del modelo.


### Recursos para Explorar Más:

- **[3Blue1Brown's YouTube Series on Neural Networks](https://youtu.be/mwHiaTrQOiI?si=j_a-9WxP_1um9YVc)** - Una serie de videos educativos que visualizan estos procesos de manera intuitiva.

---

# Día8

---
## Algoritmos de Optimización  🚀📈

¡Hola a todos! En el día 8 de nuestro reto #100DaysOfAI, vamos a profundizar en los **algoritmos de optimización avanzados**. Estos algoritmos son esenciales para mejorar el rendimiento y la eficiencia de los modelos de aprendizaje profundo. ¡Vamos a explorarlos juntos! 🌟

### ¿Qué es la Optimización?

La **optimización** en el contexto del aprendizaje profundo se refiere al proceso de ajustar los parámetros del modelo (como los pesos de las redes neuronales) para minimizar la función de pérdida. Este proceso es crucial para que el modelo pueda aprender de los datos y hacer predicciones precisas.

### Algoritmos de Optimización Comunes

1. **Gradiente Descendente Estocástico (SGD)**:
   - **Descripción**: En lugar de utilizar todo el conjunto de datos para calcular los gradientes, el SGD actualiza los parámetros del modelo usando un solo ejemplo de entrenamiento a la vez.
   - **Ventaja**: Es más rápido y puede manejar grandes conjuntos de datos.

2. **Gradiente Descendente por Minilotes (Mini-batch Gradient Descent)**:
   - **Descripción**: Combina los enfoques de SGD y del gradiente descendente de lote completo, actualizando los parámetros utilizando un pequeño subconjunto (mini-lote) de los datos de entrenamiento.
   - **Ventaja**: Equilibra la estabilidad del gradiente descendente de lote completo y la rapidez del SGD.

### Algoritmos de Optimización Avanzados

1. **Momentum**:
   - **Descripción**: Agrega una fracción del gradiente anterior al gradiente actual para acelerar la convergencia y evitar quedarse atrapado en mínimos locales.
   - **Ventaja**: Mejora la velocidad y estabilidad del SGD.
  

2. **RMSprop**:
   - **Descripción**: Divide la tasa de aprendizaje por una media móvil de la magnitud de los gradientes recientes. Esto ayuda a mantener una tasa de aprendizaje adecuada y evita oscilaciones.
   - **Ventaja**: Mantiene una tasa de aprendizaje adaptativa.
  

3. **Adam (Adaptive Moment Estimation)**:
   - **Descripción**: Combina las ideas de Momentum y RMSprop. Utiliza medias móviles de los gradientes y sus cuadrados, adaptando así la tasa de aprendizaje para cada parámetro.
   - **Ventaja**: Convergencia rápida y robusta.
  

4. **AdaGrad**:
   - **Descripción**: Ajusta la tasa de aprendizaje para cada parámetro en función de los gradientes acumulados pasados. 
   - **Ventaja**: Beneficioso para características raras y evita el ajuste excesivo en características comunes.
   
### Comparación de Algoritmos

- **SGD**: Simple y eficiente para grandes conjuntos de datos, pero puede ser ruidoso.
- **Momentum**: Acelera el SGD y suaviza la convergencia.
- **RMSprop**: Adapta la tasa de aprendizaje, útil para problemas con tasas de aprendizaje inestables.
- **Adam**: Combina las ventajas de Momentum y RMSprop, ampliamente utilizado.
- **AdaGrad**: Ajusta la tasa de aprendizaje para cada parámetro, útil para datos dispersos.


### Recursos para Explorar Más:

- **[Adam - A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)** - El artículo original que introduce Adam.
- **[Algoritmos de Optimización ](https://youtu.be/1GFu3nOya4c?si=v3jnhocKnb_R0Xw_)** - Explicacion completa (Video).


---

# Día9
---
## Overfitting y Técnicas de Regularización 🧠🔍

¡Hola a todos! En el día 9 de nuestro desafío #100DaysOfAI, vamos a sumergirnos en el concepto de **overfitting** y las técnicas de **regularización**. Estas son herramientas fundamentales para mejorar la capacidad predictiva y la generalización de nuestros modelos de aprendizaje profundo. ¡Vamos a explorarlas juntos! 📉📚

### ¿Qué es el Overfitting?

El **overfitting** ocurre cuando nuestro modelo se ajusta demasiado bien a los datos de entrenamiento, capturando no solo la señal real sino también el ruido. Como resultado, el modelo puede tener un rendimiento deficiente en datos nuevos y no vistos, lo que lleva a una baja capacidad de generalización.

### Técnicas de Regularización

1. **Regularización L1 y L2**:
   - **Descripción**: Agrega un término de penalización a la función de pérdida que es proporcional a la norma L1 o L2 de los pesos del modelo.
   - **Ventaja**: Ayuda a prevenir el overfitting al penalizar los pesos grandes.

2. **Dropout**:
   - **Descripción**: Aleatoriamente "apaga" una fracción de las neuronas durante el entrenamiento, lo que obliga al modelo a aprender características más robustas y reduce la dependencia entre las neuronas.
   - **Ventaja**: Actúa como una forma de regularización al evitar la coadaptación de las neuronas.

3. **Data Augmentation**:
   - **Descripción**: Aumenta el tamaño del conjunto de datos de entrenamiento aplicando transformaciones como rotaciones, traslaciones y zoom a las imágenes originales.
   - **Ventaja**: Ayuda a diversificar el conjunto de datos de entrenamiento y a mejorar la generalización del modelo.

4. **Early Stopping**:
   - **Descripción**: Detiene el entrenamiento del modelo cuando el rendimiento en un conjunto de datos de validación deja de mejorar.
   - **Ventaja**: Evita el sobreajuste al detener el entrenamiento antes de que el modelo comience a sobreajustarse a los datos de entrenamiento.

### Aplicación en la Práctica

Para aplicar estas técnicas de regularización en nuestros modelos, debemos ajustar los hiperparámetros adecuados y experimentar con diferentes configuraciones para encontrar el equilibrio óptimo entre la capacidad de ajuste y la generalización.

### Recursos para Explorar Más:

- **[Overfitting ](https://youtube.com/playlist?list=PLWP2CHQigyUSw1TJkOdAxzBC0BtKrYAnz&si=InFqmXxk1iRgX611)** - Playlists de underfitting y overfitting.
- **[Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)** - El artículo seminal que introduce la técnica de dropout.
- **[Técnicas de Regularización](https://youtu.be/qa9M4NBV9Lk?si=G09xw9uQaTsmwmY4)** - Explicaion practica.


---

# Día10
---
## Construyendo una Red Neuronal desde Cero: Clasificación de Flores Iris
### Introducción al Problema y Objetivos

En esta práctica, vamos a implementar una red neuronal simple desde cero para resolver el problema de clasificación de flores Iris. Este es un problema clásico en el aprendizaje automático y es perfecto para entender los fundamentos de las redes neuronales.

**Objetivo:** Crear una red neuronal que pueda clasificar correctamente las flores Iris en sus tres especies (setosa, versicolor, virginica) basándose en cuatro características: longitud del sépalo, ancho del sépalo, longitud del pétalo y ancho del pétalo.

**¿Por qué usar redes neuronales?** Las redes neuronales son excelentes para encontrar patrones complejos en los datos. En este caso, pueden aprender las relaciones no lineales entre las características de las flores y sus especies, permitiendo una clasificación precisa.
![iris_flowers](https://github.com/Oliver369X/100DaysOfAI/assets/110129950/c98d7fec-a4ad-478e-ba49-bdc24b63e98e)

---

## Importación de Librerías

Primero, importaremos las librerías necesarias para nuestro proyecto.

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
import matplotlib.pyplot as plt
```

Explicación:
- `numpy`: Para operaciones numéricas eficientes.
- `sklearn.datasets`: Para cargar el conjunto de datos Iris.
- `sklearn.model_selection`: Para dividir nuestros datos en conjuntos de entrenamiento y prueba.
- `sklearn.preprocessing`: Para codificar nuestras etiquetas.
- `matplotlib.pyplot`: Para visualizar nuestros resultados.

---

## Carga y Preparación del Conjunto de Datos

El conjunto de datos Iris es un conjunto clásico en aprendizaje automático. Contiene 150 muestras de flores Iris, con 50 muestras de cada una de las tres especies.

```python
# Cargar el conjunto de datos Iris
iris = load_iris()
X = iris.data
y = iris.target.reshape(-1, 1)

# One-hot encoding para las etiquetas
encoder = OneHotEncoder(sparse=False)
y = encoder.fit_transform(y)

# Dividir datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Forma de X_train:", X_train.shape)
print("Forma de y_train:", y_train.shape)
print("Forma de X_test:", X_test.shape)
print("Forma de y_test:", y_test.shape)
```

Explicación:
- Cargamos el conjunto de datos Iris.
- Aplicamos one-hot encoding a las etiquetas para convertirlas en un formato adecuado para la red neuronal.
- Dividimos los datos en conjuntos de entrenamiento (80%) y prueba (20%).
- Imprimimos las formas de nuestros conjuntos de datos para verificar.

---

## Implementación de la Red Neuronal

Ahora, implementaremos nuestra clase de red neuronal simple.

```python
class SimpleNeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        self.W1 = np.random.randn(input_size, hidden_size) * 0.01
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, output_size) * 0.01
        self.b2 = np.zeros((1, output_size))

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def softmax(self, x):
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)
```

Explicación:
- Inicializamos los pesos (`W1`, `W2`) y sesgos (`b1`, `b2`) de nuestra red.
- Implementamos la función de activación sigmoid para la capa oculta.
- Implementamos la función softmax para la capa de salida, que nos dará probabilidades para cada clase.

---

## Forward Propagation

Implementamos el paso hacia adelante (forward propagation) de nuestra red.

```python
def forward(self, X):
    self.z1 = np.dot(X, self.W1) + self.b1
    self.a1 = self.sigmoid(self.z1)
    self.z2 = np.dot(self.a1, self.W2) + self.b2
    self.a2 = self.softmax(self.z2)
    return self.a2
```

Explicación:
- Calculamos la salida de la capa oculta (`z1`) y aplicamos la función sigmoid (`a1`).
- Calculamos la salida de la capa final (`z2`) y aplicamos softmax (`a2`).
- Retornamos la salida final, que son las probabilidades para cada clase.

---

## Función de Pérdida

Implementamos la función de pérdida de entropía cruzada.

```python
def cross_entropy_loss(self, y_true, y_pred):
    m = y_true.shape[0]
    log_likelihood = -np.log(y_pred[range(m), y_true.argmax(axis=1)])
    loss = np.sum(log_likelihood) / m
    return loss
```

Explicación:
- Calculamos la pérdida de entropía cruzada entre las etiquetas verdaderas y las predicciones.
- Esta función mide qué tan bien nuestras predicciones se ajustan a las etiquetas reales.

---

## Backward Propagation

Implementamos la retropropagación (backward propagation) para actualizar los pesos.

```python
def backward(self, X, y, learning_rate):
    m = X.shape[0]
    
    dZ2 = self.a2 - y
    dW2 = np.dot(self.a1.T, dZ2) / m
    db2 = np.sum(dZ2, axis=0, keepdims=True) / m
    
    dZ1 = np.dot(dZ2, self.W2.T) * (self.a1 * (1 - self.a1))
    dW1 = np.dot(X.T, dZ1) / m
    db1 = np.sum(dZ1, axis=0, keepdims=True) / m
    
    self.W2 -= learning_rate * dW2
    self.b2 -= learning_rate * db2
    self.W1 -= learning_rate * dW1
    self.b1 -= learning_rate * db1
```

Explicación:
- Calculamos los gradientes para cada capa.
- Actualizamos los pesos y sesgos usando estos gradientes y la tasa de aprendizaje.

---

## Entrenamiento

Implementamos el bucle de entrenamiento.

```python
def train(self, X, y, epochs, learning_rate, batch_size):
    losses = []
    for epoch in range(epochs):
        for i in range(0, X.shape[0], batch_size):
            X_batch = X[i:i+batch_size]
            y_batch = y[i:i+batch_size]
            
            y_pred = self.forward(X_batch)
            loss = self.cross_entropy_loss(y_batch, y_pred)
            self.backward(X_batch, y_batch, learning_rate)
            
        if epoch % 100 == 0:
            losses.append(loss)
            print(f"Epoch {epoch}, Loss: {loss}")
    return losses
```

Explicación:
- Entrenamos la red durante un número especificado de épocas.
- Usamos mini-batch gradient descent para actualizar los pesos.
- Registramos la pérdida cada 100 épocas para monitorear el progreso.

---

## Evaluación

Implementamos funciones para hacer predicciones y calcular la precisión.

```python
def predict(self, X):
    return np.argmax(self.forward(X), axis=1)

def accuracy(self, X, y):
    predictions = self.predict(X)
    return np.mean(predictions == np.argmax(y, axis=1))
```

Explicación:
- `predict`: Hace predicciones para nuevos datos.
- `accuracy`: Calcula la precisión de nuestras predicciones.

---

## Entrenamiento y Evaluación del Modelo

Ahora, entrenamos nuestro modelo y evaluamos su rendimiento.

```python
# Crear y entrenar el modelo
model = SimpleNeuralNetwork(input_size=4, hidden_size=10, output_size=3)
losses = model.train(X_train, y_train, epochs=1000, learning_rate=0.1, batch_size=32)

# Evaluar el modelo
train_accuracy = model.accuracy(X_train, y_train)
test_accuracy = model.accuracy(X_test, y_test)

print(f"Precisión en entrenamiento: {train_accuracy:.4f}")
print(f"Precisión en prueba: {test_accuracy:.4f}")
```

Explicación:
- Creamos una instancia de nuestra red neuronal.
- Entrenamos el modelo durante 1000 épocas.
- Evaluamos la precisión en los conjuntos de entrenamiento y prueba.

---

## Visualización de Resultados

Finalmente, visualizamos cómo la pérdida cambia durante el entrenamiento.

```python
plt.plot(range(0, 1000, 100), losses)
plt.xlabel('Épocas')
plt.ylabel('Pérdida')
plt.title('Pérdida durante el entrenamiento')
plt.show()
```

Explicación:
- Graficamos la pérdida a lo largo de las épocas de entrenamiento.
- Esto nos ayuda a visualizar cómo el modelo aprende con el tiempo.


### Recursos para Explorar Más:

- **[Análisis exploratorio de datos del conjunto de datos Iris](https://youtu.be/yu4SYEYkZ6U?si=oOb1DEuG5GcS-f4e)** MasterClass (video)
- **[Analisis Exploratorio de Datos dataset Iris](https://www.kaggle.com/code/joeportilla/analisis-exploratorio-de-datos-dataset-iris)** - Notebook Kaggle.

## Colab Notebooks

- [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Qv7LRrhvzGuJPkelYWz9zYJz-oPYIqDJ?usp=sharing) [Día 10: Clasificación de Flores Iris](https://colab.research.google.com/drive/1Qv7LRrhvzGuJPkelYWz9zYJz-oPYIqDJ?usp=sharing) 


---

# Día11
---

## Redes Neuronales Artificiales (ANNs) con MNIST

### Introducción

En este proyecto, exploraremos la estructura básica de las Redes Neuronales Artificiales (ANNs) y su funcionamiento implementando un modelo para clasificar dígitos escritos a mano utilizando el dataset MNIST.

Las ANNs son modelos computacionales inspirados en el cerebro humano. Están diseñadas para reconocer patrones y resolver problemas complejos a partir de datos. Una ANN típica consta de tres tipos de capas:
- **Capa de Entrada**: Recibe los datos iniciales.
- **Capas Ocultas**: Procesan la información.
- **Capa de Salida**: Genera el resultado final.

Nuestro objetivo es construir, entrenar y evaluar una ANN usando el dataset MNIST para clasificar imágenes de dígitos escritos a mano.
![Clasificación de Numeros Escritos a Mano](https://github.com/Oliver369X/100DaysOfAI/assets/110129950/4ca7e2c8-2f28-423c-b13b-2fcca5647892)

### Importación de Bibliotecas y Dataset

En este punto, importaremos las bibliotecas necesarias y cargaremos el dataset MNIST. También explicaremos el dataset y proporcionaremos el enlace original.

#### Explicación del Código y Dataset

```python
# Importación de Bibliotecas
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt
import numpy as np

# Cargando y Preprocesando el Dataset MNIST
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalización de las Imágenes
x_train = x_train.reshape(-1, 28*28).astype('float32') / 255
x_test = x_test.reshape(-1, 28*28).astype('float32') / 255

# Conversión de Etiquetas a Categóricas
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Visualización de ejemplos de imágenes y sus etiquetas
plt.figure(figsize=(10, 5))
for i in range(10):
    plt.subplot(2, 5, i+1)
    plt.imshow(x_train[i].reshape(28, 28), cmap='gray')
    plt.title(f"Etiqueta: {np.argmax(y_train[i])}")
    plt.axis('off')
plt.show()
```

### Explicación del Dataset MNIST

El dataset MNIST (Modified National Institute of Standards and Technology) es una colección de imágenes de dígitos escritos a mano, ampliamente utilizado para entrenar y probar modelos de reconocimiento de imágenes. El dataset contiene:
- **60,000 imágenes de entrenamiento**: utilizadas para entrenar el modelo.
- **10,000 imágenes de prueba**: utilizadas para evaluar el rendimiento del modelo.

Cada imagen tiene un tamaño de 28x28 píxeles y está en escala de grises. Las etiquetas corresponden a dígitos del 0 al 9.

Enlace original al dataset MNIST: [MNIST Database](http://yann.lecun.com/exdb/mnist/)

### Definiendo la Estructura de la Red Neuronal

Ahora definiremos la estructura básica de nuestra red neuronal usando Keras, una biblioteca de alto nivel para redes neuronales.

```python
# Definición de la Estructura de la Red Neuronal
model = Sequential([
    Dense(512, input_shape=(784,), activation='relu'), # Primera capa oculta con 512 neuronas y ReLU
    Dropout(0.2), # Dropout con el 20% de las neuronas apagadas durante el entrenamiento
    Dense(512, activation='relu'), # Segunda capa oculta con 512 neuronas y ReLU
    Dropout(0.2), # Dropout con el 20% de las neuronas apagadas durante el entrenamiento
    Dense(10, activation='softmax') # Capa de salida con 10 neuronas (una por clase) y Softmax
])

# Compilando el Modelo
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Resumen de la Red Neuronal
model.summary()
```

### Explicación de la Estructura de la Red Neuronal

- **Primera Capa Oculta**: Tiene 512 neuronas y utiliza la función de activación ReLU (Rectified Linear Unit) para introducir no linealidades en el modelo.
- **Dropout**: Aplica Dropout con una tasa del 20% para evitar el sobreajuste.
- **Segunda Capa Oculta**: Similar a la primera, con 512 neuronas y ReLU.
- **Dropout**: Otro Dropout con una tasa del 20%.
- **Capa de Salida**: Tiene 10 neuronas, una para cada clase en el dataset MNIST, y utiliza la función de activación Softmax para producir probabilidades de clasificación.

La red se compila utilizando la pérdida de entropía cruzada categórica y el optimizador Adam, y se evalúa la precisión durante el entrenamiento.

### Entrenamiento del Modelo

#### Propagación Hacia Adelante

La Propagación Hacia Adelante es el proceso mediante el cual los datos de entrada se transmiten a través de la red neuronal para generar una salida. Este flujo de información comienza en la capa de entrada, pasa por las capas ocultas y finalmente llega a la capa de salida.

### Explicación del Proceso

1. **Entrada**: Los datos de entrada se presentan a la red neuronal.
2. **Ponderación**: Cada neurona en la capa de entrada envía sus datos ponderados a cada neurona en la primera capa oculta.
3. **Activación**: Las neuronas en la capa oculta calculan una suma ponderada de sus entradas, aplican una función de activación y transmiten el resultado a la siguiente capa.
4. **Salida**: Este proceso se repite capa por capa hasta que los datos llegan a la capa de salida, donde se generan las predicciones finales.

```python
# Propagación Hacia Adelante usando Keras
# Ya hemos definido y compilado el modelo en el paso anterior
# Entrenamiento del Modelo
history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2, verbose=1)
```

### Visualización del Proceso

Podemos visualizar cómo se transmiten los datos a través de la red utilizando gráficos de entrenamiento.

```python
# Graficando precisión y pérdida durante el entrenamiento
plt.figure(figsize=(12, 4))
# Precisión
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Precisión de Entrenamiento')
plt.plot(history.history['val_accuracy'], label='Precisión de Validación')
plt.title('Precisión durante el Entrenamiento')
plt.xlabel('Época')
plt.ylabel('Precisión')
plt.legend()
# Pérdida
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Pérdida de Entrenamiento')
plt.plot(history.history['val_loss'], label='Pérdida de Validación')
plt.title('Pérdida durante el Entrenamiento')
plt.xlabel('Época')
plt.ylabel('Pérdida')
plt.legend()
plt.show()
```

### Evaluación del Modelo

Evaluaremos el rendimiento del modelo en el conjunto de datos de prueba y mostraremos ejemplos de predicciones.

```python
# La retropropagación y el ajuste de pesos se realizan automáticamente durante el entrenamiento
# utilizando el método fit como se mostró anteriormente
# Aquí mostramos la evaluación del modelo
loss, accuracy = model.evaluate(x_test, y_test, verbose=0)
print(f'Pérdida en el conjunto de prueba: {loss:.4f}')
print(f'Precisión en el conjunto de prueba: {accuracy:.4f}')

import numpy as np
# Haciendo predicciones
predictions = model.predict(x_test)

# Mostrando ejemplos de predicciones
num_rows, num_cols = 2, 5
num_images = num_rows * num_cols
plt.figure(figsize=(10, 5))
for i in range(num_images):
    plt.subplot(num_rows, num_cols, i+1)
    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')
    plt.title(f"Pred: {np.argmax(predictions[i])}")
    plt.axis('off')
plt.show()
```

### Técnicas de Regularización

Implementaremos y explicaremos técnicas como Dropout y regularización L2, y mostraremos cómo estas técnicas afectan el rendimiento del modelo.

```python
from keras.layers import Dropout
# Redefiniendo el modelo con Dropout y Regularización L2
from keras.regularizers import l2

model = Sequential([
    Dense(512, activation='relu', input_shape=(784,), kernel_regularizer=l2(0.001)),
    Dropout(0.5),
    Dense(512, activation='relu', kernel_regularizer=l2(0.001)),
    Dropout(0.5),
    Dense(10, activation='softmax')
])
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Entrenando el modelo con regularización
history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2, verbose=1)

# Graficando precisión y pérdida durante el entrenamiento con regularización
plt.figure(figsize=(12, 4))
#

 Precisión
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Precisión de Entrenamiento')
plt.plot(history.history['val_accuracy'], label='Precisión de Validación')
plt.title('Precisión durante el Entrenamiento con Regularización')
plt.xlabel('Época')
plt.ylabel('Precisión')
plt.legend()
# Pérdida
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Pérdida de Entrenamiento')
plt.plot(history.history['val_loss'], label='Pérdida de Validación')
plt.title('Pérdida durante el Entrenamiento con Regularización')
plt.xlabel('Época')
plt.ylabel('Pérdida')
plt.legend()
plt.show()
```

### Recursos para Explorar Más:

- **[Hola Mundo del Deep Learning](https://youtube.com/playlist?list=PLWP2CHQigyURotrsA7m39odxXuYAOMvEc&si=nJKJn4Xm1szrwGEZ)** PlayList de 0 a 100 para poder hacer y enteder el hola mundo del Deep Learning
- **[Taller - Fundamentos de Deep Learning con Python y PyTorch](https://youtu.be/XtLpw3SFrz4?si=YeQQu8yB_zmoxcf4)** 

## Colab Notebooks


- [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1jokMDImAKHwhucMxo6ZW1Cs2OXlHUpyR?usp=sharing) [Día 11: Hola Mundo (Deep Learning)](https://colab.research.google.com/drive/1jokMDImAKHwhucMxo6ZW1Cs2OXlHUpyR?usp=sharing)

---
# Día12
---
## ¿Qué son las Redes Profundas? 🌐🧠


Las **redes profundas**, también conocidas como **redes neuronales profundas**, son un tipo de arquitectura de aprendizaje profundo que consta de múltiples capas de neuronas artificiales. A diferencia de las redes neuronales poco profundas, que tienen solo una o dos capas ocultas, las redes profundas pueden tener muchas capas ocultas, lo que les permite aprender representaciones cada vez más abstractas y complejas de los datos de entrada.


https://github.com/Oliver369X/100DaysOfAI/assets/110129950/10319956-2748-4d00-885a-f9f590ead99f


### Características Principales:

1. **Capas Ocultas Múltiples**: Las redes profundas consisten en una serie de capas ocultas entre la capa de entrada y la capa de salida. Cada capa oculta realiza transformaciones no lineales en los datos de entrada, permitiendo que el modelo aprenda características jerárquicas.

2. **Aprendizaje Jerárquico de Características**: A medida que los datos fluyen a través de las capas de la red, se extraen y aprenden características cada vez más abstractas y significativas. Esto permite a las redes profundas capturar y modelar relaciones complejas en los datos.

3. **Representaciones de Datos Abstracciones**: Las capas intermedias de una red profunda actúan como extractores de características, aprendiendo representaciones de datos cada vez más abstractas y de alto nivel. Estas representaciones abstraídas son esenciales para la capacidad del modelo de comprender y generalizar a partir de datos no vistos.

### Aplicaciones:

- **Visión por Computadora**: Las redes profundas han demostrado un rendimiento sobresaliente en tareas como clasificación de imágenes, detección de objetos, segmentación semántica y generación de imágenes.

- **Procesamiento del Lenguaje Natural**: En el campo del procesamiento del lenguaje natural (NLP), las redes profundas se utilizan para tareas como clasificación de texto, traducción automática, generación de texto y análisis de sentimientos.

- **Reconocimiento de Voz**: Las redes profundas son fundamentales en sistemas de reconocimiento de voz, donde se utilizan para traducir señales de audio en texto y viceversa.



## Ventajas y Desafíos de Redes Más Profundas 🌟🧠

Vamos a explorar las ventajas y desafíos asociados con el uso de **redes más profundas** en el aprendizaje profundo. Estas redes neuronales, con múltiples capas ocultas, han demostrado ser poderosas en la extracción de características complejas de los datos, pero también presentan ciertos desafíos que debemos tener en cuenta. ¡Vamos a sumergirnos en este tema! 🚀📊

### Ventajas de las Redes Más Profundas:

1. **Extracción Jerárquica de Características**: Las redes profundas pueden aprender representaciones de datos jerárquicas y complejas, lo que les permite capturar características abstractas y significativas de los datos de entrada.

2. **Mayor Capacidad de Aprendizaje**: Con más capas ocultas, las redes profundas tienen una mayor capacidad para aprender y modelar relaciones complejas en los datos, lo que puede llevar a un rendimiento mejorado en tareas de aprendizaje automático.

3. **Generalización Mejorada**: Al aprender representaciones de datos más abstractas y de alto nivel, las redes profundas tienden a generalizar mejor a datos no vistos, lo que les permite realizar predicciones precisas en nuevas instancias.

4. **Rendimiento Superior en Tareas Complejas**: Las redes más profundas han demostrado un rendimiento sobresaliente en una variedad de tareas complejas, como la visión por computadora, el procesamiento del lenguaje natural y el reconocimiento de voz.

### Desafíos de las Redes Más Profundas:

1. **Dificultad de Entrenamiento**: Entrenar redes profundas puede ser computacionalmente costoso y requiere grandes conjuntos de datos etiquetados, así como una capacidad de cómputo significativa, lo que puede ser un desafío en entornos con recursos limitados.

2. **Sobreajuste (Overfitting)**: Las redes profundas pueden ser propensas al sobreajuste, especialmente en conjuntos de datos pequeños o ruidosos, lo que puede resultar en un rendimiento deficiente en datos no vistos.

3. **Gradiente que Desaparece/Explode**: En redes muy profundas, el gradiente puede desvanecerse (cuando se vuelve muy pequeño) o explotar (cuando se vuelve muy grande) durante el entrenamiento, lo que puede dificultar la convergencia del modelo.

4. **Interpretabilidad Limitada**: A medida que aumenta la complejidad de la red, la interpretación de sus decisiones puede volverse más difícil, lo que puede ser problemático en aplicaciones donde la transparencia y la explicabilidad son importantes.

### Recursos para Explorar Más:

- **[¿Cuáles son los desafíos y limitaciones actuales de las redes neuronales y el aprendizaje profundo?](https://www.linkedin.com/advice/3/what-current-challenges-limitations-neural?lang=es&originalSubdomain=es)**.





---

# Día13
---
## Conceptos básicos y arquitectura general de las CNNs 🧠🖼️


https://github.com/Oliver369X/100DaysOfAI/assets/110129950/f76483f7-fe9f-4c48-8e66-bc8ab8d8d360


1️⃣ Definición de CNN 🤖
Las Redes Neuronales Convolucionales son un tipo especializado de red neuronal diseñada principalmente para procesar datos con estructura de cuadrícula, como imágenes. Se inspiran en el procesamiento visual del cerebro humano y son muy eficaces en tareas de visión por computador. 👁️‍🗨️

2️⃣ Componentes principales de una CNN 🧱
a) Capa de entrada: Recibe la imagen como tensor 3D
b) Capas convolucionales: Aplican filtros para detectar características
c) Funciones de activación: Introducen no-linealidad (típicamente ReLU)
d) Capas de pooling: Reducen la dimensionalidad espacial
e) Capa de aplanamiento: Convierte datos en vector unidimensional
f) Capas completamente conectadas: Realizan la clasificación final
g) Capa de salida: Produce la predicción final

3️⃣ Proceso de convolución 🔄
- Operación fundamental en CNNs
- Un filtro se desliza sobre la imagen de entrada
- Multiplicación elemento por elemento y suma del resultado
- Crea un mapa de características que resalta patrones específicos

4️⃣ Características clave de las CNNs 🔑
a) Conectividad local: Cada neurona se conecta solo a una región local
b) Compartición de parámetros: Mismos pesos en múltiples ubicaciones
c) Invariancia a la traslación: Detectan características independientemente de su posición

### Recursos para Explorar Más:

- **[funcionamiento de las redes neuronales convolucionales](https://youtu.be/4sWhhQwHqug?si=qvxBksruxjAbWVkC)** 
- **[¡Redes Neuronales CONVOLUCIONALES! ¿Cómo funcionan?](https://youtu.be/V8j1oENVz00?si=1PNlj6GPLEqP66sZ)**

---

# Día14
----
## ¿Cómo funcionan las CNNs en comparación con las ANNs? 🤔🔍
Vamos a explorar cómo funcionan las Redes Neuronales Convolucionales (CNNs) en comparación con las Redes Neuronales Artificiales (ANNs). Ambas son arquitecturas importantes en el campo del aprendizaje profundo, pero tienen diferencias clave en su estructura y funcionamiento. ¡Vamos a analizarlas! 🧠📊

### Redes Neuronales Artificiales (ANNs):

Las Redes Neuronales Artificiales (ANNs), también conocidas como perceptrones multicapa, son una arquitectura clásica de redes neuronales que consiste en múltiples capas de neuronas artificiales interconectadas. Cada neurona en una capa está conectada a todas las neuronas de la capa siguiente, lo que permite una representación compleja de funciones no lineales.

**Funcionamiento:**
1. **Propagación hacia Adelante (Forward Propagation):** Durante la propagación hacia adelante, los datos de entrada se alimentan a través de la red neuronal, capa por capa, y se calculan las activaciones de cada neurona utilizando una combinación lineal de las entradas y pesos, seguida de una función de activación no lineal.

2. **Cálculo del Error:** Después de la propagación hacia adelante, se compara la salida predicha de la red con la salida deseada utilizando una función de pérdida, y se calcula el error de predicción.

3. **Propagación hacia Atrás (Backward Propagation):** Durante la propagación hacia atrás, el error calculado se propaga hacia atrás a través de la red para ajustar los pesos de cada neurona, utilizando algoritmos de optimización como el descenso de gradiente estocástico (SGD).

### Redes Neuronales Convolucionales (CNNs):

Las Redes Neuronales Convolucionales (CNNs) son una variante especializada de las ANNs diseñadas específicamente para el procesamiento de imágenes. Integran capas convolucionales que aplican filtros a las imágenes de entrada para extraer características relevantes de manera eficiente.

**Principales Diferencias:**
1. **Estructura:** Mientras que las ANNs están completamente conectadas, las CNNs utilizan capas convolucionales y de pooling para operar directamente sobre las características de la imagen, lo que reduce drásticamente el número de parámetros y la complejidad computacional.

2. **Convolución:** Las CNNs utilizan operaciones de convolución para detectar características locales en las imágenes, lo que les permite capturar patrones espaciales y de proximidad que son fundamentales en tareas de visión por computadora.

3. **Parámetros Compartidos:** En las CNNs, los mismos pesos de filtro se comparten en diferentes regiones de la imagen, lo que les permite generalizar y aprender patrones independientemente de su ubicación en la imagen.

### Aplicaciones:
- Las ANNs son más adecuadas para tareas de aprendizaje supervisado en datos tabulares o secuenciales.
- Las CNNs son ideales para tareas de visión por computadora, como reconocimiento de objetos, detección de objetos, segmentación semántica y más.

### Recursos para Explorar Más:
- **[CNN vs RNN vs ANN: Explicando las redes neuronales](https://www.linkedin.com/advice/0/how-do-you-explain-concepts-intuitions-behind?lang=es&originalSubdomain=es)**.


---
# Día15
---
## Ejemplos Prácticos de Aplicación en la Industria 🏭🤖

Vamos a explorar algunos ejemplos prácticos de cómo se aplican las redes neuronales convolucionales (CNNs) en la industria. Las CNNs son una poderosa herramienta en el campo del aprendizaje profundo, especialmente en aplicaciones de visión por computadora. Veamos algunos ejemplos interesantes:


https://github.com/Oliver369X/100DaysOfAI/assets/110129950/996d7620-d10f-40dd-b6ae-329945c07cd0


#### 1. Diagnóstico Médico:
- **Detección de Cáncer de Mama:** Las CNNs pueden analizar imágenes de mamografías para detectar signos tempranos de cáncer de mama, ayudando a los médicos en el diagnóstico precoz y la planificación del tratamiento.

#### 2. Automatización Industrial:
- **Inspección de Calidad en Manufactura:** Las CNNs pueden inspeccionar visualmente productos manufacturados en busca de defectos o imperfecciones, garantizando la calidad del producto final y reduciendo el desperdicio.

#### 3. Automotriz y Conducción Autónoma:
- **Detección de Peatones y Objetos:** Las CNNs integradas en sistemas de conducción autónoma pueden identificar peatones, vehículos y otros objetos en tiempo real, permitiendo que los vehículos tomen decisiones de conducción seguras.

#### 4. Agricultura de Precisión:
- **Monitoreo de Cultivos:** Las CNNs pueden analizar imágenes satelitales para monitorear el crecimiento de los cultivos, identificar áreas de estrés vegetal y optimizar el uso de recursos agrícolas como el agua y los fertilizantes.

#### 5. Seguridad y Vigilancia:
- **Reconocimiento Facial y de Objeto:** Las CNNs pueden analizar imágenes de cámaras de seguridad para identificar caras de interés, detectar intrusiones no autorizadas y alertar sobre actividades sospechosas.

#### 6. Retail y Experiencia del Cliente:
- **Personalización de Recomendaciones:** Las CNNs pueden analizar el historial de compras y las preferencias del cliente para ofrecer recomendaciones de productos altamente personalizadas, mejorando la experiencia de compra en línea.

## Recursos sobre Aplicaciones Prácticas de Redes Neuronales Convolucionales (CNNs):

### **1. Diagnóstico Médico:**

* **Detección de Cáncer de Mama:**
    * **Artículo:** "Aplicación de redes neuronales convolucionales para la detección de cáncer de mama en imágenes de mamografía" ([https://revistascientificas.cuc.edu.co/CESTA/article/view/3922/3919](https://revistascientificas.cuc.edu.co/CESTA/article/view/3922/3919))
    * **Video:** "Redes Neuronales Convolucionales para Detección de Cáncer de Mama" ([https://www.youtube.com/watch?v=06TugnwqZCQ](https://www.youtube.com/watch?v=06TugnwqZCQ))

### **2. Automatización Industrial:**

* **Inspección de Calidad en Manufactura:**
    * **Artículo:** "Inspección de defectos en productos manufacturados utilizando redes neuronales convolucionales" ([http://www.scielo.org.co/scielo.php?script=sci_arttext&pid=S0121-750X2023000100204](http://www.scielo.org.co/scielo.php?script=sci_arttext&pid=S0121-750X2023000100204))
    * **Video:** "Automatización de la Inspección Visual en la Industria Manufacturera con Redes Neuronales Convolucionales" ([https://www.youtube.com/watch?v=FkvWe00Pjgs](https://www.youtube.com/watch?v=FkvWe00Pjgs))

### **3. Automotriz y Conducción Autónoma:**

* **Detección de Peatones y Objetos:**

    * **Video:** "Visión Artificial para Vehículos Autónomos: Detección de Peatones y Objetos con Redes Neuronales Convolucionales" ([https://www.youtube.com/watch?v=WC8dm4dxqPw](https://www.youtube.com/watch?v=WC8dm4dxqPw))

---

# Día16
---
## Comprendiendo la Convolución en Imágenes 📸🔍


https://github.com/Oliver369X/100DaysOfAI/assets/110129950/65915ade-08c5-47b8-8b9b-85dbc69435f8


#### ¿Qué es la Convolución?
La convolución es una operación matemática fundamental en el procesamiento de señales y el aprendizaje profundo. En el contexto de las imágenes, la convolución implica deslizar una pequeña ventana (llamada kernel o filtro) sobre la imagen de entrada y realizar operaciones matemáticas en cada región de la imagen.

#### Aplicación en Imágenes:
- **Extracción de Características:** La convolución se utiliza para extraer características importantes de una imagen, como bordes, texturas y patrones, mediante la detección de características locales en diferentes partes de la imagen.
- **Reducción de Dimensionalidad:** Al aplicar convoluciones sucesivas con diferentes filtros, se obtienen mapas de características que resumen la información clave de la imagen, lo que permite una representación más compacta y manejable para la red neuronal.
- **Detección de Objetos:** En el contexto del aprendizaje profundo, las convoluciones son fundamentales en las arquitecturas de redes neuronales convolucionales (CNNs) para la detección y clasificación de objetos en imágenes.

#### Proceso de Convolución:
1. **Deslizamiento del Kernel:** El kernel se desliza sobre la imagen de entrada, multiplicando sus valores por los píxeles correspondientes en cada región.
2. **Operación de Producto Punto:** Se calcula el producto punto entre los valores del kernel y los píxeles de la región de la imagen.
3. **Suma y Bias:** Se suman los resultados de la operación de producto punto y se agrega un término de sesgo (bias).
4. **Aplicación de Función de Activación:** Opcionalmente, se aplica una función de activación no lineal, como ReLU, para introducir no linealidades en la red.

### Recursos para Explorar Más:
- **[La CONVOLUCIÓN en las REDES CONVOLUCIONALES](https://youtu.be/ySbmdeqR0-4?si=_lp6W3jjBWVu0E5e)**.
- **[Convoluciones y filtros](https://youtu.be/AwTH_0yW9_I?si=2EuPLMROMmReZR1T)**.

---

# Día17
---
## Entendiendo los Filtros y su Papel en la Extracción de Características 🌟🔍


https://github.com/Oliver369X/100DaysOfAI/assets/110129950/15eb563d-a718-4ab5-9153-42a19c8839e1


Hoy vamos a explorar más a fondo los filtros en el contexto de las redes neuronales convolucionales (CNNs) y cómo desempeñan un papel crucial en la extracción de características de las imágenes.

#### ¿Qué son los Filtros en CNNs?
Los filtros, también conocidos como kernels, son matrices pequeñas de pesos que se utilizan en las capas convolucionales de las CNNs. Cada filtro se desliza sobre la imagen de entrada y realiza operaciones de convolución para extraer características específicas.

#### Función de los Filtros:
- **Detección de Características:** Cada filtro está diseñado para detectar una característica específica en la imagen, como bordes, texturas, formas o patrones.
- **Aprendizaje de Características:** Durante el entrenamiento de la red neuronal, los valores de los filtros se ajustan automáticamente para aprender las características más relevantes para la tarea específica.

#### Proceso de Extracción de Características:
1. **Convolución:** El filtro se aplica a la imagen de entrada mediante la operación de convolución, multiplicando sus valores por los píxeles correspondientes y sumando los resultados.
2. **Mapa de Activación:** La salida de la convolución se conoce como mapa de activación, que resalta la presencia de la característica detectada en diferentes regiones de la imagen.
3. **Pooling:** Opcionalmente, se puede aplicar una capa de pooling después de la convolución para reducir la dimensionalidad y mejorar la eficiencia computacional.



#### Importancia en el Aprendizaje Profundo:
- Los filtros son esenciales para el aprendizaje profundo, ya que permiten que la red neuronal aprenda representaciones jerárquicas de las características de las imágenes.
- Al apilar capas convolucionales con diferentes filtros, la red puede aprender características cada vez más abstractas y complejas, lo que mejora su capacidad para realizar tareas de visión por computadora.


### Recursos para Explorar Más:
- **[Filtros espaciales aplicados a imágenes](https://youtu.be/K9Tx4NOWUSg?si=4UdJDFUQuzCJRTJJ)**.

---

# Día18
---
## Stride y Padding en CNNs 🚶🏻‍♂️🛌

Hoy vamos a explorar dos conceptos importantes en las redes neuronales convolucionales (CNNs): Stride y Padding. Estos conceptos son fundamentales para el diseño y la configuración de las capas convolucionales.

#### Stride:
- **Definición:** El stride (paso) es la cantidad de píxeles que el filtro se desplaza en cada paso mientras se aplica a la imagen de entrada.
- **Efecto:** Un stride mayor reduce la dimensión espacial de la salida (mapa de activación), ya que el filtro se mueve más rápido a lo largo de la imagen.
- **Control de Dimensionalidad:** El stride se utiliza para controlar la reducción de dimensionalidad en las capas convolucionales, lo que puede ser útil para reducir el costo computacional y el overfitting.

#### Padding:
- **Definición:** El padding (relleno) consiste en agregar píxeles adicionales alrededor de la imagen de entrada antes de aplicar la convolución.
- **Uso:** El padding se utiliza para mantener la dimensión espacial de la salida después de la convolución, especialmente en los bordes de la imagen.
- **Beneficios:** Al agregar padding, se conserva más información espacial de la imagen de entrada y se evita la pérdida de características en los bordes.
- **Tipos de Padding:** Se pueden utilizar diferentes tipos de padding, como "same" (mismo tamaño de entrada y salida) o "valid" (sin relleno), según los requisitos de la arquitectura de la red.




#### Importancia en las CNNs:
- El stride y el padding son parámetros importantes que afectan la dimensión espacial de la salida y la cantidad de información preservada.
- Ajustar adecuadamente el stride y el padding puede mejorar el rendimiento y la eficiencia de la red neuronal convolucional en tareas de visión por computadora.

### Recursos para Explorar Más:
- **[Padding, strides, max pooling y stacking en las REDES CONVOLUCIONALES](https://youtu.be/QLy8v6LL_4A?si=6ElSwovGCi-Eljj3)**.

---

# Día19
---
## Pooling en CNNs 🏊‍♂️🔍

¡Hola a todos! Hoy vamos a explorar una técnica fundamental en las redes neuronales convolucionales (CNNs): el Pooling. El Pooling es una operación importante para la reducción de dimensionalidad y la extracción de características en las CNNs.

![Pooling](https://github.com/Oliver369X/100DaysOfAI/assets/110129950/9e06b087-f42c-4ce3-af88-cbfc80ce9d82)
![pooling](https://github.com/Oliver369X/100DaysOfAI/assets/110129950/38bc0a1a-fc89-4c66-94d4-5c43a2443bb9)

#### Introducción al Pooling:
- **Definición:** El Pooling es una operación que reduce la dimensionalidad de cada mapa de activación, conservando solo la información más importante.
- **Tipos de Pooling:** Los tipos comunes de Pooling son el Max Pooling y el Average Pooling.
- **Funcionamiento:** En el Max Pooling, se selecciona el valor máximo de un área definida en el mapa de activación. En el Average Pooling, se calcula el promedio de los valores en el área especificada.
- **Reducción de Dimensionalidad:** El Pooling reduce el tamaño espacial de la entrada, lo que disminuye el número de parámetros y operaciones en la red neuronal.

#### Impacto en las CNNs:
- **Reducción de Overfitting:** Al reducir la dimensionalidad, el Pooling ayuda a prevenir el overfitting al eliminar información redundante y mejorar la generalización del modelo.
- **Invariancia a las Transformaciones:** El Pooling hace que la red sea más invariante a pequeñas traslaciones y deformaciones en las características detectadas.
- **Extracción de Características:** Al conservar solo las características más importantes, el Pooling facilita la identificación de patrones relevantes en los mapas de activación.



En la imagen de arriba, se muestra un ejemplo de Max Pooling aplicado a un mapa de activación. La región de 2x2 se desliza sobre el mapa, seleccionando el valor máximo en cada región para formar la salida.

### Recursos para Explorar Más:
- **[CNN vs RNN vs ANN: Explicando las redes neuronales](https://www.linkedin.com/advice/0/how-do-you-explain-concepts-intuitions-behind?lang=es&originalSubdomain=es)**.
- **[Capas de pooling en una red neuronal convolucional](https://keepcoding.io/blog/capas-pooling-red-neuronal-convolucional/)**.
- **[Pooling and their types in CNN
](https://medium.com/@abhishekjainindore24/pooling-and-their-types-in-cnn-4a4b8a7a4611)**.

---

# Día20
---
##  Funciones de Activación

- **Definición**: Las funciones de activación son componentes cruciales en las redes neuronales que introducen no-linealidad en el modelo.
 - **Propósito**: Permiten a la red aprender y aproximar funciones complejas.
- **Importancia**: Sin ellas, la red sería equivalente a un modelo lineal simple.

#### ReLU (Rectified Linear Unit)
**Definición Matemática**: f(x) = max(0, x)
**Funcionamiento**:
- Si la entrada es negativa, la salida es 0.
- Si la entrada es positiva, la salida es igual a la entrada.
**Ventajas**:
- Reduce el problema del desvanecimiento del gradiente.
- Computacionalmente eficiente.
- Converge más rápido que las funciones sigmoide o tangente hiperbólica.
**Desventajas**:
- Problema de "neuronas muertas": si una neurona siempre produce salidas negativas, puede "morir" y dejar de aprender.

#### LeakyReLU
**Definición Matemática**: f(x) = max(αx, x), donde α es un valor pequeño (típicamente 0.01).
**Funcionamiento**:
- Similar a ReLU, pero permite un pequeño gradiente negativo cuando la unidad no está activa.
**Ventajas sobre ReLU**:
- Evita el problema de las neuronas muertas.
- Permite un pequeño flujo de gradientes negativos.
**Cómo elegir el valor de α**:
- Generalmente se usa 0.01, pero puede ser un hiperparámetro a optimizar.

#### Otras Variantes de ReLU
**a) PReLU (Parametric ReLU)**
- Similar a LeakyReLU, pero α es un parámetro aprendible.
- Puede adaptarse mejor a los datos específicos del problema.

**b) ELU (Exponential Linear Unit)**
- **Definición**: f(x) = x si x > 0, α(exp(x) - 1) si x ≤ 0.
- Produce salidas negativas suaves, lo que puede ayudar a empujar las activaciones medias más cerca de cero.

#### Implementación Práctica
**En TensorFlow/Keras**:
```python
from tensorflow.keras.layers import ReLU, LeakyReLU

# ReLU
model.add(ReLU())

# LeakyReLU
model.add(LeakyReLU(alpha=0.01))
```

**En PyTorch**:
```python
import torch.nn as nn

# ReLU
model.add_module('relu', nn.ReLU())

# LeakyReLU
model.add_module('leaky_relu', nn.LeakyReLU(negative_slope=0.01))
```

#### Consideraciones al Elegir Funciones de Activación
- Depende del problema específico y la arquitectura de la red.
- ReLU es una buena opción por defecto para capas ocultas.
- Para la capa de salida, la elección depende del tipo de problema (por ejemplo, softmax para clasificación multiclase).

### Recursos para Explorar Más:
- **[La FUNCIÓN DE ACTIVACIÓN
](https://youtu.be/lFODTDO8mMw?si=XZ0tsIUvYpqrtVzz)**.
- **[Funciones de Activación – Fundamentos de Deep Learning ](https://youtu.be/IdlYuBKeFXo?si=5RwnIieB0vBf-3o0)**.
- **[Clase 5 - Deep Learning - Funciones de activación: ReLU, Softmax](https://youtu.be/psVhj3Y8_rw?si=dzM13mjw1a_kc7cl)**.

---
# Día21
---
## Construcción de Capas en CNNs 🛠️🧱

### Construcción de Capas Convolucionales: 🔍
* **Definición:** Las capas convolucionales son fundamentales en las CNNs para la detección de características en datos de alta dimensión, como imágenes.
* **Operación de Convolución:** La operación de convolución aplica un filtro (o kernel) a una región de la entrada, produciendo un mapa de activación que resalta ciertas características.
* **Parámetros:** Las capas convolucionales tienen parámetros que se aprenden durante el entrenamiento de la red, lo que permite adaptarse a patrones específicos en los datos de entrada.
* **Construcción de Capas:** En la construcción de una capa convolucional, se especifican el número de filtros, el tamaño del filtro, el paso (stride) y el tipo de relleno (padding) para controlar la salida de la capa.

```python
import tensorflow as tf

inputs = tf.keras.Input(shape=(28, 28, 1))
x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
```

### Construcción de Capas de Pooling: 🔽
* **Reducción de Dimensionalidad:** Las capas de pooling reducen la dimensionalidad de los mapas de activación, manteniendo las características más importantes.
* **Operación de Pooling:** El Max Pooling y el Average Pooling son operaciones comunes en las capas de pooling, que seleccionan el valor máximo o calculan el promedio en una región definida.
* **Conexión con Capas Convolutivas:** Las capas de pooling suelen seguir a las capas convolucionales para reducir la resolución espacial y el número de parámetros.

```python
x = tf.keras.layers.MaxPooling2D((2, 2))(x)
```

### Conexión de Capas y Formación de una Red Profunda: 🏗️
* **Construcción de la Red:** Las capas convolucionales y de pooling se apilan para formar una red profunda. La conexión entre estas capas permite que la red aprenda representaciones jerárquicas de los datos.
* **Apilamiento de Capas:** Las capas convolucionales y de pooling se apilan secuencialmente, seguidas a menudo por capas totalmente conectadas (densas) para la clasificación final.

```python
x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)
x = tf.keras.layers.MaxPooling2D((2, 2))(x)
x = tf.keras.layers.Flatten()(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
outputs = tf.keras.layers.Dense(10, activation='softmax')(x)

model = tf.keras.Model(inputs=inputs, outputs=outputs)

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
```


### Recursos para Explorar Más:
- **[¿Qué es una red neuronal convolucional (CNN) y qué capas tiene?](https://youtu.be/3u3wW4T4sSA?si=cud0FqPhhwFwkvnR)**.
- **[Capas convolucionales y de pooling
](https://youtu.be/oTjzC8yxrRs?si=ijO9X7zFowr4j2Gp)**.

---

# Día22
---
## Capas Completamente Conectadas (Fully Connected Layers) 🔗🤖

#### Integración de Capas Completamente Conectadas:
- **Definición:** Las capas completamente conectadas, también conocidas como capas densas, son aquellas donde cada neurona está conectada a todas las neuronas de la capa anterior.
- **Transformación de Datos:** Después de varias capas convolucionales y de pooling, las características extraídas se aplanan en un vector de una dimensión antes de ser alimentadas a las capas completamente conectadas.
- **Función:** Estas capas combinan las características aprendidas para tomar decisiones finales. Son esenciales para tareas de clasificación y regresión.

#### Uso en la Fase de Clasificación Final:
- **Proceso de Clasificación:** En una CNN típica, después de que las capas convolucionales y de pooling han extraído y reducido las características, las capas completamente conectadas procesan esta información para realizar la clasificación final.
- **Softmax y Activaciones:** La última capa completamente conectada en un modelo de clasificación suele utilizar una función de activación softmax para convertir las salidas en probabilidades de las diferentes clases.
- **Entrenamiento:** Durante el entrenamiento, los pesos de las capas completamente conectadas se ajustan para minimizar la función de pérdida, mejorando la precisión de las predicciones.

#### Estructura de una CNN con Capas Completamente Conectadas:
- **Capas Iniciales:** Varias capas convolucionales y de pooling para extraer características.
- **Aplanamiento:** Transformación de los mapas de características en un vector de una dimensión.
- **Capas Densas:** Una o más capas completamente conectadas que procesan el vector de características.
- **Clasificación Final:** Una capa completamente conectada final con softmax para la salida de clasificación.

Las capas completamente conectadas juegan un papel crucial en la toma de decisiones finales de una CNN, integrando todas las características aprendidas y proporcionando la salida del modelo.
### Recursos para Explorar Más:
- **[Capa totalmente conectada](https://es.mathworks.com/help/deeplearning/ref/nnet.cnn.layer.fullyconnectedlayer.html)**.
- **[Fully Connected Layer
](https://medium.com/@vaibhav1403/fully-connected-layer-f13275337c7c)**.
- **[Layer (deep learning)
](https://en.wikipedia.org/wiki/Layer_(deep_learning))**.


---
# Día23
---
## Regularización en CNNs 📚🛡️

¡Hola a todos! Hoy, en el día 23 de nuestro desafío #100DaysOfAI, vamos a explorar las **técnicas de regularización en CNNs**. Estas técnicas son esenciales para prevenir el overfitting y asegurar que nuestros modelos generalicen bien en datos no vistos. ¡Vamos a sumergirnos en ellas!

#### ¿Qué es la Regularización?

La regularización en redes neuronales y, específicamente, en CNNs, se refiere a un conjunto de técnicas utilizadas para reducir el error en un conjunto de datos de prueba que es diferente del conjunto de datos de entrenamiento. En términos sencillos, ayuda a nuestro modelo a no "memorizar" el conjunto de entrenamiento y a ser capaz de generalizar bien en datos nuevos.


#### Técnicas de Regularización en CNNs

1. **Dropout**

   Dropout es una técnica muy popular para prevenir el overfitting. Implica "desconectar" aleatoriamente algunas neuronas durante el entrenamiento. Esto fuerza a la red a no depender demasiado de ninguna neurona específica y a aprender representaciones más robustas.

   **Cómo Implementar Dropout:**
   ```python
   from tensorflow.keras.layers import Dropout, Dense

   model = Sequential()
   model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
   model.add(MaxPooling2D((2, 2)))
   model.add(Conv2D(64, (3, 3), activation='relu'))
   model.add(MaxPooling2D((2, 2)))
   model.add(Flatten())
   model.add(Dense(128, activation='relu'))
   model.add(Dropout(0.5))  # Aplicar Dropout con 50% de neuronas desconectadas
   model.add(Dense(10, activation='softmax'))
   ```

2. **Data Augmentation**

   La augmentación de datos es una técnica en la que se generan nuevas muestras de datos a partir de los datos existentes aplicando transformaciones como rotaciones, desplazamientos, cambios de escala, etc. Esto ayuda a que el modelo vea una mayor diversidad de datos durante el entrenamiento y mejore su capacidad de generalización.

   **Cómo Implementar Data Augmentation:**
   ```python
   from tensorflow.keras.preprocessing.image import ImageDataGenerator

   datagen = ImageDataGenerator(
       rotation_range=20,
       width_shift_range=0.2,
       height_shift_range=0.2,
       shear_range=0.2,
       zoom_range=0.2,
       horizontal_flip=True,
       fill_mode='nearest'
   )

   datagen.fit(X_train)
   model.fit(datagen.flow(X_train, y_train, batch_size=32), epochs=50)
   ```

3. **Regularización L2 (Weight Decay)**

   La regularización L2 añade una penalización a la función de pérdida basada en el tamaño de los pesos. Esta técnica desincentiva que los pesos crezcan demasiado, lo cual puede ayudar a prevenir el overfitting.

   **Cómo Implementar L2 Regularization:**
   ```python
   from tensorflow.keras.regularizers import l2

   model = Sequential()
   model.add(Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.01), input_shape=(28, 28, 1)))
   model.add(MaxPooling2D((2, 2)))
   model.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.01)))
   model.add(MaxPooling2D((2, 2)))
   model.add(Flatten())
   model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.01)))
   model.add(Dense(10, activation='softmax'))
   ```

4. **Batch Normalization**

   La normalización por lotes (Batch Normalization) es una técnica que normaliza las activaciones de una capa para cada mini-lote. Esto acelera el entrenamiento y puede tener un efecto regularizador.

   **Cómo Implementar Batch Normalization:**
   ```python
   from tensorflow.keras.layers import BatchNormalization

   model = Sequential()
   model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
   model.add(BatchNormalization())  # Aplicar Batch Normalization
   model.add(MaxPooling2D((2, 2)))
   model.add(Conv2D(64, (3, 3), activation='relu'))
   model.add(BatchNormalization())
   model.add(MaxPooling2D((2, 2)))
   model.add(Flatten())
   model.add(Dense(128, activation='relu'))
   model.add(BatchNormalization())
   model.add(Dense(10, activation='softmax'))
   ```

---

### Recursos Adicionales

1. **[Regularización L2 y Dropout](https://youtu.be/DVpiSJVMOVo?si=As8auc_DjMfi-sKZ)**
2. **[Dropout: A Simple Way to Prevent Neural Networks from Overfitting (JMLR)](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)**
3. **[Image Augmentation for Deep Learning with Keras](https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/)**
4. **[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (arXiv)](https://arxiv.org/abs/1502.03167)**


---
# Día24

---

## Cómo Funciona el Backpropagation en las CNNs 🧠🔄

### ¿Qué es el Backpropagation?
- **Definición:** El backpropagation, o retropropagación, es un algoritmo utilizado para ajustar los pesos de una red neuronal durante el entrenamiento, permitiendo que la red aprenda al minimizar la función de pérdida.
- **Proceso:** Involucra dos fases principales: la propagación hacia adelante (forward propagation) y la propagación hacia atrás (backward propagation).

### Propagación Hacia Adelante (Forward Propagation)
- **Paso Inicial:** Los datos de entrada se pasan a través de la red capa por capa.
- **Cálculo de la Pérdida:** Se obtiene una predicción que se compara con la etiqueta real para calcular la pérdida usando una función de pérdida.

### Propagación Hacia Atrás (Backward Propagation)
- **Cálculo del Gradiente:** Se calcula el gradiente de la función de pérdida con respecto a cada peso usando la regla de la cadena, indicando cómo cambiar los pesos para reducir la pérdida.
- **Ajuste de Pesos:** Los pesos se actualizan en la dirección opuesta al gradiente para minimizar la función de pérdida, usando un optimizador como el descenso de gradiente.

### Backpropagation en CNNs
1. **Cálculo de la Pérdida:**
   - La pérdida se calcula después de la fase de forward propagation, que implica pasar la imagen de entrada a través de capas convolucionales, de pooling y completamente conectadas.
2. **Cálculo del Gradiente en Capas Completamente Conectadas:**
   - Similar a una red neuronal estándar, se calculan los gradientes de la pérdida con respecto a los pesos y sesgos en las capas completamente conectadas.
3. **Cálculo del Gradiente en Capas Convolucionales:**
   - Los gradientes se calculan con respecto a los filtros convolucionales, propagándose hacia atrás a través de las operaciones de convolución y pooling.
   - **Convolución Transpuesta:** Se realiza una operación de convolución transpuesta (deconvolución) para calcular el gradiente con respecto a los filtros.
4. **Actualización de Pesos:**
   - Los pesos y filtros en todas las capas se actualizan usando los gradientes calculados, repitiendo el proceso hasta que la función de pérdida se minimice adecuadamente.

### Resumen del Proceso
1. **Forward Propagation:** Pasar los datos de entrada a través de la red para obtener una predicción.
2. **Cálculo de la Pérdida:** Comparar la predicción con la etiqueta real y calcular la pérdida.
3. **Backward Propagation:** Calcular los gradientes de la pérdida con respecto a los pesos y filtros.
4. **Actualización de Pesos:** Ajustar los pesos y filtros en la dirección opuesta a los gradientes.

### Recursos para Explorar Más:
- **[Cómo ven el mundo las redes neuronales convolucionales](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html)**.
- - **[Backpropagation en CNNs](https://youtu.be/kDUe0RuONYo?si=7HSe8JjALmR_oW-K)**.
---
# Día25
---
## Actualización de Pesos y Ajuste de Filtros 🛠️🔄


#### Actualización de Pesos y Filtros en CNNs

1. **Cálculo de Gradientes:**
  - Durante el proceso de backpropagation, calculamos los gradientes de la función de pérdida con respecto a cada peso y filtro en la red. Estos gradientes nos indican en qué dirección y cuánto debemos ajustar los pesos y filtros para minimizar la pérdida.

2. **Uso de un Optimizador:**
  - **Descenso de Gradiente Estocástico (SGD):** Es uno de los métodos más comunes para actualizar los pesos. El SGD ajusta los pesos en la dirección opuesta a los gradientes con una tasa de aprendizaje definida.
   - **Optimizadores Avanzados:** Otros optimizadores como Adam, RMSprop y Adagrad también se utilizan ampliamente. Estos optimizadores adaptan la tasa de aprendizaje para cada peso individualmente y pueden acelerar el proceso de convergencia.



3. **Actualización de Filtros:**
  - Similar a los pesos, los filtros en las capas convolucionales se actualizan usando los gradientes calculados durante backpropagation.
   - **Convolución Transpuesta:** Se usa para propagar los gradientes a través de las capas convolucionales y calcular el ajuste necesario para los filtros.

4. **Normalización de Pesos:**
  - Para evitar problemas como el "vanishing gradient" o "exploding gradient", es importante normalizar los pesos. Técnicas como Batch Normalization se utilizan para estabilizar y acelerar el entrenamiento.

#### Ejemplo Práctico:

Imaginemos que estamos entrenando una CNN para clasificar imágenes de gatos y perros. Durante el entrenamiento, cada imagen se pasa a través de múltiples capas convolucionales y de pooling. Después de cada pasada, calculamos la pérdida y luego los gradientes para cada peso y filtro.

Usamos un optimizador, digamos Adam, para ajustar los pesos y filtros de acuerdo a las fórmulas mencionadas anteriormente. Este proceso se repite iterativamente hasta que la pérdida se minimice y la precisión del modelo se maximice.

#### Resumen:

1. **Forward Propagation:** Pasar los datos de entrada a través de la red.
2. **Cálculo de Pérdida:** Comparar la predicción con la etiqueta real.
3. **Backward Propagation:** Calcular los gradientes.
4. **Actualización de Pesos y Filtros:** Usar un optimizador para ajustar los pesos y filtros.

La actualización de pesos y el ajuste de filtros son fundamentales para el aprendizaje efectivo de las CNNs, permitiendo que el modelo mejore su precisión con el tiempo.

### Recursos para Explorar Más:
- **[¿Qué es una red neuronal convolucional (CNN) y qué capas tiene?](https://youtu.be/3u3wW4T4sSA?si=cud0FqPhhwFwkvnR)**.
---
# Día26
---
Este proyecto tiene como objetivo desarrollar modelos de inteligencia artificial capaces de clasificar imágenes de perros y gatos utilizando técnicas avanzadas de aprendizaje profundo y aumentando los datos para mejorar la precisión del modelo. Utilizando TensorFlow y TensorFlow.js, se construyen y entrenan varios modelos neurales para lograr una clasificación precisa y robusta.

### 1. **Importación de bibliotecas y descarga del conjunto de datos**

```python
# Importar las bibliotecas necesarias
import tensorflow as tf
import tensorflow_datasets as tfds

# Corrección temporal para solucionar un error en la descarga del conjunto de datos
setattr(tfds.image_classification.cats_vs_dogs, '_URL',
        "https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip")

# Descargar el conjunto de datos de perros y gatos
datos, metadatos = tfds.load('cats_vs_dogs', as_supervised=True, with_info=True)
```

Esta celda se encarga de:

- Importar las bibliotecas TensorFlow y TensorFlow Datasets.
- Aplicar una corrección temporal a la URL de descarga del conjunto de datos.
- Descargar el conjunto de datos de perros y gatos, junto con los metadatos.

### 2. **Visualización de metadatos**

```python
# Imprimir los metadatos para revisarlos
metadatos
```

Esta celda muestra los metadatos del conjunto de datos, proporcionando información sobre el mismo.

### 3. **Visualización de ejemplos del conjunto de datos (Método 1)**

```python
# Una forma de mostrar 5 ejemplos del conjunto de datos
tfds.as_dataframe(datos['train'].take(5), metadatos)
```

Esta celda convierte 5 ejemplos del conjunto de datos de entrenamiento en un DataFrame para su visualización.

### 4. **Visualización de ejemplos del conjunto de datos (Método 2)**

```python
# Otra forma de mostrar ejemplos del conjunto de datos
tfds.show_examples(datos['train'], metadatos)
```

Esta celda utiliza una función de visualización incorporada para mostrar ejemplos del conjunto de datos de entrenamiento.

### 5. **Preprocesamiento y visualización de imágenes**

```python
# Importar matplotlib para visualización y cv2 para manipulación de imágenes
import matplotlib.pyplot as plt
import cv2

# Establecer el tamaño de la figura para la visualización
plt.figure(figsize=(20,20))

# Definir tamaño de la imagen
TAMANO_IMG = 100

# Procesar y visualizar 25 imágenes del conjunto de datos de entrenamiento
for i, (imagen, etiqueta) in enumerate(datos['train'].take(25)):
    # Redimensionar la imagen a 100x100 píxeles
    imagen = cv2.resize(imagen.numpy(), (TAMANO_IMG, TAMANO_IMG))
    # Convertir la imagen a escala de grises
    imagen = cv2.cvtColor(imagen, cv2.COLOR_BGR2GRAY)
    # Añadir la imagen al subplot correspondiente
    plt.subplot(5, 5, i + 1)
    plt.xticks([])  # Eliminar marcas del eje x
    plt.yticks([])  # Eliminar marcas del eje y
    plt.imshow(imagen, cmap='gray')  # Mostrar la imagen en escala de grises
plt.show()
```

Esta celda:

- Preprocesa las imágenes redimensionándolas a 100x100 píxeles y convirtiéndolas a escala de grises.
- Visualiza 25 imágenes del conjunto de datos de entrenamiento utilizando subplots.

### 6. **Preparación de datos de entrenamiento**

```python
# Lista que contendrá todas las imágenes preprocesadas y sus etiquetas
datos_entrenamiento = []

# Procesar todas las imágenes del conjunto de datos de entrenamiento
for i, (imagen, etiqueta) in enumerate(datos['train']):
    # Redimensionar la imagen a 100x100 píxeles
    imagen = cv2.resize(imagen.numpy(), (TAMANO_IMG, TAMANO_IMG))
    # Convertir la imagen a escala de grises
    imagen = cv2.cvtColor(imagen, cv2.COLOR_BGR2GRAY)
    # Añadir una dimensión para canales (necesario para modelos de TF)
    imagen = imagen.reshape(TAMANO_IMG, TAMANO_IMG, 1)
    # Añadir la imagen y su etiqueta a la lista de datos de entrenamiento
    datos_entrenamiento.append([imagen, etiqueta])
```

Esta celda:

- Prepara los datos de entrenamiento redimensionando todas las imágenes a 100x100 píxeles, convirtiéndolas a escala de grises y agregándoles una dimensión adicional.
- Almacena cada imagen preprocesada junto con su etiqueta correspondiente en una lista.


### 7. **Separación de datos en entradas (X) y etiquetas (y)**

```python
# Preparar variables X (entradas) y y (etiquetas) separadas
X = []  # Lista para almacenar las imágenes de entrada (píxeles)
y = []  # Lista para almacenar las etiquetas (perro o gato)

# Separar las imágenes y etiquetas del conjunto de datos de entrenamiento
for imagen, etiqueta in datos_entrenamiento:
    X.append(imagen)
    y.append(etiqueta)
```

Esta celda separa las imágenes y las etiquetas en dos listas diferentes: `X` para las imágenes y `y` para las etiquetas.

### 8. **Normalización de datos**

```python
# Importar numpy para manipulación de arrays
import numpy as np

# Normalizar los datos de las imágenes
# Convertir las listas a arrays de NumPy, convertir a flotantes y dividir por 255 para normalizar al rango 0-1
X = np.array(X).astype(float) / 255
```

Esta celda normaliza los datos de las imágenes convirtiéndolas a valores flotantes entre 0 y 1.

### 9. **Conversión de etiquetas a array**

```python
# Convertir etiquetas a un array de NumPy
y = np.array(y)
```

Esta celda convierte la lista de etiquetas en un array de NumPy.

### 10. **Creación de modelos**

```python
# Crear modelos iniciales

# Modelo denso completamente conectado
modeloDenso = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(100, 100, 1)),  # Aplanar la imagen de entrada
    tf.keras.layers.Dense(150, activation='relu'),  # Capa densa con 150 neuronas y ReLU
    tf.keras.layers.Dense(150, activation='relu'),  # Capa densa con 150 neuronas y ReLU
    tf.keras.layers.Dense(1, activation='sigmoid')  # Capa de salida con sigmoid para clasificación binaria
])

# Modelo de red neuronal convolucional (CNN)
modeloCNN = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 1)),  # Capa convolucional
    tf.keras.layers.MaxPooling2D(2, 2),  # Capa de max pooling
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),  # Segunda capa convolucional
    tf.keras.layers.MaxPooling2D(2, 2),  # Segunda capa de max pooling
    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),  # Tercera capa convolucional
    tf.keras.layers.MaxPooling2D(2, 2),  # Tercera capa de max pooling
    tf.keras.layers.Flatten(),  # Aplanar antes de las capas densas
    tf.keras.layers.Dense(100, activation='relu'),  # Capa densa con 100 neuronas y ReLU
    tf.keras.layers.Dense(1, activation='sigmoid')  # Capa de salida con sigmoid para clasificación binaria
])

# Modelo CNN con dropout para regularización
modeloCNN2 = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 1)),  # Capa convolucional
    tf.keras.layers.MaxPooling2D(2, 2),  # Capa de max pooling
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),  # Segunda capa convolucional
    tf.keras.layers.MaxPooling2D(2, 2),  # Segunda capa de max pooling
    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),  # Tercera capa convolucional
    tf.keras.layers.MaxPooling2D(2, 2),  # Tercera capa de max pooling
    tf.keras.layers.Dropout(0.5),  # Capa de dropout para regularización
    tf.keras.layers.Flatten(),  # Aplanar antes de las capas densas
    tf.keras.layers.Dense(250, activation='relu'),  # Capa densa con 250 neuronas y ReLU
    tf.keras.layers.Dense(1, activation='sigmoid')  # Capa de salida con sigmoid para clasificación binaria
])
```

Esta celda crea tres modelos diferentes: un modelo denso (completamente conectado) y dos modelos de red neuronal convolucional (CNN) con diferentes arquitecturas.

### 11. **Compilación de modelos**

```python
# Compilar modelos usando binary_crossentropy para la clasificación binaria
# Usar el optimizador 'adam' y métricas de 'accuracy' para evaluar el rendimiento

modeloDenso.compile(optimizer='adam',
                    loss='binary_crossentropy',
                    metrics=['accuracy'])

modeloCNN.compile(optimizer='adam',
                  loss='binary_crossentropy',
                  metrics=['accuracy'])

modeloCNN2.compile(optimizer='adam',
                   loss='binary_crossentropy',
                   metrics=['accuracy'])
```

Esta celda compila los tres modelos, especificando el optimizador `adam`, la función de pérdida `binary_crossentropy`, y las métricas de precisión (`accuracy`).

### 12. **Entrenamiento del modelo denso con TensorBoard**

```python
# Importar TensorBoard para visualización de los resultados del entrenamiento
from tensorflow.keras.callbacks import TensorBoard

# Configurar TensorBoard para el modelo denso
tensorboardDenso = TensorBoard(log_dir='logs/denso')

# Entrenar el modelo denso
modeloDenso.fit(X, y, batch_size=32,  # Tamaño del lote
                validation_split=0.15,  # División del conjunto de datos para validación
                epochs=100,  # Número de épocas de entrenamiento
                callbacks=[tensorboardDenso])  # Registrar el progreso con TensorBoard
```

Esta celda entrena el modelo denso usando TensorBoard para registrar y visualizar el progreso del entrenamiento.

Continuando con la explicación mejorada y comentarios detallados:

### 13. **Carga de la extensión TensorBoard**

```python
# Cargar la extensión de TensorBoard de Colab para visualizar los resultados del entrenamiento
%load_ext tensorboard
```

Esta celda carga la extensión de TensorBoard en Colab, lo que permite visualizar los registros de entrenamiento directamente en el entorno de Colab.

### 14. **Ejecución de TensorBoard**

```python
# Ejecutar TensorBoard e indicarle que lea la carpeta "logs"
%tensorboard --logdir logs
```

Esta celda inicia TensorBoard y le indica que lea los registros de la carpeta "logs", lo que permite monitorear el progreso del entrenamiento en tiempo real.

### 15. **Entrenamiento del modelo CNN con TensorBoard**

```python
# Configurar TensorBoard para el modelo CNN
tensorboardCNN = TensorBoard(log_dir='logs/cnn')

# Entrenar el modelo CNN
modeloCNN.fit(X, y, batch_size=32,  # Tamaño del lote
              validation_split=0.15,  # División del conjunto de datos para validación
              epochs=100,  # Número de épocas de entrenamiento
              callbacks=[tensorboardCNN])  # Registrar el progreso con TensorBoard
```

Esta celda entrena el primer modelo CNN y utiliza TensorBoard para registrar el progreso del entrenamiento.

### 16. **Entrenamiento del modelo CNN2 con TensorBoard**

```python
# Configurar TensorBoard para el modelo CNN2
tensorboardCNN2 = TensorBoard(log_dir='logs/cnn2')

# Entrenar el modelo CNN2
modeloCNN2.fit(X, y, batch_size=32,  # Tamaño del lote
               validation_split=0.15,  # División del conjunto de datos para validación
               epochs=100,  # Número de épocas de entrenamiento
               callbacks=[tensorboardCNN2])  # Registrar el progreso con TensorBoard
```

Esta celda entrena el segundo modelo CNN y utiliza TensorBoard para registrar el progreso del entrenamiento.

### 17. **Visualización de imágenes sin aumento de datos**

```python
# Ver las imágenes de la variable X sin modificaciones por aumento de datos
plt.figure(figsize=(20, 8))

# Visualizar las primeras 10 imágenes del conjunto de datos sin modificaciones
for i in range(10):
    plt.subplot(2, 5, i + 1)
    plt.xticks([])  # Eliminar marcas del eje x
    plt.yticks([])  # Eliminar marcas del eje y
    plt.imshow(X[i].reshape(100, 100), cmap="gray")  # Mostrar la imagen en escala de grises
plt.show()
```

Esta celda visualiza 10 imágenes del conjunto de datos sin aplicar aumento de datos, mostrando las imágenes originales.

### 18. **Aumento de datos y visualización**

```python
# Importar ImageDataGenerator para realizar el aumento de datos
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Configurar el generador de datos con varias transformaciones
datagen = ImageDataGenerator(
    rotation_range=30,  # Rotar imágenes hasta 30 grados
    width_shift_range=0.2,  # Desplazar imágenes horizontalmente hasta un 20%
    height_shift_range=0.2,  # Desplazar imágenes verticalmente hasta un 20%
    shear_range=15,  # Aplicar cizalladura a las imágenes hasta 15 grados
    zoom_range=[0.7, 1.4],  # Aplicar zoom a las imágenes entre 0.7x y 1.4x
    horizontal_flip=True,  # Permitir voltear horizontalmente las imágenes
    vertical_flip=True  # Permitir voltear verticalmente las imágenes
)

# Ajustar el generador a las imágenes
datagen.fit(X)

# Visualizar ejemplos de imágenes aumentadas
plt.figure(figsize=(20, 8))

# Generar y mostrar 10 imágenes aumentadas
for imagen, etiqueta in datagen.flow(X, y, batch_size=10, shuffle=False):
    for i in range(10):
        plt.subplot(2, 5, i + 1)
        plt.xticks([])  # Eliminar marcas del eje x
        plt.yticks([])  # Eliminar marcas del eje y
        plt.imshow(imagen[i].reshape(100, 100), cmap="gray")  # Mostrar la imagen en escala de grises
    break  # Salir del bucle después de visualizar 10 imágenes
plt.show()
```

Esta celda realiza el aumento de datos aplicando varias transformaciones a las imágenes y luego visualiza 10 ejemplos de imágenes aumentadas.

### 19. **Creación de modelos con aumento de datos**

```python
# Crear nuevos modelos para entrenar con aumento de datos

# Modelo denso con aumento de datos
modeloDenso_AD = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(100, 100, 1)),  # Aplanar la imagen de entrada
    tf.keras.layers.Dense(150, activation='relu'),  # Capa densa con 150 neuronas y ReLU
    tf.keras.layers.Dense(150, activation='relu'),  # Capa densa con 150 neuronas y ReLU
    tf.keras.layers.Dense(1, activation='sigmoid')  # Capa de salida con sigmoid para clasificación binaria
])

# Modelo CNN con aumento de datos
modeloCNN_AD = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 1)),  # Capa convolucional
    tf.keras.layers.MaxPooling2D(2, 2),  # Capa de max pooling
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),  # Segunda capa convolucional
    tf.keras.layers.MaxPooling2D(2, 2),  # Segunda capa de max pooling
    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),  # Tercera capa convolucional
    tf.keras.layers.MaxPooling2D(2, 2),  # Tercera capa de max pooling
    tf.keras.layers.Flatten(),  # Aplanar antes de las capas densas
    tf.keras.layers.Dense(100, activation='relu'),  # Capa densa con 100 neuronas y ReLU
    tf.keras.layers.Dense(1, activation='sigmoid')  # Capa de salida con sigmoid para clasificación binaria
])

# Modelo CNN con dropout y aumento de datos
modeloCNN2_AD = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 1)),  # Capa convolucional
    tf.keras.layers.MaxPooling2D(2, 2),  # Capa de max pooling
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),  # Segunda capa convolucional
    tf.keras.layers.MaxPooling2D(2, 2),  # Segunda capa de max pooling
    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),  # Tercera capa convolucional
    tf.keras.layers.MaxPooling2D(2, 2),  # Tercera capa de max pooling
    tf.keras.layers.Dropout(0.5),  # Capa de dropout para regularización
    tf.keras.layers.Flatten(),  # Aplanar antes de las capas densas
    tf.keras.layers.Dense(250, activation='relu'),  # Capa densa con 250 neuronas y ReLU
    tf.keras.layers.Dense(1, activation='sigmoid')  # Capa de salida con sigmoid para clasificación binaria
])
```

Esta celda crea nuevos modelos con las mismas estructuras que los anteriores, pero se utilizarán para entrenar con datos aumentados.

Continuando con la explicación detallada y comentarios del código:

### 20. **Compilación de modelos con aumento de datos**

```python
# Compilar los nuevos modelos con datos aumentados
modeloDenso_AD.compile(optimizer='adam',
                       loss='binary_crossentropy',
                       metrics=['accuracy'])

modeloCNN_AD.compile(optimizer='adam',
                     loss='binary_crossentropy',
                     metrics=['accuracy'])

modeloCNN2_AD.compile(optimizer='adam',
                      loss='binary_crossentropy',
                      metrics=['accuracy'])
```

Esta celda compila los modelos `modeloDenso_AD`, `modeloCNN_AD` y `modeloCNN2_AD` para ser entrenados con datos aumentados. Se utiliza el optimizador Adam, la función de pérdida `binary_crossentropy` adecuada para problemas de clasificación binaria, y se evalúa la métrica de precisión (`accuracy`).

### 21. **Separación de datos de entrenamiento y validación**

```python
# Separar los datos en conjuntos de entrenamiento y validación
split_index = int(len(X) * 0.85)

X_entrenamiento = X[:split_index]
X_validacion = X[split_index:]

y_entrenamiento = y[:split_index]
y_validacion = y[split_index:]
```

Esta celda divide los datos en conjuntos de entrenamiento (85%) y validación (15%). `X_entrenamiento` y `y_entrenamiento` contienen los datos para entrenar los modelos, mientras que `X_validacion` y `y_validacion` se utilizan para validar el rendimiento de los modelos durante el entrenamiento.

### 22. **Creación del generador de datos de entrenamiento**

```python
# Crear un generador de datos para aplicar aumento de datos en tiempo real durante el entrenamiento
data_gen_entrenamiento = datagen.flow(X_entrenamiento, y_entrenamiento, batch_size=32)
```

Esta celda crea un generador de datos utilizando `datagen.flow`, que aplica aumentos de datos en tiempo real durante el entrenamiento. `batch_size=32` especifica el tamaño del lote utilizado para el entrenamiento.

### 23. **Entrenamiento del modelo denso con aumento de datos**

```python
tensorboardDenso_AD = TensorBoard(log_dir='logs/denso_AD')

modeloDenso_AD.fit(
    data_gen_entrenamiento,
    epochs=100,
    batch_size=32,
    validation_data=(X_validacion, y_validacion),
    steps_per_epoch=int(np.ceil(len(X_entrenamiento) / float(32))),
    validation_steps=int(np.ceil(len(X_validacion) / float(32))),
    callbacks=[tensorboardDenso_AD]
)
```

Esta celda entrena el modelo denso con datos aumentados. Se utiliza `data_gen_entrenamiento` como fuente de datos de entrenamiento, se especifica la validación usando `X_validacion` y `y_validacion`, y se registran métricas y registros de entrenamiento en TensorBoard con `TensorBoard`.

### 24. **Entrenamiento del modelo CNN con aumento de datos**

```python
tensorboardCNN_AD = TensorBoard(log_dir='logs-new/cnn_AD')

modeloCNN_AD.fit(
    data_gen_entrenamiento,
    epochs=150,
    batch_size=32,
    validation_data=(X_validacion, y_validacion),
    steps_per_epoch=int(np.ceil(len(X_entrenamiento) / float(32))),
    validation_steps=int(np.ceil(len(X_validacion) / float(32))),
    callbacks=[tensorboardCNN_AD]
)
```

Esta celda entrena el modelo CNN inicial con datos aumentados. Al igual que el modelo denso, se utiliza el generador de datos `data_gen_entrenamiento` para aplicar aumentos de datos en tiempo real durante el entrenamiento, se especifican las épocas (`epochs`) y el tamaño del lote (`batch_size`), y se registran métricas y registros de entrenamiento en TensorBoard.

### 25. **Entrenamiento del modelo CNN2 con aumento de datos**

```python
tensorboardCNN2_AD = TensorBoard(log_dir='logs/cnn2_AD')

modeloCNN2_AD.fit(
    data_gen_entrenamiento,
    epochs=100,
    batch_size=32,
    validation_data=(X_validacion, y_validacion),
    steps_per_epoch=int(np.ceil(len(X_entrenamiento) / float(32))),
    validation_steps=int(np.ceil(len(X_validacion) / float(32))),
    callbacks=[tensorboardCNN2_AD]
)
```

Esta celda entrena el segundo modelo CNN con datos aumentados. Se utiliza el mismo enfoque que los modelos anteriores para aplicar aumentos de datos y registrar métricas en TensorBoard.


### Demo y video en youtube por el canal de youtube Ringa Tech

https://ringa-tech.com/exportacion/perros-gatos/index.html

https://youtu.be/DbwKbsCWPSg?si=_FiIy7Lt7w-yIS3R

### Recursos para Explorar Más:
- **[Redes neuronales convolucionales
](https://www.youtube.com/live/2cz1hEb52n4?si=Z6UTm834iX2htzHI)** Taller completo de 3 horas con proyecto de Redes neuronales convolucionales.

## Colab Notebooks


- [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Efva3sau54WHusFRnfcFxL-9sLuO27L0) [Día 26: Clasificador de perros y gatos](https://colab.research.google.com/drive/1Efva3sau54WHusFRnfcFxL-9sLuO27L0)

---

# Día27
---
## Explorando arquitecturas influyentes en el aprendizaje profundo 🧠🔍

¡Hola a todos! En el día 27 de nuestro desafío #100DaysOfAI, vamos a explorar algunas de las arquitecturas más influyentes y populares en el Deep Learning. Estas arquitecturas han definido el camino del aprendizaje profundo en la última década, con aplicaciones que van desde la clasificación de imágenes hasta la detección de objetos en tiempo real. ¡Vamos a descubrirlas!

| **Arquitectura** | **Año** | **Características Principales** | **Ventajas** | **Desventajas** | **Paper** |
|------------------|---------|-----------------------------|--------------|-----------------|-----------|
| **LeNet** | 1998 | Capas convolucionales y submuestreo | Pionera en el uso de CNNs para la clasificación de dígitos manuscritos | Muy simple y no adecuada para tareas modernas complejas | [LeNet Paper](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) |
| **AlexNet** | 2012 | 5 capas convolucionales, 3 fully connected | Pionera en CNNs profundas, ganó ImageNet 2012 | Relativamente simple comparada con modelos modernos | [AlexNet Paper](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) |
| **VGGNet** | 2014 | Capas 3x3 apiladas, profundidad aumentada | Simplicidad, buena transferencia de aprendizaje | Muchos parámetros, computacionalmente costosa | [VGGNet Paper](https://arxiv.org/pdf/1409.1556.pdf) |
| **Inception (GoogLeNet)** | 2014 | Módulos Inception, 1x1 convolutions | Eficiente en parámetros, buena escala | Compleja de implementar | [Inception Paper](https://arxiv.org/pdf/1409.4842.pdf) |
| **R-CNN (y variantes)** | 2014-2015 | Regiones de interés, fine-tuning | Precisión en detección de objetos | Lenta (original), versiones posteriores más rápidas | [R-CNN Paper](https://arxiv.org/pdf/1311.2524.pdf) |
| **Faster R-CNN** | 2015 | Regiones de interés generadas por una red, detección rápida | Mejor equilibrio entre velocidad y precisión | Más compleja de implementar y entrenar | [Faster R-CNN Paper](https://arxiv.org/pdf/1506.01497.pdf) |
| **ResNet** | 2015 | Conexiones residuales (skip connections) | Muy profunda (hasta 152 capas), resuelve desvanecimiento del gradiente | Puede ser overkill para tareas simples | [ResNet Paper](https://arxiv.org/pdf/1512.03385.pdf) |
| **U-Net** | 2015 | Arquitectura en forma de U, skip connections | Excelente para segmentación de imágenes médicas | Puede ser excesiva para tareas de clasificación simples | [U-Net Paper](https://arxiv.org/pdf/1505.04597.pdf) |
| **SqueezeNet** | 2016 | Módulos Fire, convoluciones 1x1 | Muy compacta, pocos parámetros | Precisión algo menor que modelos más grandes | [SqueezeNet Paper](https://arxiv.org/pdf/1602.07360.pdf) |
| **YOLO** | 2016 | Detección en tiempo real, una sola red convolucional | Rápida y precisa en la detección de objetos | Menor precisión en comparación con métodos más lentos | [YOLO Paper](https://arxiv.org/pdf/1506.02640.pdf) |
| **DenseNet** | 2017 | Conexiones densas entre capas | Uso eficiente de parámetros, fuerte propagación de características | Alto consumo de memoria | [DenseNet Paper](https://arxiv.org/pdf/1608.06993.pdf) |
| **MobileNet** | 2017 | Convoluciones separables en profundidad | Eficiente para dispositivos móviles | Precisión ligeramente menor que modelos más grandes | [MobileNet Paper](https://arxiv.org/pdf/1704.04861.pdf) |
| **Xception** | 2017 | Convoluciones separables en profundidad extremas | Eficiente en parámetros, buena precisión | Puede ser compleja de implementar | [Xception Paper](https://arxiv.org/pdf/1610.02357.pdf) |
| **ShuffleNet** | 2017 | Group convolutions, channel shuffle | Muy eficiente para dispositivos móviles | Posible pérdida de precisión en tareas complejas | [ShuffleNet Paper](https://arxiv.org/pdf/1707.01083.pdf) |
| **NASNet** | 2018 | Arquitectura encontrada por búsqueda neural | Altamente optimizada | Compleja, costosa de entrenar | [NASNet Paper](https://arxiv.org/pdf/1707.07012.pdf) |
| **SENet** | 2017 | Módulos Squeeze-and-Excitation | Mejora la calidad de las representaciones | Ligero aumento en costo computacional | [SENet Paper](https://arxiv.org/pdf/1709.01507.pdf) |
| **FPN** | 2017 | Pirámide de características multi-escala | Excelente para detección de objetos | Puede ser excesiva para tareas de clasificación simples | [FPN Paper](https://arxiv.org/pdf/1612.03144.pdf) |
| **EfficientNet** | 2019 | Escalado compuesto de profundidad/anchura/resolución | Muy eficiente, estado del arte en precisión/eficiencia | Compleja de implementar y ajustar | [EfficientNet Paper](https://arxiv.org/pdf/1905.11946.pdf) |
| **Vision Transformers (ViT)** | 2020 | Uso de transformadores en tareas de visión por computadora | Alta precisión en tareas de clasificación de imágenes | Requiere una gran cantidad de datos para entrenar eficazmente | [ViT Paper](https://arxiv.org/pdf/2010.11929.pdf) |

Estas arquitecturas han desempeñado un papel fundamental en la evolución de la visión por computadora y el Deep Learning. Cada una tiene sus propias ventajas y desventajas, pero todas han contribuido de manera significativa al avance de la tecnología.

---

# Día28
---


## Arquitecturas Específicas en Visión por Computadora 🎯🖥️

Continuando con nuestro viaje por las arquitecturas de redes neuronales, hoy exploramos cómo diferentes arquitecturas destacan en tareas específicas dentro de la visión por computadora:

1. **Clasificación a gran escala: EfficientNet 🏆**
   - **Equilibrio óptimo:** Combina profundidad, anchura y resolución de manera eficiente.
   - **Precisión alta con menos parámetros:** Logra resultados superiores con una menor cantidad de parámetros.

2. **Detección en tiempo real: YOLO 🏃‍♂️**
   - **Enfoque de una sola pasada:** Permite una detección rápida y eficiente.
   - **Ideal para aplicaciones como conducción autónoma:** Su velocidad lo hace perfecto para escenarios que requieren respuestas inmediatas.

3. **Segmentación médica: U-Net 🏥**
   - **Arquitectura en U con conexiones de salto (skip connections):** Mejora la precisión en la segmentación.
   - **Excelente con datos limitados en imágenes biomédicas:** Ideal para aplicaciones médicas donde los datos son escasos.

4. **Dispositivos móviles: MobileNet 📱**
   - **Convoluciones separables en profundidad:** Reduce la carga computacional manteniendo un buen rendimiento.
   - **Eficiente en recursos limitados:** Diseñado para funcionar bien en dispositivos con capacidades limitadas.

5. **Visión de alto nivel: Vision Transformers (ViT) 👁️**
   - **Adaptación de transformadores a visión:** Utiliza la atención a escala completa para procesar imágenes.
   - **Rendimiento superior con grandes conjuntos de datos:** Necesita grandes volúmenes de datos para entrenarse adecuadamente.

6. **Transferencia de aprendizaje: ResNet 🔄**
   - **Conexiones residuales:** Facilitan el entrenamiento de redes muy profundas.
   - **Excelente extractor de características generales:** Muy útil en diversas tareas de visión por computadora.

Cada arquitectura brilla en su dominio, demostrando la diversidad y especialización en el campo de la visión por computadora. La elección correcta puede marcar la diferencia en el éxito de un proyecto de IA. 🌟

### Recursos Adicionales

- **EfficientNet:** [Estudio comparativo en ImageNet](https://arxiv.org/abs/1905.11946)
- **YOLO:** [Caso de éxito en conducción autónoma](https://pjreddie.com/darknet/yolo/)
- **U-Net:** [Aplicación en imágenes biomédicas](https://arxiv.org/abs/1505.04597)
- **MobileNet:** [Evaluación en dispositivos móviles](https://arxiv.org/abs/1704.04861)
- **Vision Transformers (ViT):** [Adaptación de transformadores a visión](https://arxiv.org/abs/2010.11929)
- **ResNet:** [Desempeño en diversas tareas](https://arxiv.org/abs/1512.03385)

---

# Día29
---
## Concepto de Transfer Learning 🚀🧠

¡Hola a todos! En el día 29 de nuestro desafío #100DaysOfAI, vamos a explorar el fascinante concepto de **Transfer Learning**. Esta técnica ha revolucionado la forma en que abordamos problemas de aprendizaje profundo, especialmente cuando tenemos datos limitados. ¡Vamos a sumergirnos en los detalles!


#### ¿Qué es el Transfer Learning?

El **Transfer Learning** es una técnica en la que un modelo preentrenado en una tarea (generalmente en un conjunto de datos grande y genérico) se reutiliza y ajusta para una tarea diferente, generalmente con un conjunto de datos más pequeño y específico. En lugar de entrenar un modelo desde cero, lo que puede ser costoso en términos de tiempo y recursos computacionales, utilizamos el conocimiento ya adquirido por el modelo preentrenado.


#### Ventajas del Transfer Learning

1. **Ahorro de Tiempo y Recursos**: Dado que el modelo ya ha aprendido características básicas de datos similares, el tiempo de entrenamiento se reduce significativamente.
2. **Mejor Rendimiento**: Los modelos preentrenados suelen proporcionar una mejor precisión en tareas específicas, especialmente cuando los datos disponibles son limitados.
3. **Facilidad de Implementación**: Muchas bibliotecas de Deep Learning, como TensorFlow y PyTorch, proporcionan modelos preentrenados que se pueden utilizar fácilmente.


#### ¿Cómo Funciona el Transfer Learning?

El Transfer Learning generalmente implica los siguientes pasos:

1. **Seleccionar un Modelo Preentrenado**: Elegimos un modelo que ha sido entrenado en una tarea similar, como la clasificación de imágenes en el conjunto de datos ImageNet.
2. **Ajuste del Modelo (Fine-Tuning)**: Modificamos las últimas capas del modelo para que se adapten a nuestra tarea específica. Por ejemplo, en lugar de clasificar 1000 categorías de ImageNet, podríamos clasificar solo 10 categorías específicas de nuestro problema.
3. **Entrenamiento en Datos Específicos**: Entrenamos el modelo ajustado en nuestro conjunto de datos específico. Este entrenamiento suele ser más rápido y requiere menos datos que entrenar un modelo desde cero.


#### Aplicaciones del Transfer Learning

El Transfer Learning se ha utilizado con éxito en diversas áreas, como:

- **Clasificación de Imágenes**: Uso de modelos preentrenados como ResNet, Inception o VGG para tareas de clasificación de imágenes específicas.
- **Detección de Objetos**: Modelos como YOLO o Faster R-CNN se ajustan para detectar objetos en nuevos conjuntos de datos.
- **Procesamiento del Lenguaje Natural (NLP)**: Modelos como BERT, GPT-3 y otros se utilizan para tareas de clasificación de texto, análisis de sentimientos y más.
- **Reconocimiento de Voz**: Uso de modelos preentrenados para transcribir y comprender el habla en diferentes idiomas y acentos.


#### Ejemplo Práctico en Python (con TensorFlow/Keras)

```python
import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.models import Model

# Cargar el modelo VGG16 preentrenado sin la última capa
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Congelar las capas del modelo base
for layer in base_model.layers:
    layer.trainable = False

# Añadir nuevas capas personalizadas
x = base_model.output
x = Flatten()(x)
x = Dense(1024, activation='relu')(x)
predictions = Dense(10, activation='softmax')(x)  # Asumiendo 10 clases en el nuevo conjunto de datos

# Crear el modelo final
model = Model(inputs=base_model.input, outputs=predictions)

# Compilar el modelo
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Preparar los datos
train_datagen = ImageDataGenerator(rescale=1.0/255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)
train_generator = train_datagen.flow_from_directory('path/to/train_data', target_size=(224, 224), batch_size=32, class_mode='categorical')

# Entrenar el modelo
model.fit(train_generator, epochs=10, steps_per_epoch=100)
```

---

El Transfer Learning es una herramienta poderosa en el arsenal del Deep Learning, permitiendo aprovechar modelos robustos y aplicarlos a nuevas tareas con eficiencia y precisión.

# Día30
---
###  Técnicas de Transfer Learning 📚🚀

¡Hola a todos! En el día 30 de nuestro desafío #100DaysOfAI, vamos a profundizar en las **técnicas de Transfer Learning** y cómo utilizar modelos preentrenados para abordar nuevas tareas. Esta metodología permite ahorrar tiempo y mejorar el rendimiento en tareas específicas. ¡Vamos a explorar cómo hacerlo!


#### Técnicas de Transfer Learning

1. **Feature Extraction (Extracción de Características)**

   En esta técnica, utilizamos un modelo preentrenado como extractor de características. Las capas convolucionales de un modelo, por ejemplo, ResNet o VGG, actúan como un filtro que extrae características relevantes de las imágenes. Luego, agregamos y entrenamos capas adicionales para la tarea específica que queremos abordar.

   **Pasos:**
   - Cargar un modelo preentrenado sin la última capa de clasificación.
   - Congelar las capas del modelo base.
   - Añadir nuevas capas personalizadas para la tarea específica.
   - Entrenar solo las nuevas capas.

   ```python
   from tensorflow.keras.applications import VGG16
   from tensorflow.keras.models import Model
   from tensorflow.keras.layers import Dense, Flatten

   # Cargar el modelo VGG16 preentrenado sin la última capa
   base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

   # Congelar las capas del modelo base
   for layer in base_model.layers:
       layer.trainable = False

   # Añadir nuevas capas personalizadas
   x = base_model.output
   x = Flatten()(x)
   x = Dense(1024, activation='relu')(x)
   predictions = Dense(10, activation='softmax')(x)  # Asumiendo 10 clases en el nuevo conjunto de datos

   # Crear el modelo final
   model = Model(inputs=base_model.input, outputs=predictions)

   # Compilar el modelo
   model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
   ```

2. **Fine-Tuning (Ajuste Fino)**

   El ajuste fino implica descongelar algunas de las capas superiores del modelo base y entrenarlas junto con las nuevas capas añadidas. Esto permite que el modelo ajuste las características preentrenadas a la tarea específica de manera más precisa.

   **Pasos:**
   - Cargar un modelo preentrenado sin la última capa de clasificación.
   - Congelar la mayoría de las capas del modelo base, pero dejar algunas capas superiores entrenables.
   - Añadir nuevas capas personalizadas para la tarea específica.
   - Entrenar tanto las nuevas capas como las capas superiores descongeladas del modelo base.

   ```python
   # Descongelar algunas capas del modelo base para el fine-tuning
   for layer in base_model.layers[-4:]:
       layer.trainable = True

   # Recompilar el modelo
   model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

   # Entrenar el modelo
   model.fit(train_generator, epochs=10, steps_per_epoch=100)
   ```

   Consejos adicionales para Fine-Tuning:
   - Utiliza una tasa de aprendizaje más baja para evitar destruir el conocimiento preentrenado.
   - Considera el uso de "discriminative fine-tuning", donde diferentes capas tienen diferentes tasas de aprendizaje.
   - Monitorea el rendimiento en un conjunto de validación para evitar el sobreajuste.

3. **Gradual Unfreezing**
   Esta técnica es una extensión del fine-tuning donde descongelamos gradualmente más capas del modelo base a medida que avanza el entrenamiento.

   **Pasos:**
   - Comenzar con todas las capas del modelo base congeladas, excepto la última.
   - Entrenar por algunas épocas.
   - Descongelar la siguiente capa y continuar el entrenamiento.
   - Repetir hasta alcanzar el rendimiento deseado o hasta descongelar todas las capas.

```python
def unfreeze_model(model):
    for layer in model.layers:
        layer.trainable = True
    return model

epochs_per_stage = 5
total_stages = len(base_model.layers) // 3

for i in range(total_stages):
    if i > 0:
        base_model.layers[-3*i:] = unfreeze_model(base_model.layers[-3*i:])
    
    model.compile(optimizer=tf.keras.optimizers.Adam(1e-5*(0.9**i)),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    
    model.fit(train_generator, epochs=epochs_per_stage, validation_data=val_generator)
```

4. **Domain Adaptation**
   Esta técnica se utiliza cuando el dominio de los datos de entrenamiento (fuente) es diferente al dominio de los datos de prueba (objetivo).

   **Idea principal:**
   - Entrenar un modelo que pueda extraer características que sean invariantes entre los dominios fuente y objetivo.
   - Utilizar técnicas como Adversarial Domain Adaptation para alinear las distribuciones de características.

```python
from tensorflow.keras.layers import Input, Dense, Lambda
from tensorflow.keras.models import Model
import tensorflow.keras.backend as K

def build_domain_adaptation_model(base_model, num_classes):
    input = Input(shape=(224, 224, 3))
    features = base_model(input)
    class_output = Dense(num_classes, activation='softmax', name='class_output')(features)
    
    # Domain classifier
    domain_output = Dense(1, activation='sigmoid', name='domain_output')(Lambda(lambda x: K.reverse(x, axes=1))(features))
    
    model = Model(inputs=input, outputs=[class_output, domain_output])
    return model

domain_model = build_domain_adaptation_model(base_model, num_classes=10)
domain_model.compile(optimizer='adam',
                     loss={'class_output': 'categorical_crossentropy',
                           'domain_output': 'binary_crossentropy'},
                     loss_weights={'class_output': 1., 'domain_output': 0.1},
                     metrics={'class_output': 'accuracy', 'domain_output': 'accuracy'})
```

5. **Few-shot Learning**
   Esta técnica se utiliza cuando solo tenemos unos pocos ejemplos de las nuevas clases que queremos clasificar.

   **Enfoques comunes:**
   - Prototypical Networks: Aprenden un espacio de embedding donde los puntos de la misma clase se agrupan alrededor de un "prototipo".
   - Matching Networks: Utilizan atención para comparar nuevas muestras con un conjunto de soporte etiquetado.

La elección de la técnica de Transfer Learning dependerá de la naturaleza de tu tarea, la cantidad de datos disponibles y la similitud entre el dominio fuente y el objetivo. Experimenta con diferentes enfoques para encontrar el que mejor se adapte a tu problema específico.




### Recursos Adicionales

1. **[Transfer Learning Guide by TensorFlow](https://www.tensorflow.org/tutorials/images/transfer_learning)**
2. **[PyTorch Transfer Learning Tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)**

---
# Día31
---
## Detección de Objetos 🕵️‍♂️🔍

#### ¿Qué es la Detección de Objetos?

La detección de objetos es una técnica que permite a los modelos de visión por computadora identificar y localizar múltiples objetos dentro de una imagen. A diferencia de la clasificación de imágenes, donde el objetivo es identificar la clase principal de una imagen, la detección de objetos busca encontrar todas las instancias de objetos de interés y sus ubicaciones específicas.


#### Conceptos Básicos de la Detección de Objetos

1. **Bounding Box (Caja Delimitadora)**

   La detección de objetos generalmente implica la predicción de una caja delimitadora para cada objeto en la imagen. Una caja delimitadora está definida por sus coordenadas (x, y) del vértice superior izquierdo, así como su ancho y alto.

   

2. **Clasificación de Objetos**

   Además de localizar un objeto, el modelo también necesita clasificar qué tipo de objeto está presente dentro de cada caja delimitadora.

3. **Intersección sobre Unión (IoU)**

   IoU es una métrica utilizada para evaluar la precisión de la predicción de la caja delimitadora. Se calcula como el área de superposición entre la caja predicha y la caja real dividida por el área de unión de ambas cajas.

  

4. **Modelos Comunes de Detección de Objetos**

  - **R-CNN (Region-Based Convolutional Neural Networks)**: Propone regiones de interés y aplica CNNs a cada región.
   - **Fast R-CNN**: Optimiza R-CNN utilizando la detección de regiones propuestas y CNNs en una sola pasada.
   - **Faster R-CNN**: Introduce una red separada para proponer regiones de interés, lo que mejora la velocidad.
   - **YOLO (You Only Look Once)**: Predice las cajas delimitadoras y las clases de objetos en una sola pasada de la red, lo que lo hace muy rápido.
   - **SSD (Single Shot Multibox Detector)**: Similar a YOLO, realiza detección en una sola pasada, pero con múltiples cajas de diferentes tamaños.

---

### Ejemplo Práctico: Implementando YOLO para Detección de Objetos

A continuación, se muestra un ejemplo de cómo implementar el modelo YOLO utilizando la librería `opencv` y un modelo preentrenado.

**Paso 1: Instalación de Dependencias**
```python
!pip install opencv-python-headless
!pip install numpy
!pip install matplotlib

!wget https://pjreddie.com/media/files/yolov3.weights
!wget https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg
!wget https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names

```

**Paso 2: Cargar el Modelo YOLO Preentrenado y Realizar la Detección**
```python

# Paso 3: Importar las bibliotecas necesarias
import cv2
import numpy as np
import matplotlib.pyplot as plt
from google.colab.patches import cv2_imshow
import urllib.request

# Paso 4: Cargar el modelo YOLO preentrenado y los archivos de configuración
net = cv2.dnn.readNet("yolov3.weights", "yolov3.cfg")
with open("coco.names", "r") as f:
    classes = [line.strip() for line in f.readlines()]
layer_names = net.getLayerNames()
output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]

# Función para descargar una imagen de ejemplo
def download_image(url, filename):
    urllib.request.urlretrieve(url, filename)

# Descargar una imagen de ejemplo
image_url = "https://raw.githubusercontent.com/pjreddie/darknet/master/data/dog.jpg"
image_filename = "example_image.jpg"
download_image(image_url, image_filename)

# Paso 5: Cargar y preprocesar la imagen
image = cv2.imread(image_filename)
height, width, channels = image.shape
blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)

# Paso 6: Realizar la detección de objetos
net.setInput(blob)
outs = net.forward(output_layers)

# Paso 7: Procesar los resultados
class_ids = []
confidences = []
boxes = []
for out in outs:
    for detection in out:
        scores = detection[5:]
        class_id = np.argmax(scores)
        confidence = scores[class_id]
        if confidence > 0.5:
            # Obtener las coordenadas de la caja delimitadora
            center_x = int(detection[0] * width)
            center_y = int(detection[1] * height)
            w = int(detection[2] * width)
            h = int(detection[3] * height)
            # Coordenadas de la caja delimitadora
            x = int(center_x - w / 2)
            y = int(center_y - h / 2)
            boxes.append([x, y, w, h])
            confidences.append(float(confidence))
            class_ids.append(class_id)

# Paso 8: Aplicar Non-Maximum Suppression (NMS)
indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)

# Paso 9: Dibujar las cajas delimitadoras y etiquetas
colors = np.random.uniform(0, 255, size=(len(classes), 3))
for i in range(len(boxes)):
    if i in indexes:
        x, y, w, h = boxes[i]
        label = str(classes[class_ids[i]])
        color = colors[class_ids[i]]
        cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)
        cv2.putText(image, f"{label} {confidences[i]:.2f}", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

# Paso 10: Mostrar la imagen resultante
plt.figure(figsize=(12, 8))
plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
plt.axis('off')
plt.show()

print("Detección de objetos completada.")
```

---

### Recursos Adicionales

1. **[YOLO: You Only Look Once (arXiv)](https://arxiv.org/pdf/1506.02640.pdf)**
2. **[SSD: Single Shot MultiBox Detector (arXiv)](https://arxiv.org/pdf/1512.02325.pdf)**
3. **[Faster R-CNN: Towards Real-Time Object Detection (arXiv)](https://arxiv.org/pdf/1506.01497.pdf)**
4. **[Detecting Objects in Images Using OpenCV YOLO](https://www.learnopencv.com/object-detection-using-yolo/)**

---

La detección de objetos es una técnica poderosa y versátil con muchas aplicaciones prácticas. ¡Espero que esta introducción les haya resultado útil y emocionante!



---
# Día32
---
### Evolución de YOLO: Desde 2015 hasta 2024

La serie de modelos YOLO (You Only Look Once) ha visto una evolución significativa desde su creación en 2015. Aquí se presenta un resumen de las principales versiones y sus mejoras a lo largo del tiempo:

1. **YOLO (2015)**
   - Introdujo el concepto de detección de objetos en tiempo real utilizando una sola red convolucional.
   - Ventaja: Alta velocidad de inferencia.
   - Desventaja: Menor precisión en comparación con otros métodos existentes en ese momento.

2. **YOLO9000 (2016)**
   - Capaz de detectar más de 9000 clases de objetos mediante la combinación de detección y clasificación jerárquica.
   - Mejora en precisión y capacidad de detección de múltiples clases.

3. **YOLOv2 (2017)**
   - Introdujo mejoras como anclas dimensionadas y normalización por lotes.
   - Aumentó la precisión y la velocidad en comparación con YOLO9000.

4. **Fast YOLO (2017)**
   - Optimización adicional para aumentar la velocidad de inferencia sin sacrificar demasiada precisión.

5. **YOLOv3 (2018)**
   - Implementó una arquitectura más profunda con ResNet, mejorando la precisión en detección de objetos pequeños.
   - Introducción de detección en múltiples escalas.

6. **YOLOv4 (Abril de 2020)**
   - Incorporó varias técnicas de mejora de precisión como CSPDarknet53, MISH, y regularización por recorte.
   - Mejoras significativas en velocidad y precisión.

7. **YOLOv5 (2020)**
   - Desarrollo por Ultralytics con optimizaciones adicionales en el entrenamiento y la inferencia.
   - Aumento de la flexibilidad y la facilidad de uso.

8. **YOLOR (2021)**
   - Introducción de conocimientos representacionales y operacionales unificados para mejorar la precisión.
   - Capacidad de realizar múltiples tareas simultáneamente.

9. **YOLOv6 (2022)**
   - Mejoras en la arquitectura para una mayor eficiencia y rendimiento en dispositivos de baja potencia.

10. **YOLOv7 (2022)**
    - Introducción de técnicas avanzadas para reducir la latencia y mejorar la precisión en tiempo real.

11. **YOLOv8 (2023)**
    - Mejora en la detección de objetos pequeños y en situaciones de baja iluminación.
    - Incorporación de módulos de atención para mejor rendimiento.

12. **YOLOv9 (2024)**
    - Primer modelo inducido por transformador, utilizando GELAN (Red de Agregación de Capas Eficientes Generalizadas) y PGI (Información de Gradiente Programable).
    - Mejora en la eficiencia computacional y reducción de parámetros sin sacrificar precisión.

13. **YOLOv10 (2024)**
    - Introducción de entrenamiento sin NMS (Supresión No Máxima), lo que reduce la dependencia de la post-procesamiento y mejora la velocidad de inferencia.
    - Empleo de asignaciones duales consistentes para mayor eficiencia y precisión.

Estas mejoras han permitido que YOLO mantenga su posición como una de las arquitecturas de detección de objetos más rápidas y precisas, adaptándose continuamente a las necesidades y desafíos de las aplicaciones modernas de visión por computadora.

Referencias:
- [YOLOv9: El primer modelo inducido por transformador](https://visionplatform.ai/es/yolov9-el-primer-modelo-inducido-por-transformador/)
- [YOLOv10: Mejor, más rápido y más pequeño ahora en GitHub](https://docs.ultralytics.com/es/models/yolov10/#what-are-the-performance-benchmarks-for-yolov10-models)
- [YOLOv9 Performance Comparisons](https://arxiv.org/pdf/2405.14458v1)

---

# Día33
---
## YOLOv8 y sus Variantes con Ultralytics

En el día de hoy, vamos a profundizar en YOLOv8 y sus variantes, así como en la suite de herramientas ofrecidas por Ultralytics que estaremos utilizando en nuestros proyectos de detección de objetos. ¡Vamos a ello!

#### 🚀 Introducción a YOLOv8

YOLO (You Only Look Once) ha sido una referencia en la detección de objetos desde su primera versión lanzada en 2015. YOLOv8, desarrollado por Ultralytics, es la última iteración de esta serie, trayendo mejoras significativas en precisión, velocidad y eficiencia.

**Características de YOLOv8:**
- **Alta Precisión:** Mejoras en la arquitectura que permiten detectar objetos con mayor exactitud.
- **Velocidad de Inferencia:** Optimizado para realizar detecciones en tiempo real.
- **Eficiencia Computacional:** Reduce la carga computacional manteniendo un rendimiento superior.

#### 🛠️ Ultralytics y su Ecosistema

Ultralytics no solo ha desarrollado YOLOv8, sino que también ha creado un conjunto de herramientas y recursos para facilitar su implementación y uso en diversos proyectos de visión por computadora.

**Principales Componentes:**
- **YOLOv8 Modelos:** Variantes optimizadas para diferentes necesidades, como precisión máxima (YOLOv8x) y eficiencia (YOLOv8n).
- **Ultralytics Hub:** Plataforma centralizada para gestionar, entrenar y desplegar modelos de YOLO.
- **Documentación y Soporte:** Guías detalladas, ejemplos y una comunidad activa para ayudar a los desarrolladores.

#### 🧩 Variantes de YOLOv8

Ultralytics ha lanzado varias variantes de YOLOv8, cada una ajustada para diferentes escenarios de uso:

1. **YOLOv8n (Nano):**
   - **Características:** Optimizado para dispositivos con recursos limitados, como móviles.
   - **Ventajas:** Alta eficiencia y bajo consumo de recursos.

2. **YOLOv8s (Small):**
   - **Características:** Equilibrio entre precisión y velocidad.
   - **Ventajas:** Ideal para aplicaciones en tiempo real en dispositivos moderadamente potentes.

3. **YOLOv8m (Medium):**
   - **Características:** Mayor precisión con un compromiso razonable en velocidad.
   - **Ventajas:** Uso en aplicaciones que requieren un balance entre rendimiento y precisión.

4. **YOLOv8l (Large):**
   - **Características:** Alta precisión para tareas más exigentes.
   - **Ventajas:** Uso en sistemas con capacidad computacional alta.

5. **YOLOv8x (Extra Large):**
   - **Características:** Máxima precisión disponible en la serie YOLOv8.
   - **Ventajas:** Ideal para aplicaciones donde la precisión es crítica.

#### 🔗 Recursos de Ultralytics

- [Ultralytics GitHub](https://github.com/ultralytics): Repositorio oficial con código fuente y ejemplos.
- [Documentación de YOLOv8](https://docs.ultralytics.com/yolov8): Guía completa de uso y configuración.
- [Ultralytics Hub](https://ultralytics.com/hub): Plataforma para gestionar y desplegar modelos.

---
# Día34
---
### Aplicaciones Avanzadas de Detección de Objetos 🌍🚀**


#### 📌 Aplicaciones en Seguridad
La detección de objetos se utiliza en sistemas de videovigilancia para identificar intrusos, detectar comportamientos anómalos y alertar a las autoridades en tiempo real. Las soluciones basadas en IA pueden analizar grandes volúmenes de datos de video con precisión y rapidez, mejorando la seguridad en áreas públicas y privadas.

#### 📊 Aplicaciones en el Sector Salud
En el campo de la salud, la detección de objetos ayuda en el análisis de imágenes médicas, como radiografías y resonancias magnéticas. Esto permite a los médicos identificar anomalías, diagnosticar enfermedades y planificar tratamientos con mayor precisión.

#### 🚗 Aplicaciones en Automóviles Autónomos
Los vehículos autónomos utilizan sistemas de detección de objetos para identificar peatones, otros vehículos, señales de tráfico y obstáculos en la carretera. Esto es crucial para la navegación segura y eficiente, reduciendo el riesgo de accidentes.

#### 🏗️ Aplicaciones en la Construcción
En la industria de la construcción, la detección de objetos se usa para monitorear el progreso de proyectos, asegurar la seguridad de los trabajadores y gestionar recursos de manera eficiente. Las cámaras equipadas con IA pueden identificar áreas peligrosas y alertar a los supervisores en tiempo real.

#### 🛒 Aplicaciones en el Retail
En el comercio minorista, la detección de objetos se utiliza para el control de inventarios, la prevención de pérdidas y la mejora de la experiencia del cliente. Los sistemas inteligentes pueden rastrear productos, detectar robos y ofrecer recomendaciones personalizadas a los compradores.

#### 🌱 Aplicaciones en la Agricultura
La detección de objetos en la agricultura ayuda a monitorear el crecimiento de cultivos, identificar plagas y enfermedades, y optimizar el uso de recursos como agua y fertilizantes. Esto mejora la eficiencia y sostenibilidad de las prácticas agrícolas.

---
# Día35
---
## Técnicas de Mejora de Precisión en Detección de Objetos 🎯🔍**


#### 📈 Uso de Múltiples Escalas
Una técnica efectiva para mejorar la precisión es el uso de múltiples escalas. Al entrenar y evaluar los modelos en diferentes resoluciones de imagen, podemos captar mejor los objetos de distintos tamaños y mejorar la detección en escenarios variados.

#### 🧩 Aumento de Datos
El aumento de datos (data augmentation) implica aplicar transformaciones como rotaciones, recortes, cambios de brillo y contraste, y más a las imágenes de entrenamiento. Esto ayuda a los modelos a generalizar mejor y a ser más robustos frente a variaciones en los datos de entrada. Ultralytics facilita el aumento de datos a través de configuraciones sencillas en sus scripts de entrenamiento.

#### 🔄 Ajuste Fino de Modelos Preentrenados
El ajuste fino (fine-tuning) de modelos preentrenados es una forma poderosa de mejorar la precisión. Podemos empezar con un modelo preentrenado en un gran conjunto de datos y ajustarlo con nuestros datos específicos. Ultralytics permite la fácil configuración y ajuste fino de modelos como YOLOv5 y YOLOv8 a través de su interfaz intuitiva y comandos accesibles.

#### ⚖️ Equilibrio de Clases
En conjuntos de datos desbalanceados, algunas clases pueden estar subrepresentadas, lo que afecta la precisión. Podemos aplicar técnicas como el re-muestreo (over-sampling y under-sampling) o la ponderación de pérdida para equilibrar las clases y mejorar el rendimiento del modelo. Ultralytics proporciona opciones para manejar desequilibrios de clase en sus configuraciones de entrenamiento.

#### 📊 Evaluación y Métricas
Es crucial usar las métricas adecuadas para evaluar el desempeño de nuestros modelos. Métricas como precisión (precision), recall, F1-score y mean Average Precision (mAP) nos proporcionan una visión completa de cómo está funcionando nuestro modelo y dónde podemos mejorar. Las herramientas de Ultralytics incluyen opciones detalladas de evaluación para obtener estos indicadores clave.

#### 💡 Implementación de Ensembles
Los modelos de ensembles combinan las predicciones de múltiples modelos para obtener un resultado final más preciso. Al promediar o votar entre las predicciones, podemos reducir el sesgo y la varianza, mejorando la precisión general. Ultralytics permite la configuración de ensembles de manera eficiente, facilitando la implementación de esta técnica avanzada.

#### 🔧 Herramientas de Ultralytics
Ultralytics ofrece una serie de herramientas y configuraciones que hacen que el proceso de entrenamiento, ajuste fino y evaluación de modelos de detección de objetos sea más accesible y eficiente. Entre las características destacadas se incluyen:

- **Configuraciones de entrenamiento:** Ajustes sencillos para hiperparámetros y estrategias de aumento de datos.
- **Modelos preentrenados:** Acceso a una variedad de modelos preentrenados, listos para ajuste fino.
- **Evaluación avanzada:** Métricas detalladas y análisis de desempeño para una comprensión profunda del modelo.

Para más detalles sobre estas herramientas, visita la [documentación de Ultralytics](https://docs.ultralytics.com/es/modes/train/#what-are-the-common-training-settings-and-how-do-i-configure-them).

---
# Día36
---
### Segmentación de Imágenes con Redes Neuronales Convolucionales 🖼️🧠**

#### 🌟 ¿Qué es la Segmentación de Imágenes?
La segmentación de imágenes es una técnica en visión por computadora que divide una imagen en segmentos significativos para facilitar su análisis. A diferencia de la clasificación de imágenes, que asigna una etiqueta a toda la imagen, la segmentación de imágenes asigna una etiqueta a cada píxel, permitiendo una comprensión más detallada y precisa del contenido visual.

#### 🧩 Tipos de Segmentación de Imágenes
1. **Segmentación Semántica:** Asigna una etiqueta a cada píxel basado en la clase a la que pertenece. Por ejemplo, en una imagen de una calle, todos los píxeles pertenecientes a "coches" se etiquetan como tal, sin distinguir entre coches individuales.
2. **Segmentación de Instancias:** No solo clasifica cada píxel sino que también distingue entre diferentes instancias de la misma clase. Siguiendo el ejemplo anterior, no solo se etiqueta "coches", sino que se distingue entre cada coche individual.
3. **Segmentación Panóptica:** Combina la segmentación semántica y de instancias para ofrecer una vista completa, etiquetando tanto las clases como las instancias únicas.

#### 🛠️ Herramientas y Funciones de Ultralytics para Segmentación de Imágenes
Ultralytics proporciona herramientas poderosas para implementar y entrenar modelos de segmentación de imágenes. Aquí hay algunas características clave:

- **Modelos Preentrenados:** Utiliza modelos como YOLOv5 y YOLOv8, que ofrecen capacidades avanzadas de segmentación.
- **Configuraciones de Entrenamiento:** Ajusta parámetros como tasa de aprendizaje, épocas, y aumento de datos para optimizar el rendimiento.
- **Aumento de Datos:** Aplica técnicas de data augmentation específicas para segmentación, como rotaciones, recortes, y ajustes de brillo.
- **Evaluación Avanzada:** Usa métricas especializadas para evaluar el rendimiento de los modelos de segmentación, como Intersection over Union (IoU) y mean Average Precision (mAP).


---
# Día37
---
## Implementación de Segmentación de Imágenes con YOLO y Ultralytics - Demo Práctica 🛠️📊**

### 🔧 Herramientas Necesarias:
1. **Ultralytics YOLOv8:** Nuestro modelo de elección para la segmentación.
2. **Dataset:** Un conjunto de datos adecuado para segmentación (puede ser COCO, Pascal VOC, etc.).
3. **Entorno de Desarrollo:** Puede ser Jupyter Notebook o cualquier IDE que prefieras.

### 📚 Paso a Paso:

1. **Preparación del Entorno:**
   - Asegúrate de tener Python y las bibliotecas necesarias instaladas.
   - Clona el repositorio de Ultralytics y navega a la carpeta correspondiente.
   - Instala las dependencias:
     ```bash
     pip install ultralytics
     ```

2. **Carga del Dataset:**
   - Descarga y prepara el dataset.
   - Configura las rutas en el archivo de configuración de Ultralytics.

3. **Configuración del Modelo:**
   - Selecciona y configura el modelo YOLOv8 para segmentación.
   - Ajusta los parámetros de entrenamiento, como la tasa de aprendizaje y el número de épocas.

4. **Entrenamiento del Modelo:**
   - Inicia el entrenamiento utilizando el script de Ultralytics:
     ```python
     from ultralytics import YOLO

     # Cargar el modelo
     model = YOLO('yolov8-seg.pt')

     # Entrenar el modelo
     model.train(data='path/to/dataset', epochs=50, batch=16)
     ```

5. **Evaluación y Resultados:**
   - Después del entrenamiento, evalúa el modelo usando el conjunto de datos de validación.
   - Visualiza los resultados de la segmentación:
     ```python
     # Evaluar el modelo
     results = model.val()

     # Mostrar los resultados
     results.show()
     ```

6. **Implementación y Demo:**
   - Usa el modelo entrenado para realizar predicciones en imágenes nuevas.
   - Muestra los resultados de la segmentación en una demo práctica.
     ```python
     # Realizar inferencia en una nueva imagen
     results = model.predict('path/to/image.jpg')

     # Mostrar el resultado de la segmentación
     results.show()
     ```

### Recursos Adicionales:
- [Documentación de Ultralytics](https://docs.ultralytics.com/es/modes/train/#what-are-the-common-training-settings-and-how-do-i-configure-them)
- [Repositorio de YOLOv8 en GitHub](https://github.com/ultralytics/yolov8)

---
# Día38
---
## Introducción a los Modelos Preentrenados 📚💡


### ¿Qué son los Modelos Preentrenados? 🤔
Los modelos preentrenados son redes neuronales que han sido previamente entrenadas en grandes conjuntos de datos y están listos para ser reutilizados en diferentes tareas sin necesidad de entrenamiento desde cero.



### Beneficios de Utilizar Modelos Preentrenados 🌟
- **Ahorro de Tiempo y Recursos**: No necesitas entrenar modelos desde cero, lo que ahorra tiempo y recursos computacionales.
- **Mejor Rendimiento**: Aprovechan el conocimiento adquirido de vastos conjuntos de datos, mejorando el rendimiento en tareas específicas.
- **Fácil de Personalizar**: Puedes ajustar y adaptar estos modelos a tus necesidades específicas mediante fine-tuning.

### Ejemplos Populares 📈
- **ResNet**: Excelente para tareas de clasificación de imágenes.
- **BERT**: Popular en procesamiento del lenguaje natural (NLP).
- **YOLO**: Usado para detección de objetos en tiempo real.



### Recursos para Encontrar Modelos Preentrenados 🛠️
- **[Hugging Face](https://huggingface.co/models)**: Amplia biblioteca de modelos de NLP.
- **[TensorFlow Hub](https://tfhub.dev/)**: Gran colección de modelos para visión por computadora y más.


---
# Día39
---

## ¡Explorando los Avances en Detección de Objetos con YOLOv5, YOLOv8 y YOLOv10! 🚀

¡Hola comunidad! 🌟 Hoy quiero compartir con ustedes una revisión fascinante sobre la evolución de los algoritmos de detección de objetos YOLO (You Only Look Once). Este documento, elaborado por Muhammad Hussain de la Universidad de Huddersfield, nos lleva a través de los hitos alcanzados por YOLOv5, YOLOv8 y el revolucionario YOLOv10. Aquí les dejo algunos puntos destacados:

#### 🔍 YOLOv5
- **Innovaciones Clave**: Introduce la columna vertebral CSPDarknet y la Augmentación de Mosaico, logrando un equilibrio perfecto entre velocidad y precisión.
- **Rendimiento Superior**: Variantes del modelo desde Nano hasta Extra Grande, cada una optimizada para diferentes necesidades.

#### ⚙️ YOLOv8
- **Mejoras Arquitectónicas**: Detalles como la detección sin anclas y el uso del módulo PANet hacen que YOLOv8 sea una herramienta extremadamente versátil y eficiente.
- **Eficiencia de Entrenamiento**: Optimización de hiperparámetros automatizada y entrenamiento de precisión mixta, haciendo que el proceso sea más rápido y efectivo.

#### 🌟 YOLOv10
- **Avances Revolucionarios**: Entrenamiento sin NMS, convoluciones de gran kernel y downsampling desacoplado, permitiendo una precisión sin precedentes con menor carga computacional.
- **Perfecto para el Borde**: Diseñado específicamente para ser eficiente en dispositivos con recursos limitados, ideal para aplicaciones en tiempo real.



### ¿Por qué es Importante? 💡
Estos avances no solo mejoran la precisión y la velocidad, sino que también hacen que la implementación en dispositivos de borde sea más práctica y efectiva. ¡Imagina todas las posibilidades que esto abre en el campo de la visión por computadora!

#### 📚 ¿Te interesa profundizar más?
¡No dudes en revisar el documento completo! Conocer estos avances puede ser crucial para tus proyectos actuales y futuros en detección de objetos y visión por computadora. Aquí tienes el enlace al documento original: [YOLOV5, YOLOV8 AND YOLOV10: THE GO-TO DETECTORS FOR REAL-TIME VISION](https://arxiv.org/pdf/2407.02988v1)


---
# Día40
---
## RT-DETR revoluciona la detección de objetos en tiempo real 🚀
Hoy estoy emocionado de compartir algunos avances de vanguardia en la detección de objetos en tiempo real. Esto proviene de un emocionante artículo titulado **"DETRs Beat YOLOs on Real-time Object Detection"**. Escrito por investigadores de la Universidad de Huddersfield, presenta RT-DETR (Real-Time Detection Transformer), un cambio de juego que supera a los famosos modelos YOLO en velocidad y precisión. Aquí tienes un desglose amigable:

#### 🚀 ¿Por qué es importante?
La detección de objetos en tiempo real es crucial para aplicaciones como:
- **Seguimiento de objetos**
- **Vigilancia por video**
- **Conducción autónoma**

#### 🔍 ¿Cuál es el problema con YOLO?
Los modelos YOLO son rápidos, pero dependen de la Supresión de Máximos No Máximos (NMS), lo que los ralentiza y afecta su precisión.

#### 🌟 Presentando RT-DETR
RT-DETR es el primer detector de objetos en tiempo real basado en la arquitectura Transformer. Elimina la necesidad de NMS, logrando una mejor velocidad y precisión. ¡Vamos a profundizar en los detalles!

#### 📚 Puntos clave

1. **Codificador Híbrido Eficiente**
   - Combina la interacción de características intra-escala y la fusión de características entre escalas.
   - Reduce la latencia computacional y aumenta la precisión.

2. **Selección de Consultas con Mínima Incertidumbre**
   - Selecciona consultas de objetos de alta calidad minimizando la incertidumbre epistémica.
   - Mejora las puntuaciones de clasificación y la precisión de localización.

3. **Compensación Flexible entre Velocidad y Precisión**
   - Ajusta la velocidad sin necesidad de reentrenamiento mediante la modulación de capas del decodificador.
   - Se adapta fácilmente a diferentes escenarios en tiempo real.

#### 🧪 Los experimentos muestran…
RT-DETR fue probado contra modelos YOLO y otros detectores basados en Transformer. ¿Los resultados? RT-DETR superó a todos en velocidad y precisión, demostrando su efectividad en varios escenarios.

#### 🚧 Limitaciones y trabajo futuro
- **Desafíos:** Aún hay algunos obstáculos en escenarios específicos.
- **Mejoras Futuras:** Investigación continua para mejorar aún más el rendimiento de RT-DETR.

#### 📜 Conclusión
RT-DETR marca un avance significativo en la detección de objetos en tiempo real. Al eliminar la NMS y ofrecer ajustes flexibles de velocidad, establece un nuevo estándar, superando a los modelos avanzados de YOLO.

### ¡Profundiza más!
¿Tienes curiosidad por aprender más? Consulta el artículo completo: [DETRs Beat YOLOs on Real-time Object Detection](https://arxiv.org/pdf/2304.08069v3.pdf).
#### Recursos para Explorar Más:

- [RT-DETR: Revolucionando la Detección de Objetos en Tiempo Rea](https://youtu.be/fqgHlUH3OXQ?si=oeaOc72hnXnbigcm)
- [Notebook](https://colab.research.google.com/github/alarcon7a/rt-detr/blob/main/RT_DETR.ipynb#scrollTo=9CWLwh3Q5ybt)

---
# Día41
---
## Exploración de Segmentadores de Imágenes: Desde U-Net hasta las Arquitecturas Modernas

En mi reciente lectura del paper **"U-Net: Convolutional Networks for Biomedical Image Segmentation"** de Olaf Ronneberger, Philipp Fischer y Thomas Brox, me impresionó la innovación y eficacia de la arquitectura U-Net en la segmentación de imágenes biomédicas. Aquí les comparto un resumen y mi análisis sobre esta poderosa herramienta y otras arquitecturas relevantes en el campo.

### Resumen del Paper de U-Net

La U-Net es una red convolucional diseñada específicamente para la segmentación de imágenes biomédicas. Los puntos clave del paper son:

1. **Introducción y Motivación:**
   - La segmentación precisa en imágenes biomédicas requiere grandes cantidades de datos anotados. U-Net aborda este problema mediante una red y estrategia de entrenamiento que optimiza el uso de muestras anotadas disponibles a través de una fuerte augmentación de datos.

2. **Arquitectura del U-Net:**
   - Consiste en un camino de contracción (para capturar el contexto) y un camino de expansión (para una localización precisa), formando una estructura en forma de "U".
   - Esta arquitectura permite entrenar la red de extremo a extremo con pocas imágenes, logrando resultados superiores en desafíos de segmentación neuronal y seguimiento de células.

3. **Resultados y Rendimiento:**
   - U-Net ha ganado los desafíos ISBI 2012 y 2015 en sus respectivas categorías.
   - La segmentación de una imagen de 512x512 píxeles toma menos de un segundo en una GPU reciente.

4. **Estrategia de Entrenamiento:**
   - Uso intensivo de la augmentación de datos y entrenamiento basado en parches para manejar grandes imágenes.
   - Estrategia de superposición de parches para segmentación sin costuras.

Puedes leer el paper completo aquí: [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597).

### Otras Arquitecturas de Segmentación de Imágenes

Aparte de U-Net, hay varias arquitecturas modernas diseñadas para segmentación de imágenes, cada una con sus propias fortalezas y enfoques únicos. Aquí algunas destacadas:

1. **Mask R-CNN:**
   - **Introducción:** Extiende Faster R-CNN para la segmentación de instancias.
   - **Arquitectura:** Añade una rama de máscara en paralelo con la detección de bounding boxes.
   - **Ventajas:** Capaz de realizar detección de objetos y segmentación de instancias simultáneamente con alta precisión.
   - **Paper:** [Mask R-CNN](https://arxiv.org/abs/1703.06870)

2. **DeepLab:**
   - **Introducción:** Serie de arquitecturas con múltiples versiones (V1, V2, V3, V3+).
   - **Arquitectura:** Emplea convoluciones dilatadas para capturar información de contexto a múltiples escalas sin perder resolución espacial.
   - **Ventajas:** Excelente equilibrio entre precisión y velocidad, especialmente en aplicaciones donde la precisión es crucial.
   - **Paper:** [DeepLabV3+](https://arxiv.org/abs/1802.02611)

---
# Día42
---

## Inferencia  con YOLOv8 sobre Santa Cruz de la Sierra 🚁🔍


🌆 **Destacado del Proyecto:** Detección de objetos en tiempo real utilizando YOLOv8 en imágenes de dron de Santa Cruz de la Sierra, Bolivia.

🤖 **Stack Tecnológico:**
* Modelo: YOLOv8 de Ultralytics ajustado finamente
* Aplicación: Inferencia en tiempo real en transmisión de video

🎥 **Qué Esperar:** En este video, verán YOLOv8 en acción mientras identifica y clasifica varios elementos urbanos en tiempo real. Observen cómo el modelo detecta:
* Vehículos (coches, autobuses, camiones)
* Peatones
* Edificios
* Espacios verdes
* ¡Y más!

🧠 **Por Qué Es Importante:** Este proyecto demuestra:
1. El poder de la detección de objetos en tiempo real en entornos dinámicos
2. Posibles aplicaciones en planificación urbana, gestión del tráfico y seguridad pública
3. La adaptabilidad de los modelos de IA a contextos geográficos específicos

🔬 **Perspectivas Técnicas:**
* Rendimiento del modelo en diversas condiciones de iluminación y ángulos
* Manejo de oclusiones y vistas parciales en un entorno urbano
* Equilibrio entre velocidad de procesamiento y precisión en análisis en tiempo real

---
# Día43
---

## Visualización Avanzada de Datos con Ultralytics YOLOv8 🔥

### Introducción

En el análisis de datos, los mapas de calor son una herramienta esencial para identificar patrones y tendencias de manera visual. Utilizando la tecnología avanzada de detección de objetos de Ultralytics YOLOv8, podemos generar mapas de calor precisos que destacan las áreas de mayor actividad en un entorno determinado. Este enfoque es ideal para aplicaciones como el análisis de tráfico, monitoreo de multitudes y estudios medioambientales.

### ¿Qué es un Mapa de Calor?

Un mapa de calor es una representación gráfica de datos en la que los valores individuales en una matriz se representan con colores. Los colores cálidos indican áreas de alta densidad, mientras que los fríos muestran menor concentración. Este tipo de visualización permite una rápida interpretación de grandes volúmenes de datos.

### Ventajas de los Mapas de Calor en el Análisis de Datos

#### Visualización Intuitiva
- **Interpretación Sencilla:** Transforma datos complejos en gráficos fáciles de entender.
- **Distribución Espacial:** Ideal para mostrar cómo se distribuyen los datos en un espacio, útil en análisis geoespaciales.

#### Detección de Patrones
- **Identificación de Tendencias:** Facilita la identificación de agrupaciones y valores atípicos.
- **Comparación de Datos:** Permite analizar diferentes conjuntos de datos simultáneamente.

#### Apoyo en la Toma de Decisiones
- **Aplicaciones Empresariales:** Mejora la toma de decisiones al ofrecer una visión clara de las métricas clave.
- **Planificación Urbana y Medioambiental:** Ayuda en la visualización de recursos y la densidad poblacional.

### Cómo Funciona YOLOv8 en la Generación de Mapas de Calor

#### Detección en Tiempo Real
YOLOv8 detecta objetos en tiempo real, recopilando datos de ubicaciones y frecuencias, que luego se usan para generar un mapa de calor.

#### Codificación por Colores
Los datos se transforman en una escala de colores donde tonos cálidos indican mayor actividad.

#### Implementación con Ultralytics YOLOv8

Aquí tienes un ejemplo de cómo generar un mapa de calor utilizando YOLOv8:

```python
import cv2
from ultralytics import YOLO, solutions

# Cargar el modelo YOLOv8
model = YOLO("yolov8n.pt")
cap = cv2.VideoCapture("ruta/al/archivo/video.mp4")
assert cap.isOpened(), "Error al leer el archivo de video"
w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))

# Escritor de video
video_writer = cv2.VideoWriter("heatmap_output.avi", cv2.VideoWriter_fourcc(*"mp4v"), fps, (w, h))

# Inicializar el mapa de calor
heatmap_obj = solutions.Heatmap(
    colormap=cv2.COLORMAP_PARULA,
    view_img=True,
    shape="circle",
    names=model.names,
)

while cap.isOpened():
    success, im0 = cap.read()
    if not success:
        print("El procesamiento del video ha sido completado.")
        break
    tracks = model.track(im0, persist=True, show=False)

    im0 = heatmap_obj.generate_heatmap(im0, tracks)
    video_writer.write(im0)

cap.release()
video_writer.release()
cv2.destroyAllWindows()
```

Este código muestra cómo usar YOLOv8 para procesar un video y generar un mapa de calor en función de los objetos detectados. La visualización resultante puede ser utilizada en diversas aplicaciones, desde análisis de tráfico hasta la seguridad en eventos masivos.



### Recursos

Para aquellos que deseen profundizar en este tema, aquí tienes una selección de recursos útiles:

- **Artículo:** [Ultralytics YOLOv8 Heatmaps Documentation](https://docs.ultralytics.com/es/guides/heatmaps/#why-should-businesses-choose-ultralytics-yolov8-for-heatmap-generation-in-data-analysis)
- **Video Tutorial:** [Generación de Mapas de Calor con YOLOv8](https://youtu.be/4ezde5-nZZw?si=wEB0_0hzwqEbhVu_)

---

# Día44
---

## Recuento de Objetos Mediante Ultralytics YOLOv8 🎯

### ¿Qué es el Recuento de Objetos?

El recuento de objetos con Ultralytics YOLOv8 implica la identificación y el recuento precisos de objetos específicos en vídeos y secuencias de cámaras. YOLOv8 destaca en aplicaciones en tiempo real, proporcionando un recuento de objetos eficiente y preciso para diversos escenarios, como el análisis de multitudes y la vigilancia, gracias a sus algoritmos de última generación y a sus capacidades de aprendizaje profundo.

### Ventajas del Recuento de Objetos

#### Optimización de Recursos
El recuento de objetos facilita una gestión eficaz de los recursos, proporcionando recuentos precisos y optimizando la asignación de recursos en aplicaciones como la gestión de inventarios.

#### Seguridad Mejorada
El recuento de objetos mejora la seguridad y la vigilancia mediante el seguimiento y recuento precisos de entidades, ayudando a la detección proactiva de amenazas.

#### Toma de Decisiones Informada
El recuento de objetos ofrece información valiosa para la toma de decisiones, optimizando los procesos en el comercio minorista, la gestión del tráfico y otros ámbitos diversos.

### Implementación con Ultralytics YOLOv8

A continuación, se muestra un ejemplo de código para implementar el recuento de objetos utilizando YOLOv8:

```python
import cv2
from ultralytics import YOLO, solutions

# Cargar el modelo YOLOv8
model = YOLO("yolov8n.pt")
cap = cv2.VideoCapture("ruta/al/archivo/video.mp4")
assert cap.isOpened(), "Error al leer el archivo de video"
w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))

# Definir puntos de región
region_points = [(20, 400), (1080, 404), (1080, 360), (20, 360)]

# Escritor de video
video_writer = cv2.VideoWriter("object_counting_output.avi", cv2.VideoWriter_fourcc(*"mp4v"), fps, (w, h))

# Inicializar el contador de objetos
counter = solutions.ObjectCounter(
    view_img=True,
    reg_pts=region_points,
    names=model.names,
    draw_tracks=True,
    line_thickness=2,
)

while cap.isOpened():
    success, im0 = cap.read()
    if not success:
        print("El procesamiento del video ha sido completado.")
        break
    tracks = model.track(im0, persist=True, show=False)

    im0 = counter.start_counting(im0, tracks)
    video_writer.write(im0)

cap.release()
video_writer.release()
cv2.destroyAllWindows()
```

Este código demuestra cómo configurar un sistema de recuento de objetos utilizando YOLOv8. Los objetos detectados dentro de una región específica se contarán y se visualizarán en tiempo real.



### Recursos

Para profundizar más en este tema, aquí tienes algunos recursos útiles:

- **Documentación Oficial:** [Ultralytics YOLOv8 Object Counting Documentation](https://docs.ultralytics.com/es/guides/object-counting/#can-i-use-yolov8-for-advanced-applications-like-crowd-analysis-and-traffic-management)
- **Video Tutorial:** [Recuento de Objetos con YOLOv8](https://youtu.be/Ag2e-5_NpS0?si=JJP14f3g2agCnMfl)

---

# Día45

---

## Proyecto de Sistema de Alarma de Seguridad Mediante Ultralytics YOLOv8 🚨

### Sistema de Alarma de Seguridad

El Proyecto de Sistema de Alarma de Seguridad que utiliza Ultralytics YOLOv8 integra capacidades avanzadas de visión por ordenador para mejorar las medidas de seguridad. YOLOv8, desarrollado por Ultralytics, proporciona detección de objetos en tiempo real, lo que permite al sistema identificar y responder rápidamente a posibles amenazas para la seguridad. Este proyecto ofrece varias ventajas:

#### Detección en Tiempo Real
La eficacia de YOLOv8 permite al Sistema de Alarma de Seguridad detectar y responder a los incidentes de seguridad en tiempo real, minimizando el tiempo de respuesta.

#### Precisión
YOLOv8 es conocido por su precisión en la detección de objetos, lo que reduce los falsos positivos y aumenta la fiabilidad del sistema de alarma de seguridad.

#### Capacidad de Integración
El proyecto puede integrarse perfectamente con la infraestructura de seguridad existente, proporcionando una capa mejorada de vigilancia inteligente.

### Implementación con Ultralytics YOLOv8

A continuación, se muestra un ejemplo de código para implementar un sistema de alarma de seguridad que envía notificaciones por correo electrónico cuando se detectan objetos:

```python
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from time import time
import cv2
import torch
from ultralytics import YOLO
from ultralytics.utils.plotting import Annotator, colors

# Configuración de los parámetros del correo electrónico
password = "tu_contraseña_de_aplicación"
from_email = "tu_correo@gmail.com"
to_email = "correo_destinatario@gmail.com"

# Creación y autenticación del servidor
server = smtplib.SMTP("smtp.gmail.com: 587")
server.starttls()
server.login(from_email, password)

def send_email(to_email, from_email, object_detected=1):
    """Envía una notificación por correo electrónico indicando el número de objetos detectados; por defecto 1 objeto."""
    message = MIMEMultipart()
    message["From"] = from_email
    message["To"] = to_email
    message["Subject"] = "Alerta de Seguridad"
    message_body = f"ALERTA - ¡Se han detectado {object_detected} objetos!"
    message.attach(MIMEText(message_body, "plain"))
    server.sendmail(from_email, to_email, message.as_string())

class ObjectDetection:
    def __init__(self, capture_index):
        """Inicializa una instancia de ObjectDetection con un índice de cámara dado."""
        self.capture_index = capture_index
        self.email_sent = False
        self.model = YOLO("yolov8n.pt")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

    def predict(self, im0):
        """Realiza la predicción utilizando un modelo YOLO para la imagen de entrada `im0`."""
        results = self.model(im0)
        return results

    def display_fps(self, im0):
        """Muestra los FPS en una imagen `im0` calculando y superponiéndolos como texto blanco sobre un rectángulo negro."""
        fps = 1 / (time() - self.start_time)
        text = f"FPS: {int(fps)}"
        text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1.0, 2)[0]
        gap = 10
        cv2.rectangle(im0, (20 - gap, 70 - text_size[1] - gap), (20 + text_size[0] + gap, 70 + gap), (255, 255, 255), -1)
        cv2.putText(im0, text, (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 0), 2)

    def plot_bboxes(self, results, im0):
        """Dibuja las cajas delimitadoras en una imagen dada los resultados de la detección; retorna la imagen anotada y las IDs de clase."""
        class_ids = []
        annotator = Annotator(im0, 3, results[0].names)
        boxes = results[0].boxes.xyxy.cpu()
        clss = results[0].boxes.cls.cpu().tolist()
        for box, cls in zip(boxes, clss):
            class_ids.append(cls)
            annotator.box_label(box, label=results[0].names[int(cls)], color=colors(int(cls), True))
        return im0, class_ids

    def __call__(self):
        """Ejecuta la detección de objetos en fotogramas de video desde una transmisión de cámara, dibujando y mostrando los resultados."""
        cap = cv2.VideoCapture(self.capture_index)
        assert cap.isOpened()
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        while True:
            self.start_time = time()
            ret, im0 = cap.read()
            assert ret
            results = self.predict(im0)
            im0, class_ids = self.plot_bboxes(results, im0)

            if len(class_ids) > 0 and not self.email_sent:  # Solo envía correo si no se ha enviado antes
                send_email(to_email, from_email, len(class_ids))
                self.email_sent = True
            elif len(class_ids) == 0:
                self.email_sent = False

            self.display_fps(im0)
            cv2.imshow("Detección YOLOv8", im0)
            if cv2.waitKey(5) & 0xFF == 27:
                break
        cap.release()
        cv2.destroyAllWindows()
        server.quit()

# Llama a la clase Detección de Objetos y ejecuta la inferencia
detector = ObjectDetection(capture_index=0)
detector()
```

Este código muestra cómo configurar un sistema de alarma de seguridad que envía una notificación por correo electrónico si se detecta algún objeto. La notificación se envía una sola vez por detección, pero puedes personalizar el código según las necesidades de tu proyecto.


### Recursos

Para aprender más sobre cómo implementar y mejorar sistemas de alarma de seguridad utilizando YOLOv8, aquí tienes algunos recursos adicionales:

- **Documentación Oficial:** [Ultralytics YOLOv8 Security Alarm System Documentation](https://docs.ultralytics.com/es/guides/security-alarm-system/#how-can-i-reduce-the-frequency-of-false-positives-in-my-security-system-using-ultralytics-yolov8)
- **Video Tutorial:** [Cómo Configurar un Sistema de Alarma de Seguridad con YOLOv8](https://youtu.be/_1CmwUzoxY4?si=iOT9_q3aRQrh3FIF)


---

# Día46
---

## Gestión de Colas Mediante Ultralytics YOLOv8 🚀

### ¿Qué es la Gestión de Colas?

La gestión de colas mediante Ultralytics YOLOv8 consiste en organizar y controlar colas de personas o vehículos para reducir los tiempos de espera y mejorar la eficiencia. Se trata de optimizar las colas para mejorar la satisfacción del cliente y el rendimiento del sistema en diversos entornos como comercios, bancos, aeropuertos y centros sanitarios.

### Ventajas de la Gestión de Colas

#### Tiempos de Espera Reducidos
Los sistemas de gestión de colas organizan eficazmente las colas, minimizando los tiempos de espera de los clientes. Esto mejora los niveles de satisfacción, ya que los clientes pasan menos tiempo esperando y más tiempo interactuando con los productos o servicios.

#### Mayor Eficiencia
La implantación de la gestión de colas permite a las empresas asignar recursos de forma más eficaz. Analizando los datos de las colas y optimizando el despliegue de personal, las empresas pueden agilizar las operaciones, reducir costes y mejorar la productividad general.

### Aplicaciones en el Mundo Real

#### Logística
- **Gestión de colas en el mostrador de venta de billetes del aeropuerto mediante Ultralytics YOLOv8:** En aeropuertos, YOLOv8 se utiliza para monitorizar y gestionar las colas en los mostradores de venta de billetes, reduciendo los tiempos de espera y mejorando la experiencia del pasajero. 
#### Venta al por Menor
- **Control de colas en multitudes mediante Ultralytics YOLOv8:** En tiendas minoristas, YOLOv8 ayuda a gestionar las colas en las cajas registradoras, mejorando el flujo de clientes y reduciendo la congestión. 

### Ejemplo de Implementación de Gestión de Colas Mediante YOLOv8

A continuación, se muestra un ejemplo de código que implementa un sistema de gestión de colas utilizando YOLOv8:

```python
import cv2
from ultralytics import YOLO, solutions

# Cargar el modelo YOLOv8
model = YOLO("yolov8n.pt")

# Capturar el video
cap = cv2.VideoCapture("path/to/video/file.mp4")
assert cap.isOpened(), "Error al leer el archivo de video"
w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))

# Configurar el escritor de video
video_writer = cv2.VideoWriter("queue_management.avi", cv2.VideoWriter_fourcc(*"mp4v"), fps, (w, h))

# Definir la región de la cola
queue_region = [(20, 400), (1080, 404), (1080, 360), (20, 360)]

# Inicializar el gestor de colas
queue = solutions.QueueManager(
    names=model.names,
    reg_pts=queue_region,
    line_thickness=3,
    fontsize=1.0,
    region_color=(255, 144, 31),
)

while cap.isOpened():
    success, im0 = cap.read()

    if success:
        tracks = model.track(im0, show=False, persist=True, verbose=False)
        out = queue.process_queue(im0, tracks)

        video_writer.write(im0)
        if cv2.waitKey(1) & 0xFF == ord("q"):
            break
        continue

    print("El fotograma de video está vacío o el procesamiento de video se ha completado con éxito.")
    break

cap.release()
cv2.destroyAllWindows()
```

Este código demuestra cómo gestionar colas en tiempo real utilizando Ultralytics YOLOv8, proporcionando un sistema eficiente para reducir los tiempos de espera y mejorar la experiencia del usuario.



### Recursos

Para profundizar en la gestión de colas utilizando Ultralytics YOLOv8, te recomendamos los siguientes recursos:

- **Documentación Oficial:** [Ultralytics YOLOv8 Queue Management Documentation](https://docs.ultralytics.com/es/guides/queue-management/#what-are-some-real-world-applications-of-ultralytics-yolov8-in-queue-management)
- **Video Tutorial:** [Cómo Implementar Gestión de Colas con YOLOv8](https://youtu.be/gX5kSRD56Gs?si=dN2FFjxXj0JyY_-z)
- **Artículo Técnico:** [Revolutionizing Retail Analytics: Advancing Inventory and Customer Insight with AI](https://arxiv.org/abs/2405.00023#)


# Día47

---

## Gestión de Aparcamientos Mediante Ultralytics YOLOv8 🚀

### ¿Qué es el Sistema de Gestión de Aparcamientos?

La gestión de aparcamientos con Ultralytics YOLOv8 garantiza un aparcamiento eficaz y seguro, organizando las plazas y controlando la disponibilidad en tiempo real. YOLOv8 optimiza la gestión de los aparcamientos mediante la detección de vehículos en tiempo real y proporciona información sobre la ocupación de los espacios, lo que permite una experiencia de usuario más fluida y una mayor seguridad.

### Ventajas del Sistema de Gestión de Aparcamientos

#### Eficacia
La gestión de aparcamientos optimiza el uso de las plazas disponibles, reduciendo la congestión y mejorando el flujo de tráfico dentro de los aparcamientos.

#### Seguridad y Protección
La integración de YOLOv8 en la gestión de aparcamientos mejora la seguridad de las personas y los vehículos mediante medidas avanzadas de vigilancia y detección de incidentes.

#### Reducción de Emisiones
La gestión eficiente del flujo de tráfico en los aparcamientos minimiza los tiempos muertos y, por ende, las emisiones de los vehículos, contribuyendo a un entorno más limpio y sostenible.

### Aplicaciones en el Mundo Real

#### Aparcamientos Inteligentes
- **Aparcamientos Analíticos Utilizando Ultralytics YOLOv8:** Implementación de YOLOv8 para el análisis en tiempo real de la ocupación de plazas de aparcamiento, proporcionando datos críticos para la optimización de recursos y la mejora de la experiencia del usuario. [Leer más aquí](https://www.smartcitiesdive.com/parking-management/yolov8/).

#### Gestión de Tráfico
- **Gestión del Aparcamiento Vista Aérea Mediante Ultralytics YOLOv8:** Utilización de YOLOv8 en cámaras de visión aérea para gestionar y monitorear el uso de los aparcamientos en grandes instalaciones como centros comerciales y aeropuertos. [Leer más aquí](https://www.techrepublic.com/article/ai-in-traffic-management/).

### Flujo de Trabajo del Código del Sistema de Gestión de Aparcamientos

#### Selección de Puntos de Aparcamiento

Definir las zonas de aparcamiento es una tarea crítica en la gestión de aparcamientos. Ultralytics facilita este proceso con una herramienta que permite delinear zonas de aparcamiento de manera sencilla y visual. A continuación, te mostramos cómo implementar esta funcionalidad:

1. **Captura de Imagen:**
   Captura un fotograma de la secuencia de vídeo o cámara donde quieras gestionar el aparcamiento.

2. **Interfaz Gráfica para la Selección de Zonas:**
   Utiliza el siguiente código para iniciar una interfaz gráfica donde puedes seleccionar una imagen y empezar a delinear las regiones de aparcamiento haciendo clic con el ratón para crear polígonos.

   ```python
   from ultralytics import solutions

   solutions.ParkingPtsSelection()
   ```

3. **Guardado de Zonas:**
   Después de definir las zonas de aparcamiento, haz clic en "save" para almacenar un archivo JSON con los datos en tu directorio de trabajo. Este archivo se utilizará para el procesamiento adicional.

#### Ejemplo de Implementación del Sistema de Gestión de Aparcamientos

A continuación, se muestra un ejemplo de código para gestionar un aparcamiento utilizando YOLOv8:

```python
import cv2
from ultralytics import solutions

# Ruta al archivo JSON creado con la aplicación de selección de puntos
polygon_json_path = "bounding_boxes.json"

# Captura de video
cap = cv2.VideoCapture("Path/to/video/file.mp4")
assert cap.isOpened(), "Error al leer el archivo de video"
w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))

# Escritor de video
video_writer = cv2.VideoWriter("parking_management.avi", cv2.VideoWriter_fourcc(*"mp4v"), fps, (w, h))

# Inicializar el objeto de gestión de aparcamientos
management = solutions.ParkingManagement(model_path="yolov8n.pt")

while cap.isOpened():
    ret, im0 = cap.read()
    if not ret:
        break

    json_data = management.parking_regions_extraction(polygon_json_path)
    results = management.model.track(im0, persist=True, show=False)

    if results[0].boxes.id is not None:
        boxes = results[0].boxes.xyxy.cpu().tolist()
        clss = results[0].boxes.cls.cpu().tolist()
        management.process_data(json_data, im0, boxes, clss)

    management.display_frames(im0)
    video_writer.write(im0)

cap.release()
video_writer.release()
cv2.destroyAllWindows()
```

Este código proporciona un flujo de trabajo completo para la gestión de aparcamientos mediante YOLOv8, desde la selección de zonas de aparcamiento hasta la monitorización y análisis en tiempo real.
 

### Recursos

Para explorar más sobre la gestión de aparcamientos utilizando Ultralytics YOLOv8, te recomendamos los siguientes recursos:

- **Documentación Oficial:** [Ultralytics YOLOv8 Parking Management Documentation](https://docs.ultralytics.com/es/guides/parking-management/#what-are-some-real-world-applications-of-ultralytics-yolov8-in-parking-lot-management)
- **Video Tutorial:** [Cómo Implementar Gestión de Aparcamientos con YOLOv8](https://www.youtube.com/watch?v=3K4vXGgf5rk)
- **Video Tutorial:** [Detección de espacios libres de parking en tiempo real](https://youtu.be/j93sLIV2bHU?si=cbY7Y_nC0m0ORHwy)

# Día48
---
## Detección de Incendios Forestales con Tecnología Avanzada 🚁

### ¿Qué es la Detección de Incendios Forestales?
La detección de incendios forestales implica el uso de tecnologías avanzadas como la visión por computadora, drones y dispositivos IoT para identificar rápidamente señales de incendios en áreas forestales. Este enfoque combina imágenes satelitales, sensores en tiempo real y algoritmos de inteligencia artificial para monitorear vastas extensiones de terreno, alertando a las autoridades y equipos de emergencia de forma temprana y precisa.

### ¿Ventajas de la Detección de Incendios Forestales?
- **Respuesta rápida y precisa**: La tecnología avanzada permite la detección y el monitoreo en tiempo real, lo que reduce significativamente el tiempo de respuesta ante un incendio.
- **Cobertura amplia**: Drones y satélites pueden cubrir grandes áreas, incluso en terrenos difíciles, proporcionando una vigilancia constante y detallada.
- **Reducción de daños**: La detección temprana permite a las autoridades tomar medidas antes de que el incendio se propague, minimizando los daños ambientales y económicos.

### Aplicaciones en el Mundo Real
- **España**: El uso de drones equipados con cámaras térmicas e inteligencia artificial ha sido implementado en varias regiones para detectar focos de incendios y realizar un monitoreo constante del entorno forestal .
- **Estados Unidos**: En California, donde los incendios forestales son un problema recurrente, se utilizan redes de sensores IoT y satélites de monitoreo para alertar de incendios en sus fases iniciales, permitiendo una respuesta más efectiva .
- **Australia**: Después de los devastadores incendios de 2019-2020, el país ha intensificado el uso de tecnología avanzada, como drones y análisis de imágenes satelitales, para mejorar sus capacidades de respuesta ante incendios forestales .

### Ejemplo de Flujo de Trabajo para la Detección de Incendios
1. **Implementación de Drones**: Drones equipados con cámaras térmicas sobrevuelan áreas forestales.
2. **Análisis de Imágenes**: Las imágenes capturadas son analizadas mediante algoritmos de visión por computadora que detectan patrones asociados a incendios.
3. **Alertas en Tiempo Real**: Los dispositivos IoT y las redes de sensores envían alertas automáticas a los centros de control.
4. **Acciones Correctivas**: Las autoridades movilizan recursos a las áreas afectadas antes de que el incendio se propague.



### Recursos Adicionales
- **[[Video sobre un sistema de prevención, detección y monitorización de incendios forestales](https://youtu.be/WF5Rwg4tajE?si=wkpDhcoIcJxNomjW)** video

- **[Aprovechar la inteligencia artificial para luchar contra los incendios forestales](https://youtu.be/PECWS9aDwcY?si=SF-5BiTEYnpXzHTU)**


---

# Día49

---
## Detección de Plagas en Cultivos 🌾

### ¿Qué es la Detección de Plagas en Cultivos?
La detección de plagas en cultivos mediante visión artificial y tecnologías avanzadas se enfoca en identificar y monitorear la presencia de plagas y enfermedades en plantas. Este proceso es parte de la agricultura de precisión, que utiliza herramientas tecnológicas como drones, sensores IoT y algoritmos de aprendizaje automático para mejorar la gestión de cultivos y optimizar el uso de recursos.

### ¿Ventajas de la Detección de Plagas en Cultivos?
- **Monitoreo Proactivo**: La detección temprana permite a los agricultores intervenir antes de que las plagas causen daños significativos, reduciendo la necesidad de tratamientos agresivos.
- **Uso Eficiente de Recursos**: La visión artificial permite aplicar pesticidas y fertilizantes solo en áreas afectadas, minimizando el uso de químicos y reduciendo el impacto ambiental.
- **Aumento del Rendimiento**: Identificar problemas de manera temprana y específica mejora la salud de las plantas y, en consecuencia, el rendimiento de los cultivos.

### Aplicaciones en el Mundo Real
- **Estados Unidos**: En California, se utilizan drones equipados con cámaras multispectrales para detectar plagas en cultivos de almendras, ayudando a los agricultores a identificar áreas afectadas y aplicar tratamientos localizados .
- **Países Bajos**: En los Países Bajos, un sistema integrado que combina sensores IoT y visión por computadora se utiliza en invernaderos para monitorear condiciones de cultivo y detectar plagas, optimizando la producción hortícola .
- **India**: En la región agrícola de Punjab, se ha implementado un sistema basado en visión artificial para monitorear cultivos de arroz, identificando infestaciones de plagas y enfermedades con alta precisión .

### Ejemplo de Flujo de Trabajo para la Detección de Plagas
1. **Captura de Imágenes**: Drones equipados con cámaras multispectrales o sensores IoT recopilan imágenes de los cultivos.
2. **Análisis de Imágenes**: Algoritmos de visión artificial procesan las imágenes para identificar signos de plagas y enfermedades.
3. **Generación de Informes**: Se generan informes detallados sobre la ubicación y severidad de las infestaciones.
4. **Intervención Selectiva**: Los agricultores aplican tratamientos específicos en las áreas afectadas, reduciendo el impacto ambiental y mejorando la eficacia del tratamiento.

### Casos de Éxito
- **Agricultura de Precisión en California**: Los agricultores han implementado sistemas de detección de plagas basados en drones y visión artificial que han logrado una reducción del 40% en el uso de pesticidas, aumentando la eficiencia y sostenibilidad de la producción de almendras .
- **Invernaderos en los Países Bajos**: La integración de sensores y visión artificial en invernaderos ha permitido a los productores reducir los costos de control de plagas en un 30% y mejorar el rendimiento de los cultivos de vegetales .
- **Sistema en India**: El uso de visión artificial para detectar plagas en cultivos de arroz ha permitido a los agricultores reducir las pérdidas por infestación en un 25%, optimizando el uso de recursos y aumentando el rendimiento de la cosecha .


---

# Día50
---
## Introducción a NLP - Definición, Aplicaciones e Historia

#### Introducción

El Procesamiento de Lenguaje Natural (NLP, por sus siglas en inglés) es una de las áreas más dinámicas de la inteligencia artificial, con aplicaciones que van desde asistentes virtuales hasta traducción automática. Esta tecnología permite a las máquinas entender y generar lenguaje humano de manera significativa, conectando la comunicación humana con las capacidades computacionales. En este artículo, exploraremos la definición de NLP, sus aplicaciones más relevantes, y un recorrido por su historia hasta el presente.


#### Definición del NLP

El Procesamiento de Lenguaje Natural es un campo interdisciplinario que combina la lingüística, la informática y la inteligencia artificial con el objetivo de desarrollar sistemas capaces de comprender, interpretar y generar lenguaje humano. 

**Componentes clave del NLP:**
- **Análisis morfológico:** Estudio de la estructura interna de las palabras.
- **Análisis sintáctico:** Examen de la estructura gramatical de las oraciones.
- **Análisis semántico:** Interpretación del significado de las palabras y frases.
- **Análisis pragmático:** Comprensión del contexto y la intención del hablante.

**Desafíos del NLP:**
- **Ambigüedad del lenguaje:** Las palabras pueden tener múltiples significados.
- **Variaciones lingüísticas:** Dialectos, jergas y expresiones idiomáticas.
- **Contexto cultural:** Interpretación de referencias culturales y humor.
- **Procesamiento en tiempo real:** Análisis y respuesta rápida en conversaciones.


#### Aplicaciones del NLP

El NLP ha encontrado aplicaciones en una amplia variedad de campos:

- **Asistentes Virtuales:** Siri, Alexa, Google Assistant.
- **Traducción Automática:** Google Translate, DeepL.
- **Análisis de Sentimientos:** Clasificación emocional de textos en redes sociales.
- **Sistemas de Recomendación:** Netflix, Amazon.
- **Chatbots y Atención al Cliente:** Mejorando la eficiencia en la resolución de consultas.
- **Resumen Automático de Textos:** Creación de resúmenes coherentes de documentos largos.
- **Corrección Ortográfica y Gramatical:** Herramientas como Grammarly.
- **Reconocimiento y Síntesis de Voz:** Dictado y transcripción automática.
- **Extracción de Información:** Obtención de datos estructurados de textos no estructurados.
- **Sistemas de Respuesta a Preguntas:** Plataformas como IBM Watson.


#### Historia del NLP

##### **Los Primeros Pasos (1950s-1960s)**
El NLP surge como una disciplina formal en la década de 1950, cuando Alan Turing propone la famosa prueba de Turing en su artículo "Computing Machinery and Intelligence". La prueba se convierte en un criterio para evaluar la inteligencia de las máquinas, marcando el inicio de un campo que se centraría en la interacción entre humanos y máquinas a través del lenguaje.

Uno de los primeros logros en NLP fue el Experimento de Georgetown en 1954, donde se tradujeron automáticamente más de 60 oraciones rusas al inglés. Aunque los resultados iniciales generaron grandes expectativas, el progreso fue más lento de lo esperado, y el informe ALPAC en 1966 llevó a una reducción significativa en la financiación para la traducción automática.

##### **La Era de las Reglas (1960s-1980s)**
Durante las décadas de 1960 y 1970, el NLP se enfocó en sistemas basados en reglas, como ELIZA, un programa que simulaba conversaciones humanas, y SHRDLU, que comprendía instrucciones en un contexto limitado. Sin embargo, la complejidad del lenguaje humano y las limitaciones de los sistemas basados en reglas evidenciaron la necesidad de enfoques más robustos.

##### **El Giro Estadístico (1980s-1990s)**
El auge del poder computacional y la disponibilidad de grandes volúmenes de texto llevaron a una revolución en NLP con la introducción de métodos estadísticos. Los Modelos Ocultos de Markov (HMM) y los primeros algoritmos de aprendizaje automático empezaron a reemplazar los enfoques basados en reglas. Estos métodos permitieron un análisis más flexible y adaptativo del lenguaje, sentando las bases para los avances futuros.

##### **El Aprendizaje Profundo y la Explosión de Datos (2000s-2010s)**
Con el aumento exponencial de datos disponibles y la potencia computacional, los modelos de redes neuronales comenzaron a dominar el campo del NLP. En 2018, Google introdujo BERT (Bidirectional Encoder Representations from Transformers), un modelo que revolucionó el campo al interpretar el contexto bidireccional de las palabras, mejorando significativamente la precisión en tareas como la traducción y la generación de texto.

##### **El Presente y el Futuro del NLP (2020s-Presente)**
En la última década, la investigación en NLP ha avanzado a pasos agigantados con el desarrollo de modelos como GPT-4 de OpenAI, Gemini de Google DeepMind, Claude de Anthropic, y LLaMA 3 de Meta. Estos modelos no solo han incrementado la precisión en tareas de procesamiento de lenguaje, sino que también han abierto nuevas posibilidades para la generación de texto coherente y natural.

Estos avances se deben a técnicas innovadoras como los transformers, la atención jerárquica, y la integración de grandes volúmenes de datos no estructurados. Sin embargo, el futuro del NLP también enfrenta desafíos como la necesidad de modelos más eficientes, la reducción de sesgos, y el desarrollo de tecnologías que sean éticamente responsables y accesibles a nivel global.

---

#### Recursos para Profundizar

1. **[Curso de NLP en Coursera por Stanford University](https://www.coursera.org/specializations/natural-language-processing)**
2. **[Documentación de GPT-4 en OpenAI](https://platform.openai.com/docs/guides/gpt)**
3. **[Papers on Gemini AI and Google DeepMind](https://www.deepmind.com/research)**
4. **[Anthropic’s Claude: Model Overview](https://www.anthropic.com/news/claude-3-5-sonnet)**
5. **[Research on LLaMA 3 by Meta AI](https://ai.facebook.com/research/)**
6. **[Exploración del futuro del NLP: Publicación de Microsoft Research](https://www.microsoft.com/en-us/research/)**

---

# Día51
---
## Conceptos Clave en NLP: Tokenización, Lematización y Stemming


En el procesamiento de lenguaje natural (NLP), la **tokenización**, **lematización** y **stemming** son pasos clave en el preprocesamiento de datos de texto, permitiendo a los algoritmos de aprendizaje automático entender y manipular el lenguaje humano de manera efectiva. Vamos a explorar en qué consisten estas técnicas, sus aplicaciones más comunes y cuándo es adecuado utilizarlas en un proyecto de NLP.

## 1. Tokenización

### Definición
La tokenización es el proceso de dividir un texto en partes más pequeñas llamadas "tokens", que suelen ser palabras, aunque también pueden ser frases o caracteres, dependiendo de la granularidad necesaria. 

### ¿Por qué se usa?
La tokenización se utiliza para descomponer texto en unidades que los modelos puedan entender. En muchas aplicaciones de NLP, los modelos no pueden trabajar con grandes secuencias de caracteres o palabras, por lo que dividir el texto en tokens permite el análisis y procesamiento más detallado. Es fundamental en tareas como clasificación de texto, análisis de sentimientos y traducción automática.

### Casos de uso:
- **Análisis de sentimientos**: Detectar palabras clave para determinar si una reseña es positiva o negativa.
- **Clasificación de documentos**: Dividir los textos en palabras clave para categorizarlos.
- **Generación de texto**: Modelos como GPT requieren tokenizar los datos para procesar la entrada y generar respuestas.

### Ejemplo de código actualizado usando `nltk`:
```python
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize

text = "El sol brilla intensamente hoy"
tokens = word_tokenize(text)
print(tokens)
```

### Librerías recomendadas:
- **nltk**: Ideal para prototipos rápidos y proyectos educativos.
- **spaCy**: Más eficiente en proyectos de gran escala.

## 2. Stemming

### Definición
El stemming es un proceso que reduce las palabras a su raíz o base morfológica. El objetivo es normalizar las variaciones de una palabra que tienen significados similares pero distintas formas gramaticales.

### ¿Por qué se usa?
Se utiliza cuando se busca una forma simplificada y rápida de reducir las palabras a sus formas básicas. Aunque el stemming no siempre devuelve palabras válidas del idioma (p. ej., "corriendo" se convierte en "corr"), es útil para tareas en las que las variaciones de la misma palabra no deben tener un impacto en el modelo, como en sistemas de recuperación de información o motores de búsqueda.

### Casos de uso:
- **Motores de búsqueda**: Facilita la búsqueda encontrando la raíz común entre palabras relacionadas (p. ej., buscar "corriendo" también devuelve resultados para "correr").
- **Clasificación de texto**: Simplificar las palabras ayuda a reducir la dimensionalidad de los datos y mejorar el rendimiento de los modelos.

### Ejemplo de código actualizado usando `nltk`:
```python
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()
words = ["corriendo", "corrí", "correrá"]
stemmed_words = [stemmer.stem(word) for word in words]
print(stemmed_words)
```

### Librerías recomendadas:
- **nltk**: Implementa diversos algoritmos de stemming, como el Porter Stemmer.
- **SnowballStemmer**: Una versión más avanzada y multilingüe del Porter Stemmer.

## 3. Lematización

### Definición
La lematización es un proceso más avanzado que el stemming, ya que reduce las palabras a su lema, que es la forma base de una palabra según su categoría gramatical. A diferencia del stemming, la lematización siempre devuelve palabras reales del idioma.

### ¿Por qué se usa?
Se utiliza cuando se necesita un análisis más preciso del lenguaje. Al tener en cuenta el contexto y la gramática, la lematización permite obtener formas de palabras que son gramaticalmente correctas, lo cual es útil en aplicaciones que requieren un entendimiento detallado del lenguaje.

### Casos de uso:
- **Traducción automática**: Es importante obtener la forma correcta de una palabra según su contexto gramatical.
- **Análisis de textos legales**: La lematización permite entender el significado preciso de las palabras, lo que es crucial en estos entornos.

### Ejemplo de código actualizado usando `nltk`:
```python
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
nltk.download('omw-1.4')

lemmatizer = WordNetLemmatizer()
words = ["corriendo", "corrí", "correrá"]
lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]
print(lemmatized_words)
```

### Librerías recomendadas:
- **nltk**: Facilita el uso de WordNet para lematización.
- **spaCy**: Ofrece una lematización rápida y precisa, ideal para grandes volúmenes de datos.

## Recursos adicionales

 **Documentación oficial:**
   - [NLTK Documentation](https://www.nltk.org/)
   - [spaCy Tokenization and Lemmatization](https://spacy.io/usage/linguistic-features#tokenization)


[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1sB8GS_IzmCn1-UwVOLFar0nbtBzZ2eja?usp=sharing) [Lematización y Stemming](https://colab.research.google.com/drive/1sB8GS_IzmCn1-UwVOLFar0nbtBzZ2eja?usp=sharing) 

---
# Día52
---
## Preprocesamiento de Texto y Normalización


En procesamiento de lenguaje natural (NLP), el **preprocesamiento de texto** y la **normalización** son pasos fundamentales para transformar datos textuales no estructurados en un formato que los modelos puedan entender. Este proceso involucra la limpieza y estructuración de texto, eliminando ruido y asegurando que las palabras estén en su forma más útil. Al igual que otros métodos de preprocesamiento en ciencia de datos, este paso es esencial para mejorar la precisión y eficiencia de los modelos.

Vamos a explorar las principales técnicas de preprocesamiento y normalización y por qué son esenciales en cualquier proyecto de NLP.

## 1. Conversión a minúsculas

La conversión de texto a minúsculas asegura que todas las palabras estén en un formato consistente. Por ejemplo, "Casa" y "casa" se convertirán en "casa".

### ¿Por qué se usa?
En muchos casos, los modelos NLP no hacen distinción entre mayúsculas y minúsculas, por lo que la conversión a minúsculas reduce la cantidad de vocabulario y mejora la eficiencia del modelo.

### Casos de uso:
- **Análisis de sentimientos**: Evita considerar palabras en mayúsculas como términos diferentes.
- **Clasificación de textos**: Simplifica el vocabulario, haciendo que las palabras se procesen de manera uniforme.

### Ejemplo de código:
```python
text = "El Sol Brilla Intensamente."
lower_text = text.lower()
print(lower_text)  # Resultado: el sol brilla intensamente.
```

## 2. Eliminación de stopwords

Las **stopwords** son palabras comunes como "el", "de", "y", que no aportan mucho valor semántico en el análisis y pueden ser eliminadas para reducir el ruido en el texto.

### ¿Por qué se usa?
Eliminar estas palabras puede reducir significativamente la dimensionalidad del texto sin perder significado. Esto facilita el procesamiento y mejora la velocidad de los modelos.

### Casos de uso:
- **Clasificación de documentos**: Filtra las palabras comunes que no son útiles para identificar la categoría del documento.
- **Motores de búsqueda**: Ayuda a enfocar las búsquedas en términos relevantes.

### Ejemplo de código actualizado:
```python
from nltk.corpus import stopwords
nltk.download('stopwords')

text = "El sol brilla intensamente sobre el mar."
stop_words = set(stopwords.words('spanish'))
filtered_text = [word for word in text.split() if word.lower() not in stop_words]
print(filtered_text)  # Resultado: ['sol', 'brilla', 'intensamente', 'mar']
```

## 3. Eliminación de puntuación

La puntuación, como comas, puntos y signos de exclamación, no aporta significado en muchas tareas de NLP, por lo que se elimina durante el preprocesamiento.

### ¿Por qué se usa?
Elimina elementos que no son útiles para los modelos y que podrían distorsionar el análisis del texto.

### Casos de uso:
- **Análisis de sentimientos**: Las emociones no están influenciadas por la puntuación, por lo que eliminarla mejora la interpretación del texto.
- **Traducción automática**: Facilita la correspondencia de términos entre idiomas al eliminar signos innecesarios.

### Ejemplo de código:
```python
import string

text = "¡Hola! ¿Cómo estás?"
clean_text = text.translate(str.maketrans('', '', string.punctuation))
print(clean_text)  # Resultado: Hola Cómo estás
```

## 4. Normalización de contracciones


Este paso involucra expandir palabras contraídas como "I'm" a "I am" o "he's" a "he is". Es más común en inglés, pero también se puede aplicar en otros idiomas.

### ¿Por qué se usa?
Para evitar que las contracciones sean tratadas como términos diferentes, la expansión de contracciones unifica el vocabulario.

### Casos de uso:
- **Chatbots**: Un chatbot necesita comprender la forma completa de una palabra para dar respuestas más precisas.
- **Análisis de texto social**: Al lidiar con texto informal, es necesario expandir contracciones para mejorar la comprensión.

### Ejemplo de código:
```python
import contractions

text = "I'm going to the store."
expanded_text = contractions.fix(text)
print(expanded_text)  # Resultado: I am going to the store.
```

## 5. Lematización y Stemming

Estos procesos, que ya exploramos en detalle en el **Día 51**, se usan en el preprocesamiento para reducir las palabras a sus formas base.

- **Stemming**: Reduce las palabras a su raíz, aunque esta no siempre es una palabra válida.
- **Lematización**: Reduce las palabras a su forma gramatical base (lema), asegurando que el resultado sea una palabra correcta.

## 6. Remoción de caracteres especiales y números

Elimina caracteres no alfabéticos y números del texto que no aportan valor semántico en muchas aplicaciones de NLP.

### ¿Por qué se usa?
El texto a menudo contiene caracteres especiales, como "@" o "#", que no son relevantes para muchas tareas de procesamiento. La eliminación de estos caracteres facilita el análisis.

### Casos de uso:
- **Análisis de comentarios en redes sociales**: Remover hashtags, menciones o números que no contribuyen al análisis de sentimientos o a la comprensión de temas.
- **Traducción automática**: Facilita el alineamiento de texto en múltiples idiomas eliminando caracteres no alfabéticos.

### Ejemplo de código:
```python
import re

text = "La temperatura es de 25°C, pero subirá a 30°C."
clean_text = re.sub(r'\d+|\W+', ' ', text)
print(clean_text)  # Resultado: La temperatura es de C pero subirá a C
```

## Recursos adicionales

 **Documentación oficial:**
   - [NLTK Documentation](https://www.nltk.org/)
   - [spaCy Tokenization and Preprocessing](https://spacy.io/usage/linguistic-features#tokenization)

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1_hwnv-ZM0ZKlnxthGNs8bPfB7OJffiT7?usp=sharing) [Preprocesamiento de texto](https://colab.research.google.com/drive/1_hwnv-ZM0ZKlnxthGNs8bPfB7OJffiT7?usp=sharing) 

---
# Día53
# Día54
# Día55
# Día56
# Día57
# Día58
# Día59
# Día60
# Día61
# Día62
# Día63
# Día64
# Día65
# Día66
# Día67
# Día68
# Día69
# Día70
# Día71
# Día72
# Día73
# Día74
# Día75
# Día76
# Día77
# Día78
# Día79
# Día80
# Día81
# Día82
# Día83
# Día84
# Día85
# Día86
# Día87
# Día88
# Día89
# Día90
# Día91
# Día92
# Día93
# Día94
# Día95
# Día96
# Día97
# Día98
# Día99
# Día100
