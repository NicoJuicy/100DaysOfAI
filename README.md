# 100 D√≠as de IA

| Libros y Recursos | Estado de Finalizaci√≥n |
| ----- | -----|
| 1. [**Machine Learning Specialization**](https://www.coursera.org/specializations/machine-learning-introduction?page=1) | ‚úÖ |
| 2. [**Deep Learning Specialization**](https://www.coursera.org/specializations/deep-learning?)| ‚úÖ  |
| 3. [**IA generativa con grandes modelos ling√º√≠sticos**](https://www.coursera.org/learn/generative-ai-with-llms/) | ‚úÖ |
| 4. [**Curso de Deep Learning**](https://youtube.com/playlist?list=PLcfxtMhW8iFNMTFKrYMYYzVTNzu-xG-Ys&si=lqAlbDIhtOJ5zMP8) | ‚úÖ |
| 5. [**Computer Vision**](https://youtube.com/playlist?list=PLISuMnTdVU-yvm6X7SwKtUosfr4ZarStU&si=FOMUjJ5SvotgMhHW) | ‚úÖ |

| Proyectos Completados |
| ----------------- |
| 1.  |
| 2.  |
| 3. ... |

# Temas Aprendidos en Cada D√≠a
| **D√≠as** | **Temas Cubiertos** | 
|--------- | ------------------ |
| [D√≠a1](#D√≠a1) | Introducci√≥n a Deep Learning | 
| [D√≠a2](#D√≠a2) | Historia y Evoluci√≥n de Deep Learning | 
| [D√≠a3](#D√≠a3) | Breve Descripci√≥n de las Diferentes T√©cnicas en Deep Learning | 
| [D√≠a4](#D√≠a4) | Comparaci√≥n y Aplicaciones de T√©cnicas de Deep Learning en el Mundo Real | 
| [D√≠a5](#D√≠a5) | Redes Neuronales Artificiales (ANNs) | 
| [D√≠a6](#D√≠a6) | Forward y Backward Propagation | 
| [D√≠a7](#D√≠a7) | Coste y Funciones de P√©rdida | 
| [D√≠a8](#D√≠a8) |  | 
| [D√≠a9](#D√≠a9) |  | 
| [D√≠a10](#D√≠a10) |  | 
| [D√≠a11](#D√≠a11) |  | 
| [D√≠a12](#D√≠a12) |  | 
| [D√≠a13](#D√≠a13) |  | 
| [D√≠a14](#D√≠a14) |  | 
| [D√≠a15](#D√≠a15) |  | 
| [D√≠a16](#D√≠a16) |  | 
| [D√≠a17](#D√≠a17) |  | 
| [D√≠a18](#D√≠a18) |  | 
| [D√≠a19](#D√≠a19) |  | 
| [D√≠a20](#D√≠a20) |  | 
| [D√≠a21](#D√≠a21) |  | 
| [D√≠a22](#D√≠a22) |  | 
| [D√≠a23](#D√≠a23) |  | 
| [D√≠a24](#D√≠a24) |  | 
| [D√≠a25](#D√≠a25) |  | 
| [D√≠a26](#D√≠a26) |  | 
| [D√≠a27](#D√≠a27) |  | 
| [D√≠a28](#D√≠a28) |  | 
| [D√≠a29](#D√≠a29) |  | 
| [D√≠a30](#D√≠a30) |  | 
| [D√≠a31](#D√≠a31) |  | 
| [D√≠a32](#D√≠a32) |  | 
| [D√≠a33](#D√≠a33) |  | 
| [D√≠a34](#D√≠a34) |  | 
| [D√≠a35](#D√≠a35) |  | 
| [D√≠a36](#D√≠a36) |  | 
| [D√≠a37](#D√≠a37) |  | 
| [D√≠a38](#D√≠a38) |  | 
| [D√≠a39](#D√≠a39) |  | 
| [D√≠a40](#D√≠a40) |  | 
| [D√≠a41](#D√≠a41) |  | 
| [D√≠a42](#D√≠a42) |  | 
| [D√≠a43](#D√≠a43) |  | 
| [D√≠a44](#D√≠a44) |  | 
| [D√≠a45](#D√≠a45) |  | 
| [D√≠a46](#D√≠a46) |  | 
| [D√≠a47](#D√≠a47) |  | 
| [D√≠a48](#D√≠a48) |  | 
| [D√≠a49](#D√≠a49) |  | 
| [D√≠a50](#D√≠a50) |  | 
| [D√≠a51](#D√≠a51) |  | 
| [D√≠a52](#D√≠a52) |  | 
| [D√≠a53](#D√≠a53) |  | 
| [D√≠a54](#D√≠a54) |  | 
| [D√≠a55](#D√≠a55) |  | 
| [D√≠a56](#D√≠a56) |  | 
| [D√≠a57](#D√≠a57) |  | 
| [D√≠a58](#D√≠a58) |  | 
| [D√≠a59](#D√≠a59) |  | 
| [D√≠a60](#D√≠a60) |  | 
| [D√≠a61](#D√≠a61) |  | 
| [D√≠a62](#D√≠a62) |  | 
| [D√≠a63](#D√≠a63) |  | 
| [D√≠a64](#D√≠a64) |  | 
| [D√≠a65](#D√≠a65) |  | 
| [D√≠a66](#D√≠a66) |  | 
| [D√≠a67](#D√≠a67) |  | 
| [D√≠a68](#D√≠a68) |  | 
| [D√≠a69](#D√≠a69) |  | 
| [D√≠a70](#D√≠a70) |  | 
| [D√≠a71](#D√≠a71) |  | 
| [D√≠a72](#D√≠a72) |  | 
| [D√≠a73](#D√≠a73) |  | 
| [D√≠a74](#D√≠a74) |  | 
| [D√≠a75](#D√≠a75) |  | 
| [D√≠a76](#D√≠a76) |  | 
| [D√≠a77](#D√≠a77) |  | 
| [D√≠a78](#D√≠a78) |  | 
| [D√≠a79](#D√≠a79) |  | 
| [D√≠a80](#D√≠a80) |  | 
| [D√≠a81](#D√≠a81) |  | 
| [D√≠a82](#D√≠a82) |  | 
| [D√≠a83](#D√≠a83) |  | 
| [D√≠a84](#D√≠a84) |  | 
| [D√≠a85](#D√≠a85) |  | 
| [D√≠a86](#D√≠a86) |  | 
| [D√≠a87](#D√≠a87) |  | 
| [D√≠a88](#D√≠a88) |  | 
| [D√≠a89](#D√≠a89) |  | 
| [D√≠a90](#D√≠a90) |  | 
| [D√≠a91](#D√≠a91) |  | 
| [D√≠a92](#D√≠a92) |  | 
| [D√≠a93](#D√≠a93) |  | 
| [D√≠a94](#D√≠a94) |  | 
| [D√≠a95](#D√≠a95) |  | 
| [D√≠a96](#D√≠a96) |  | 
| [D√≠a97](#D√≠a97) |  | 
| [D√≠a98](#D√≠a98) |  | 
| [D√≠a99](#D√≠a99) |  | 
| [D√≠a100](#D√≠a100) |  | 

# D√≠a1
---
## Introducci√≥n a Deep Learning üåü

¬°Bienvenidos al primer d√≠a de mi viaje de 100 d√≠as explorando la Inteligencia Artificial! üöÄ Hoy comenzamos con **Deep Learning**.

### ¬øQu√© es Deep Learning?

Deep Learning, o Aprendizaje Profundo, es una rama avanzada del **Machine Learning** que se inspira en la estructura y funci√≥n del cerebro humano. Utiliza **redes neuronales artificiales** para aprender de grandes vol√∫menes de datos y tomar decisiones o hacer predicciones precisas.

### ¬øPor qu√© es importante?

En los √∫ltimos a√±os, el Deep Learning ha revolucionado muchas industrias. Desde la **visi√≥n por computadora** que permite a los veh√≠culos aut√≥nomos ver el mundo, hasta el **procesamiento de lenguaje natural** que ayuda a las m√°quinas a entender y responder en lenguaje humano. Deep Learning es la tecnolog√≠a detr√°s de innovaciones impresionantes que est√°n cambiando la forma en que interactuamos con el mundo digital.

### ¬øC√≥mo funciona?

Las redes neuronales profundas est√°n compuestas por capas de neuronas artificiales. Cada capa transforma la entrada de datos en algo m√°s √∫til para la siguiente capa. A trav√©s de un proceso de entrenamiento, estas redes aprenden a extraer caracter√≠sticas complejas y patrones directamente de los datos.

### Ejemplos de Aplicaciones de Deep Learning:

- **Reconocimiento de Im√°genes**: Identificar objetos y personas en fotos y videos.
- **Traducci√≥n Autom√°tica**: Convertir texto de un idioma a otro con gran precisi√≥n.
- **Diagn√≥stico M√©dico**: Analizar im√°genes m√©dicas para detectar enfermedades.



 **Recursos para comenzar**üß†:
- **[APRENDIZAJE PROFUNDO EN INTELIGENCIA ARTIFICIAL](https://youtu.be/Zcb8R2TF3bI?si=f1NIEJgXh7cWdadV)** - Una breve esplicacion dew que es deep learning.
- **[¬øQUE ES EL DEEP LEARNING? - EXPLICADO MUY FACIL](https://youtu.be/s0SbvGiG28w?si=Rr51xld8H8ilsrz9)** - Video de Dalto explicando que es deep learning.
- **[¬øQu√© son el MACHINE LEARNING y el DEEP LEARNING?](https://youtu.be/HMEjoBnCc9c?si=U5MXn98cY7Yovy8w)** - Diferencias entre el Machine Learning y el Deep Learning.
- **[¬øDe qu√© es capaz la inteligencia artificial? ](https://youtu.be/34Kz-PP_X7c?si=sbV0ENQYtvT2JKiI)** - Documental de DW.

¬°√önete a m√≠ en este emocionante viaje y no dudes en compartir tus pensamientos y preguntas! üöÄ

---
# D√≠a2
---
## Historia y Evoluci√≥n de Deep Learning üìú

¬°Bienvenidos al segundo d√≠a de nuestra traves√≠a de 100 d√≠as en el mundo de la Inteligencia Artificial! Hoy, exploramos la fascinante **historia y evoluci√≥n de Deep Learning**. üåü

### Or√≠genes y Primeros Pasos

#### 1943: La Idea de una Neurona Artificial üí°
El viaje de Deep Learning comenz√≥ con Warren McCulloch y Walter Pitts, quienes propusieron el primer modelo matem√°tico de una **neurona artificial**. Su trabajo sent√≥ las bases para las redes neuronales, sugiriendo que las neuronas podr√≠an ser el equivalente funcional de un interruptor binario.

#### 1958: El Perceptr√≥n ü§ñ
Frank Rosenblatt desarroll√≥ el **Perceptr√≥n**, el primer modelo de red neuronal capaz de aprender. El perceptr√≥n es un tipo simple de red que puede clasificar datos en dos categor√≠as. Aunque su capacidad era limitada, fue un hito importante que inspir√≥ investigaciones futuras.

### El Invierno de la IA ‚ùÑÔ∏è

#### A√±os 70-80: Desaf√≠os y Dudas
Durante los a√±os 70 y 80, las expectativas sobre las redes neuronales no se cumplieron, y la falta de poder computacional y datos llev√≥ a lo que se conoce como el **"invierno de la IA"**. Durante este per√≠odo, la investigaci√≥n en redes neuronales se desaceler√≥ debido al escepticismo y la falta de avances significativos.

### Renacimiento y Avances üöÄ

#### 1986: El Redescubrimiento de la Propagaci√≥n hacia Atr√°s
En 1986, David Rumelhart, Geoffrey Hinton y Ronald Williams revitalizaron el inter√©s en las redes neuronales con su trabajo sobre la **retropropagaci√≥n**. Este algoritmo permiti√≥ el entrenamiento eficaz de redes neuronales multicapa, allanando el camino para el desarrollo de modelos m√°s complejos.

#### A√±os 90: Aplicaciones Pr√°cticas üåê
A medida que aumentaba el poder computacional y se dispon√≠a de m√°s datos, las redes neuronales comenzaron a mostrar su potencial en √°reas como el reconocimiento de patrones y la predicci√≥n financiera. Sin embargo, a√∫n quedaban desaf√≠os significativos por superar.

### La Era de Deep Learning üí•

#### 2006: El Avance de las Redes Profundas
Geoffrey Hinton y su equipo introdujeron el concepto de **preentrenamiento de capas** en redes profundas, lo que permiti√≥ entrenar eficientemente modelos con muchas capas. Este avance marc√≥ el comienzo de la **era de Deep Learning**, demostrando que las redes neuronales profundas pod√≠an superar a los m√©todos tradicionales en tareas complejas.

#### 2012: El Triunfo en ImageNet üèÜ
El hito crucial lleg√≥ en 2012 cuando una red profunda conocida como **AlexNet**, desarrollada por Alex Krizhevsky, Ilya Sutskever y Geoffrey Hinton, gan√≥ el desaf√≠o de reconocimiento de im√°genes de **ImageNet** con un margen significativo. Esto consolid√≥ a Deep Learning como la tecnolog√≠a l√≠der en visi√≥n por computadora.

### Transformadores y Nuevas Fronteras üöÄ

#### 2017: El Surgimiento de los Transformadores
En 2017, el art√≠culo "Attention is All You Need" de Google introdujo el **modelo Transformer**, revolucionando el procesamiento del lenguaje natural (NLP). Los Transformers, como **BERT** y **GPT**, demostraron capacidades impresionantes en tareas de lenguaje, superando a los modelos anteriores.

#### 2018: GPT y el Avance de los Modelos de Lenguaje
OpenAI lanz√≥ **GPT (Generative Pre-trained Transformer)**, seguido por GPT-2 y el famoso **GPT-3** en 2020. Estos modelos mostraron habilidades sin precedentes en generaci√≥n de texto, comprensi√≥n y traducci√≥n, marcando un hito en el desarrollo de la IA.

### Innovaciones Recientes üîÑ

#### 2021: DALL-E y la Creatividad Artificial
OpenAI present√≥ **DALL-E**, un modelo capaz de generar im√°genes a partir de descripciones textuales. Esta innovaci√≥n destac√≥ la capacidad de la IA para combinar lenguaje y visi√≥n, abriendo nuevas posibilidades en arte y dise√±o.

#### 2021: AlphaFold y la Revoluci√≥n en la Biolog√≠a
DeepMind's **AlphaFold** resolvi√≥ uno de los mayores desaf√≠os en biolog√≠a: la predicci√≥n de estructuras proteicas. Este avance promete acelerar el descubrimiento de medicamentos y mejorar nuestra comprensi√≥n de la biolog√≠a molecular.

#### 2022: ChatGPT y la Conversaci√≥n Natural
OpenAI lanz√≥ **ChatGPT**, una versi√≥n mejorada de GPT-3 optimizada para conversaciones interactivas. Este modelo demostr√≥ habilidades avanzadas en el di√°logo, respondiendo preguntas y asistiendo en diversas tareas de manera coherente y precisa.


### Recursos para Explorar M√°s:

- **[Breve Historia de las Redes Neuronales Artificiales](https://www.aprendemachinelearning.com/breve-historia-de-las-redes-neuronales-artificiales/)** - Un art√≠culo detallado sobre la evoluci√≥n de las redes neuronales.
- **[The brief history of artificial intelligence](https://ourworldindata.org/brief-history-of-ai)** - Un art√≠culo detallado sobre la evoluci√≥n de la IA.

### Evoluci√≥n de los modelos de IA con respecto a la computaci√≥n utilizada en su entrenamiento

<<<<<<< HEAD
=======
https://github.com/Oliver369X/100DaysOfAI/assets/110129950/64c6b46d-4c12-4e7a-8511-b35a2ad5be8e

>>>>>>> 52f223851c1f64cb143e7b84519e39d23a2985f8
---
# D√≠a3
---
## Breve Descripci√≥n de las Diferentes T√©cnicas en Deep Learning üß†


### 1. Redes Neuronales Convolucionales (CNN) üñºÔ∏è

#### Descripci√≥n
Las **Redes Neuronales Convolucionales (CNN)** est√°n dise√±adas para procesar datos con una estructura de grilla, como las im√°genes. Utilizan capas convolucionales que aplican filtros para detectar caracter√≠sticas como bordes, texturas y patrones en las im√°genes.

#### Componentes Clave
- **Capas Convolucionales**: Aplican filtros para extraer caracter√≠sticas locales.
- **Capas de Pooling**: Reducen la dimensionalidad y ayudan a generalizar.
- **Capas Completamente Conectadas**: Usadas para clasificar y tomar decisiones basadas en las caracter√≠sticas extra√≠das.

### 2. Redes Neuronales Recurrentes (RNN) üîÅ

#### Descripci√≥n
Las **Redes Neuronales Recurrentes (RNN)** est√°n dise√±adas para procesar secuencias de datos, como texto o series temporales. Tienen conexiones recurrentes que permiten que la informaci√≥n persista, lo que es √∫til para modelar dependencias temporales.

#### Componentes Clave
- **Celdas Recurrentes**: Mantienen un estado oculto que captura informaci√≥n de pasos anteriores.
- **LSTM y GRU**: Variantes avanzadas de RNN que abordan problemas de memoria a largo plazo.

### 3. Redes Generativas Adversariales (GAN) üé®

#### Descripci√≥n
Las **Redes Generativas Adversariales (GAN)** constan de dos redes: una generadora y una discriminadora. La generadora crea datos falsos, mientras que la discriminadora intenta distinguir entre datos reales y falsos. Este proceso competitivo mejora la capacidad de la generadora para producir datos realistas.

#### Componentes Clave
- **Generador**: Crea datos sint√©ticos.
- **Discriminador**: Distingue entre datos reales y generados.
- **Juego Adversarial**: La competencia entre las dos redes mejora el rendimiento del sistema.

### 4. Transformadores üîÑ

#### Descripci√≥n
Los **Transformadores** han revolucionado el procesamiento del lenguaje natural (NLP) con su mecanismo de atenci√≥n que permite procesar todas las palabras de una oraci√≥n en paralelo. Esto los hace altamente eficientes y precisos en tareas de lenguaje.

#### Componentes Clave
- **Mecanismo de Atenci√≥n**: Pondera la importancia de diferentes palabras en una oraci√≥n.
- **Codificadores y Decodificadores**: Procesan las secuencias de entrada y generan secuencias de salida.

### 5. Modelos de Difusi√≥n üå´Ô∏è

#### Descripci√≥n
Los **Modelos de Difusi√≥n** son una t√©cnica emergente en generaci√≥n de datos. Funcionan modelando la distribuci√≥n de los datos y luego generando nuevos ejemplos a partir de esta distribuci√≥n, similar a los procesos f√≠sicos de difusi√≥n.

#### Componentes Clave
- **Proceso de Difusi√≥n**: Modela c√≥mo los datos cambian con el tiempo.
- **Reconstrucci√≥n Inversa**: Genera nuevos datos a partir del proceso de difusi√≥n.

### 6. Modelos Multimodales üé•üéµüìù

#### Descripci√≥n
Los **Modelos Multimodales** integran y procesan m√∫ltiples tipos de datos, como texto, im√°genes y audio, para realizar tareas complejas que requieren comprensi√≥n de informaci√≥n diversa.

#### Componentes Clave
- **Fusi√≥n de Modalidades**: Combina diferentes tipos de datos en una representaci√≥n unificada.
- **Atenci√≥n Cruzada**: Captura interacciones entre diferentes modalidades.


### Recursos para Explorar M√°s:

- **[¬°Redes Neuronales CONVOLUCIONALES! ](https://youtu.be/V8j1oENVz00?si=RY91rvLjMXPbjRbF)** - Video detallado sobre CNN.
- **[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)** - Una explicaci√≥n profunda sobre las RNN y LSTM.
- **[GANs in Action](https://www.youtube.com/watch?v=8L11aMN5KY8)** - Un video tutorial sobre GANs.
- **[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)** - Una gu√≠a visual sobre transformadores.
- **[C√≥mo funciona la generaci√≥n de im√°genes con IA (modelos de difusi√≥n)](https://youtu.be/mNxzQvdVSQI?si=_Lno74MYiqcbidei)** - Introducci√≥n a los modelos de difusi√≥n.
- **[Multimodal learning](https://en.wikipedia.org/wiki/Multimodal_learning)** - Definicion de Wikipedia.

---

# D√≠a4
---
## Comparaci√≥n y Aplicaciones de T√©cnicas de Deep Learning en el Mundo Real üåç

¬°Hola a todos! compararemos las diferentes t√©cnicas de Deep Learning que discutimos ayer y exploraremos sus aplicaciones en el mundo real. Vamos a sumergirnos en c√≥mo se utilizan las **CNN, RNN, GAN, Transformadores, Modelos de Difusi√≥n y Modelos Multimodales** en diversos campos. üåê

### Comparaci√≥n de T√©cnicas de Deep Learning

| T√©cnica         | Descripci√≥n                                                   | Fortalezas                                                     | Limitaciones                                                       |
|-----------------|---------------------------------------------------------------|----------------------------------------------------------------|--------------------------------------------------------------------|
| **CNN**         | Procesan datos con estructura de grilla (como im√°genes).       | Excelente para tareas de visi√≥n por computadora.                | No maneja bien datos secuenciales o dependencias temporales.       |
| **RNN**         | Procesan secuencias de datos (como texto o series temporales). | Capturan dependencias temporales y contextuales.                | Pueden sufrir de problemas de gradiente desaparecido/explosivo.    |
| **GAN**         | Generan datos sint√©ticos mediante una competencia entre dos redes. | Producen datos realistas en imagen, video y audio.              | Dificultad en entrenamiento y estabilidad.                         |
| **Transformadores** | Procesan secuencias de datos en paralelo utilizando atenci√≥n. | Eficientes y precisos en procesamiento de lenguaje natural.     | Requieren grandes cantidades de datos y recursos computacionales.  |
| **Modelos de Difusi√≥n** | Modelan la distribuci√≥n de datos para generaci√≥n.        | Alta calidad en generaci√≥n de im√°genes y datos.                 | T√©cnicamente complejos y requieren mucho tiempo de entrenamiento.  |
| **Modelos Multimodales** | Integran m√∫ltiples tipos de datos (texto, imagen, audio). | Capturan interacciones complejas entre diferentes tipos de datos. | Complejidad en la fusi√≥n de datos y gesti√≥n de m√∫ltiples modalidades. |

### Aplicaciones en el Mundo Real

#### 1. Redes Neuronales Convolucionales (CNN) üñºÔ∏è

**Aplicaciones:**
- **Reconocimiento de Im√°genes**: Identificaci√≥n de objetos, personas y escenas en im√°genes.
- **Diagn√≥stico M√©dico**: An√°lisis de im√°genes m√©dicas, como radiograf√≠as y resonancias magn√©ticas.
- **Seguridad y Vigilancia**: Detecci√≥n de anomal√≠as y reconocimiento facial.

#### 2. Redes Neuronales Recurrentes (RNN) üîÅ

**Aplicaciones:**
- **Procesamiento del Lenguaje Natural (NLP)**: Traducci√≥n autom√°tica, generaci√≥n de texto, chatbots.
- **An√°lisis de Series Temporales**: Predicci√≥n de mercados financieros, demanda energ√©tica, clima.
- **Reconocimiento de Voz**: Transcripci√≥n y comandos de voz en asistentes virtuales.

#### 3. Redes Generativas Adversariales (GAN) üé®

**Aplicaciones:**
- **Generaci√≥n de Im√°genes y Videos**: Creaci√≥n de arte digital, efectos visuales en pel√≠culas.
- **Aumento de Datos**: Generaci√≥n de datos sint√©ticos para mejorar el entrenamiento de modelos.
- **Restauraci√≥n de Im√°genes**: Mejora de resoluci√≥n, eliminaci√≥n de ruido, restauraci√≥n de im√°genes antiguas.

#### 4. Transformadores üîÑ

**Aplicaciones:**
- **Procesamiento del Lenguaje Natural (NLP)**: Modelos de lenguaje avanzados como GPT, BERT, traducci√≥n autom√°tica.
- **Generaci√≥n de Texto**: Resumen autom√°tico, generaci√≥n de contenido, respuestas autom√°ticas en chats.
- **An√°lisis de Datos**: Clasificaci√≥n de documentos, detecci√≥n de entidades nombradas, an√°lisis de sentimientos.

#### 5. Modelos de Difusi√≥n üå´Ô∏è

**Aplicaciones:**
- **Generaci√≥n de Im√°genes**: Creaci√≥n de im√°genes de alta calidad a partir de descripciones textuales.
- **Simulaci√≥n de Procesos F√≠sicos**: Modelado de fen√≥menos naturales como la difusi√≥n de gases.
- **Dise√±o Gr√°fico**: Creaci√≥n de patrones y texturas para dise√±o digital.

#### 6. Modelos Multimodales üé•üéµüìù

**Aplicaciones:**
- **Sistemas de Recomendaci√≥n**: Recomendaciones personalizadas basadas en m√∫ltiples tipos de datos (texto, im√°genes, audio).
- **An√°lisis de Redes Sociales**: Comprensi√≥n de publicaciones multimedia, an√°lisis de sentimientos.
- **Asistentes Virtuales**: Integraci√≥n de voz, texto e im√°genes para interacci√≥n m√°s natural y completa.


### Recursos para Explorar M√°s:


- **[The GAN Zoo](https://github.com/hindupuravinash/the-gan-zoo)** - Una colecci√≥n de diferentes tipos de GANs.
- **[Attention is All You Need](https://arxiv.org/abs/1706.03762)** - El art√≠culo seminal sobre transformadores.
- **[Explicaci√≥n Completa: Attention is All You Need](https://youtu.be/as2FFM3c6mI?si=_pNuRFCEHHYsizro)** - Un video detallado explicando los transformadores.

---

# D√≠a5
---
## Redes Neuronales Artificiales (ANNs)  üß†

¬°Hola a todos! En el quinto d√≠a de nuestra traves√≠a de 100 d√≠as en el mundo de la Inteligencia Artificial, exploraremos la estructura b√°sica de las Redes Neuronales Artificiales (ANNs) y entenderemos c√≥mo funcionan sus capas neuronales. üåü

### ¬øQu√© son las Redes Neuronales Artificiales (ANNs)?

Las Redes Neuronales Artificiales (ANNs) son modelos computacionales inspirados en el funcionamiento del cerebro humano. Est√°n dise√±adas para reconocer patrones y resolver problemas complejos mediante el aprendizaje a partir de datos. üåê

### Estructura B√°sica de una Red Neuronal

Una red neuronal t√≠pica consta de tres tipos de capas:

1. **Capa de Entrada (Input Layer)**: Recibe los datos iniciales.
2. **Capas Ocultas (Hidden Layers)**: Procesan la informaci√≥n recibida de la capa de entrada.
3. **Capa de Salida (Output Layer)**: Genera el resultado final.


#### 1. **Capa de Entrada (Input Layer)**
La capa de entrada es la primera capa de la red neuronal. Cada nodo en esta capa representa una caracter√≠stica del conjunto de datos de entrada. Por ejemplo, en una red que procesa im√°genes, cada nodo podr√≠a representar el valor de un p√≠xel de la imagen.

#### 2. **Capas Ocultas (Hidden Layers)**
Las capas ocultas son las encargadas de realizar la mayor parte del procesamiento de la red. Pueden existir m√∫ltiples capas ocultas, cada una compuesta por m√∫ltiples nodos o "neuronas". Cada neurona en una capa est√° conectada a todas las neuronas de la capa anterior y de la capa siguiente.

##### Funcionamiento de las Capas Ocultas:
- **Pesos y Sesgos (Weights and Biases)**: Cada conexi√≥n entre neuronas tiene un peso asignado que indica la importancia de la entrada correspondiente. Adem√°s, cada neurona tiene un valor de sesgo que ajusta la salida del nodo.
- **Funciones de Activaci√≥n (Activation Functions)**: Despu√©s de que una neurona recibe la entrada ponderada, aplica una funci√≥n de activaci√≥n para introducir no linealidades en el modelo. Las funciones de activaci√≥n comunes incluyen ReLU (Rectified Linear Unit), Sigmoid y Tanh.



#### 3. **Capa de Salida (Output Layer)**
La capa de salida es la √∫ltima capa de la red neuronal y proporciona el resultado final. La estructura de esta capa depende del tipo de tarea que est√© realizando la red. Por ejemplo, en un problema de clasificaci√≥n binaria, la capa de salida podr√≠a tener una sola neurona con una funci√≥n de activaci√≥n Sigmoid.

### ¬øC√≥mo Aprenden las Redes Neuronales?

El aprendizaje en redes neuronales implica ajustar los pesos y los sesgos de la red para minimizar el error en las predicciones. Este proceso se realiza mediante un algoritmo de optimizaci√≥n llamado **Backpropagation** (retropropagaci√≥n), que utiliza el **Gradiente Descendente** para ajustar los pesos de manera iterativa.


### Recursos para Explorar M√°s:

- **[C√≥mo funcionan las redes neuronales](https://youtu.be/CU24iC3grq8?si=9UT2DpOAA1cQ1Ay0)** (Video).
- **[¬øQu√© es una Red Neuronal?](https://youtu.be/jKCQsndqEGQ?si=jNASfwuoQB9tXyle)** - (Video).
- **[Funciones de activaci√≥n a detalle](https://youtu.be/_0wdproot34?si=B27NeiOze7QGGi6K)** - (Video).
- **[Juegue con una red neuronal ](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=1&seed=0.87931&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)** - Juegue con una red neuronal aqu√≠ mismo en su navegador.
No te preocupes, no puedes romperlo.
![ANNs](https://github.com/Oliver369X/100DaysOfAI/assets/110129950/6de8c3e3-ea5a-46e0-8fd0-600e794b422d)

![back2](https://github.com/Oliver369X/100DaysOfAI/assets/110129950/2ebfb67d-7af9-49bd-a509-b1d6babf0148)

---

# D√≠a6
---
## Conceptos de Forward y Backward Propagation üß†üîÑ

¬°Hola a todos! Hoy, en el sexto d√≠a de nuestro viaje de 100 d√≠as en el mundo de la Inteligencia Artificial, exploraremos dos conceptos fundamentales para el entrenamiento de redes neuronales: **Forward Propagation** y **Backward Propagation**. Estos procesos son esenciales para que las redes neuronales aprendan de los datos y mejoren su rendimiento. üöÄ

### ¬øQu√© es Forward Propagation?

**Forward Propagation** es el proceso mediante el cual los datos de entrada se transmiten a trav√©s de la red neuronal para generar una salida. Este flujo de informaci√≥n comienza en la capa de entrada, pasa por las capas ocultas y finalmente llega a la capa de salida.

#### Pasos de Forward Propagation:

1. **Entrada**: Los datos de entrada se presentan a la red neuronal.
2. **Ponderaci√≥n**: Cada neurona en la capa de entrada env√≠a sus datos ponderados a cada neurona de la primera capa oculta.
3. **Activaci√≥n**: Las neuronas de la capa oculta calculan una suma ponderada de sus entradas, aplican una funci√≥n de activaci√≥n y transmiten el resultado a la siguiente capa.
4. **Salida**: Este proceso se repite capa por capa hasta que los datos alcanzan la capa de salida, donde se generan las predicciones finales.

### ¬øQu√© es Backward Propagation?

**Backward Propagation** (o retropropagaci√≥n) es el proceso mediante el cual la red neuronal ajusta sus pesos y sesgos para minimizar el error en sus predicciones. Este ajuste se realiza mediante la propagaci√≥n del error desde la capa de salida hacia atr√°s a trav√©s de las capas ocultas, hasta llegar a la capa de entrada.

#### Pasos de Backward Propagation:

1. **C√°lculo del Error**: Se calcula la diferencia entre la salida real de la red y la salida esperada (etiquetas verdaderas).
2. **Propagaci√≥n del Error**: El error se propaga hacia atr√°s a trav√©s de la red. En cada neurona, se calcula el gradiente del error con respecto a sus pesos y sesgos.
3. **Ajuste de Pesos y Sesgos**: Los pesos y sesgos se actualizan utilizando el gradiente calculado y una tasa de aprendizaje, reduciendo as√≠ el error de la red.

### C√≥mo Funcionan Juntos Forward y Backward Propagation

1. **Forward Propagation**: Los datos de entrada se procesan a trav√©s de la red para generar una predicci√≥n.
2. **C√°lculo del Error**: Se compara la predicci√≥n con la etiqueta verdadera para calcular el error.
3. **Backward Propagation**: El error se propaga hacia atr√°s a trav√©s de la red, y los pesos y sesgos se ajustan en consecuencia.
4. **Actualizaci√≥n de Par√°metros**: Los par√°metros de la red se actualizan para reducir el error en futuras predicciones.

### Ejemplo Simplificado

Imaginemos que estamos entrenando una red neuronal para predecir el precio de una casa basado en su tama√±o.

1. **Forward Propagation**:
   - Entrada: Tama√±o de la casa.
   - C√°lculo: La red multiplica el tama√±o por un peso, a√±ade un sesgo y aplica una funci√≥n de activaci√≥n.
   - Salida: Predicci√≥n del precio de la casa.

2. **C√°lculo del Error**:
   - Comparamos la predicci√≥n con el precio real y calculamos el error.

3. **Backward Propagation**:
   - Propagamos el error hacia atr√°s a trav√©s de la red, calculando el gradiente del error con respecto a cada peso y sesgo.
   - Ajustamos los pesos y sesgos para minimizar el error en futuras predicciones.


### Recursos para Explorar M√°s:

- **[Redes Neuronales (forward propagation y backpropagation)](https://youtu.be/A9jZflhT2R0?si=uQj8Xw1xa2_O1kDO)** -Explicacion matematica(Video).
- **[Las Matem√°ticas de Backpropagation | DotCSV](https://youtu.be/M5QHwkkHgAA?si=ZiX3Gp9I25liaNFq)** - Explicacion matematica(Video).

---

# D√≠a7

---
## Conceptos de Coste y Funciones de P√©rdida üí°üìâ

¬°Hola a todos! Hoy, en el s√©ptimo d√≠a de nuestro reto #100DaysOfAI, exploraremos dos conceptos fundamentales para el entrenamiento de redes neuronales: **coste** y **funciones de p√©rdida**. Estos conceptos son esenciales para evaluar el rendimiento de nuestros modelos y guiar el proceso de aprendizaje. üöÄ

### ¬øQu√© es el Coste?

El **coste** se refiere a la medida de lo mal que un modelo de red neuronal est√° realizando sus predicciones en comparaci√≥n con los valores reales. En otras palabras, es una representaci√≥n cuantitativa del error del modelo. Cuanto menor sea el coste, mejor ser√° el rendimiento del modelo.

### ¬øQu√© es una Funci√≥n de P√©rdida?

Una **funci√≥n de p√©rdida** es una funci√≥n matem√°tica que mide la discrepancia entre las predicciones del modelo y los valores reales. Durante el entrenamiento, el objetivo es minimizar esta funci√≥n de p√©rdida para mejorar la precisi√≥n del modelo. 

### Tipos Comunes de Funciones de P√©rdida:

1. **Error Cuadr√°tico Medio (Mean Squared Error, MSE)**:

2. **Error Absoluto Medio (Mean Absolute Error, MAE)**:


3. **Entrop√≠a Cruzada (Cross-Entropy)**:


### Relaci√≥n entre Coste y Funci√≥n de P√©rdida:

- **Coste Total**: La funci√≥n de p√©rdida calcula el error para una sola instancia de datos, mientras que el coste total (tambi√©n conocido como funci√≥n de coste o funci√≥n de error) es la media de las p√©rdidas para todo el conjunto de entrenamiento.
- **Optimizaci√≥n**: Durante el entrenamiento, el algoritmo de optimizaci√≥n ajusta los pesos de la red neuronal para minimizar el coste total. Esto se realiza t√≠picamente mediante un algoritmo de optimizaci√≥n como el gradiente descendente.

### Importancia en el Entrenamiento

1. **Evaluaci√≥n del Modelo**: Las funciones de p√©rdida nos permiten evaluar cu√°n bien o mal est√° desempe√±√°ndose el modelo.
2. **Gu√≠a para la Optimizaci√≥n**: Proveen la se√±al que gu√≠a el proceso de optimizaci√≥n durante el entrenamiento. Sin una funci√≥n de p√©rdida, no podr√≠amos ajustar los pesos de manera efectiva.
3. **Selecci√≥n de Modelos**: Diferentes problemas pueden requerir diferentes funciones de p√©rdida. Elegir la funci√≥n correcta es crucial para el √©xito del modelo.


### Recursos para Explorar M√°s:

- **[3Blue1Brown's YouTube Series on Neural Networks](https://youtu.be/mwHiaTrQOiI?si=j_a-9WxP_1um9YVc)** - Una serie de videos educativos que visualizan estos procesos de manera intuitiva.

---

# D√≠a8
# D√≠a9
# D√≠a10
# D√≠a11
# D√≠a12
# D√≠a13
# D√≠a14
# D√≠a15
# D√≠a16
# D√≠a17
# D√≠a18
# D√≠a19
# D√≠a20
# D√≠a21
# D√≠a22
# D√≠a23
# D√≠a24
# D√≠a25
# D√≠a26
# D√≠a27
# D√≠a28
# D√≠a29
# D√≠a30
# D√≠a31
# D√≠a32
# D√≠a33
# D√≠a34
# D√≠a35
# D√≠a36
# D√≠a37
# D√≠a38
# D√≠a39
# D√≠a40
# D√≠a41
# D√≠a42
# D√≠a43
# D√≠a44
# D√≠a45
# D√≠a46
# D√≠a47
# D√≠a48
# D√≠a49
# D√≠a50
# D√≠a51
# D√≠a52
# D√≠a53
# D√≠a54
# D√≠a55
# D√≠a56
# D√≠a57
# D√≠a58
# D√≠a59
# D√≠a60
# D√≠a61
# D√≠a62
# D√≠a63
# D√≠a64
# D√≠a65
# D√≠a66
# D√≠a67
# D√≠a68
# D√≠a69
# D√≠a70
# D√≠a71
# D√≠a72
# D√≠a73
# D√≠a74
# D√≠a75
# D√≠a76
# D√≠a77
# D√≠a78
# D√≠a79
# D√≠a80
# D√≠a81
# D√≠a82
# D√≠a83
# D√≠a84
# D√≠a85
# D√≠a86
# D√≠a87
# D√≠a88
# D√≠a89
# D√≠a90
# D√≠a91
# D√≠a92
# D√≠a93
# D√≠a94
# D√≠a95
# D√≠a96
# D√≠a97
# D√≠a98
# D√≠a99
# D√≠a100
